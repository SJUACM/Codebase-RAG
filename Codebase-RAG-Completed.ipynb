{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTmVgAC90r3Z"
      },
      "source": [
        "![Img](https://app.theheadstarter.com/static/hs-logo-opengraph.png)\n",
        "\n",
        "# Headstarter Codebase RAG Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSQbb-WI0Nb2"
      },
      "source": [
        "![Screenshot 2024-11-25 at 7 12 58 PM](https://github.com/user-attachments/assets/48dd9de1-b4d2-4318-8f52-85ec209d8ebc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpmkP4rM1KRt"
      },
      "source": [
        "# Install Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BGFWnzpBDkWH",
        "outputId": "0887ecd1-0916-4cf0-bacb-93de6223ca82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pygithub\n",
            "  Downloading PyGithub-2.6.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.22-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.75.0)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-6.0.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting langchain_pinecone\n",
            "  Downloading langchain_pinecone-0.2.5-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Collecting pynacl>=1.4.0 (from pygithub)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from pygithub) (2.32.3)\n",
            "Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->pygithub) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from pygithub) (4.13.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pygithub) (2.3.0)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from pygithub) (1.2.18)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.55)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.33)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2025.1.31)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.8.2)\n",
            "Collecting pinecone<7.0.0,>=6.0.0 (from pinecone[async]<7.0.0,>=6.0.0->langchain_pinecone)\n",
            "  Downloading pinecone-6.0.2-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
            "  Downloading aiohttp-3.10.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting langchain-tests<1.0.0,>=0.3.7 (from langchain_pinecone)\n",
            "  Downloading langchain_tests-0.3.19-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
            "Requirement already satisfied: pytest<9,>=7 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (8.3.5)\n",
            "Collecting pytest-asyncio<1,>=0.20 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_asyncio-0.26.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting syrupy<5,>=4 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading syrupy-4.9.1-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting pytest-socket<1,>=0.6.0 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_socket-0.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "\u001b[33mWARNING: pinecone 6.0.2 does not provide the extra 'async'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->pygithub) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pynacl>=1.4.0->pygithub) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->pygithub) (3.4.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated->pygithub) (1.17.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->pygithub) (2.22)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.5.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading PyGithub-2.6.1-py3-none-any.whl (410 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.22-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_client-6.0.0-py3-none-any.whl (6.7 kB)\n",
            "Downloading langchain_pinecone-0.2.5-py3-none-any.whl (16 kB)\n",
            "Downloading aiohttp-3.10.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_tests-0.3.19-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone-6.0.2-py3-none-any.whl (421 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.9/421.9 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_asyncio-0.26.0-py3-none-any.whl (19 kB)\n",
            "Downloading pytest_socket-0.7.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading syrupy-4.9.1-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, pinecone-plugin-interface, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, httpx-sse, typing-inspect, tiktoken, syrupy, pytest-socket, pytest-asyncio, pynacl, pinecone-client, pinecone, nvidia-cusparse-cu12, nvidia-cudnn-cu12, aiohttp, pydantic-settings, nvidia-cusolver-cu12, dataclasses-json, pygithub, langchain-tests, langchain_pinecone, langchain-community\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.11.15\n",
            "    Uninstalling aiohttp-3.11.15:\n",
            "      Successfully uninstalled aiohttp-3.11.15\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed aiohttp-3.10.11 dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.22 langchain-tests-0.3.19 langchain_pinecone-0.2.5 marshmallow-3.26.1 mypy-extensions-1.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pinecone-6.0.2 pinecone-client-6.0.0 pinecone-plugin-interface-0.0.7 pydantic-settings-2.9.1 pygithub-2.6.1 pynacl-1.5.0 pytest-asyncio-0.26.0 pytest-socket-0.7.0 python-dotenv-1.1.0 syrupy-4.9.1 tiktoken-0.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "! pip install pygithub langchain langchain-community openai tiktoken pinecone-client langchain_pinecone sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zAIXpUxWDFSV"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from google.colab import userdata\n",
        "from pinecone import Pinecone\n",
        "import os\n",
        "import tempfile\n",
        "from github import Github, Repository\n",
        "from git import Repo\n",
        "from openai import OpenAI\n",
        "from pathlib import Path\n",
        "from langchain.schema import Document\n",
        "from pinecone import Pinecone\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTLsQ9Ma1FpK"
      },
      "source": [
        "# Clone a GitHub Repo locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jhXMD6j6-Xfh"
      },
      "outputs": [],
      "source": [
        "github_repo = \"https://github.com/CoderAgent/SecureAgent\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "F_1zslPsDmJQ"
      },
      "outputs": [],
      "source": [
        "def clone_repository(repo_url):\n",
        "    \"\"\"Clones a GitHub repository to a temporary directory.\n",
        "\n",
        "    Args:\n",
        "        repo_url: The URL of the GitHub repository.\n",
        "\n",
        "    Returns:\n",
        "        The path to the cloned repository.\n",
        "    \"\"\"\n",
        "\n",
        "    repo_name = github_repo.split(\"/\")[-1]\n",
        "    repo_path = f\"/content/{repo_name}\"\n",
        "    Repo.clone_from(repo_url, str(repo_path))\n",
        "    return repo_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hFrrr5rjEfYn"
      },
      "outputs": [],
      "source": [
        "path = clone_repository(github_repo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOIPXbV_KvIT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rm1vwr5KVQZ"
      },
      "source": [
        "# Define which types of files to parse and which files / folders to ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MQOcyi6DE5bL"
      },
      "outputs": [],
      "source": [
        "SUPPORTED_EXTENSIONS = {'.py', '.js', '.tsx', '.jsx', '.ipynb', '.java',\n",
        "                         '.cpp', '.ts', '.go', '.rs', '.vue', '.swift', '.c', '.h'}\n",
        "\n",
        "IGNORED_DIRS = {'node_modules', 'venv', 'env', 'dist', 'build', '.git',\n",
        "                '__pycache__', '.next', '.vscode', 'vendor'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qi0FbfdrF6Hd"
      },
      "outputs": [],
      "source": [
        "def get_file_content(file_path, repo_path):\n",
        "    \"\"\"\n",
        "    Get content of a single file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the file\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict[str, str]]: Dictionary with file name and content\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "\n",
        "        rel_path = os.path.relpath(file_path, repo_path)\n",
        "\n",
        "        return {\n",
        "            \"name\": rel_path,\n",
        "            \"content\": content\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def get_main_files_content(repo_path: str):\n",
        "    \"\"\"\n",
        "    Get content of supported code files from the local repository.\n",
        "\n",
        "    Args:\n",
        "        repo_path: Path to the local repository\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries containing file names and contents\n",
        "    \"\"\"\n",
        "\n",
        "    files_content = []\n",
        "\n",
        "    try:\n",
        "\n",
        "        for root, _, files in os.walk(repo_path):\n",
        "            if any(ignored_dir in root for ignored_dir in IGNORED_DIRS):\n",
        "                continue\n",
        "\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                if os.path.splitext(file)[1] in SUPPORTED_EXTENSIONS:\n",
        "                    file_content = get_file_content(file_path, repo_path)\n",
        "\n",
        "                    if file_content:\n",
        "                        files_content.append(file_content)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    return files_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5mMaHXkrKoFa"
      },
      "outputs": [],
      "source": [
        "files_content = get_main_files_content(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HvNX_WcUKoH6",
        "outputId": "210d586c-b01d-4809-e181-50666230f2bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'src/app.ts',\n",
              "  'content': 'import { Octokit } from \"@octokit/rest\";\\nimport { createNodeMiddleware } from \"@octokit/webhooks\";\\nimport { WebhookEventMap } from \"@octokit/webhooks-definitions/schema\";\\nimport * as http from \"http\";\\nimport { App } from \"octokit\";\\nimport { Review } from \"./constants\";\\nimport { env } from \"./env\";\\nimport { processPullRequest } from \"./review-agent\";\\nimport { applyReview } from \"./reviews\";\\n\\n// This creates a new instance of the Octokit App class.\\nconst reviewApp = new App({\\n  appId: env.GITHUB_APP_ID,\\n  privateKey: env.GITHUB_PRIVATE_KEY,\\n  webhooks: {\\n    secret: env.GITHUB_WEBHOOK_SECRET,\\n  },\\n});\\n\\nconst getChangesPerFile = async (payload: WebhookEventMap[\"pull_request\"]) => {\\n  try {\\n    const octokit = await reviewApp.getInstallationOctokit(\\n      payload.installation.id\\n    );\\n    const { data: files } = await octokit.rest.pulls.listFiles({\\n      owner: payload.repository.owner.login,\\n      repo: payload.repository.name,\\n      pull_number: payload.pull_request.number,\\n    });\\n    console.dir({ files }, { depth: null });\\n    return files;\\n  } catch (exc) {\\n    console.log(\"exc\");\\n    return [];\\n  }\\n};\\n\\n// This adds an event handler that your code will call later. When this event handler is called, it will log the event to the console. Then, it will use GitHub\\'s REST API to add a comment to the pull request that triggered the event.\\nasync function handlePullRequestOpened({\\n  octokit,\\n  payload,\\n}: {\\n  octokit: Octokit;\\n  payload: WebhookEventMap[\"pull_request\"];\\n}) {\\n  console.log(\\n    `Received a pull request event for #${payload.pull_request.number}`\\n  );\\n  // const reposWithInlineEnabled = new Set<number>([601904706, 701925328]);\\n  // const canInlineSuggest = reposWithInlineEnabled.has(payload.repository.id);\\n  try {\\n    console.log(\"pr info\", {\\n      id: payload.repository.id,\\n      fullName: payload.repository.full_name,\\n      url: payload.repository.html_url,\\n    });\\n    const files = await getChangesPerFile(payload);\\n    const review: Review = await processPullRequest(\\n      octokit,\\n      payload,\\n      files,\\n      true\\n    );\\n    await applyReview({ octokit, payload, review });\\n    console.log(\"Review Submitted\");\\n  } catch (exc) {\\n    console.log(exc);\\n  }\\n}\\n\\n// This sets up a webhook event listener. When your app receives a webhook event from GitHub with a `X-GitHub-Event` header value of `pull_request` and an `action` payload value of `opened`, it calls the `handlePullRequestOpened` event handler that is defined above.\\n//@ts-ignore\\nreviewApp.webhooks.on(\"pull_request.opened\", handlePullRequestOpened);\\n\\nconst port = process.env.PORT || 3000;\\nconst reviewWebhook = `/api/review`;\\n\\nconst reviewMiddleware = createNodeMiddleware(reviewApp.webhooks, {\\n  path: \"/api/review\",\\n});\\n\\nconst server = http.createServer((req, res) => {\\n  if (req.url === reviewWebhook) {\\n    reviewMiddleware(req, res);\\n  } else {\\n    res.statusCode = 404;\\n    res.end();\\n  }\\n});\\n\\n// This creates a Node.js server that listens for incoming HTTP requests (including webhook payloads from GitHub) on the specified port. When the server receives a request, it executes the `middleware` function that you defined earlier. Once the server is running, it logs messages to the console to indicate that it is listening.\\nserver.listen(port, () => {\\n  console.log(`Server is listening for events.`);\\n  console.log(\"Press Ctrl + C to quit.\");\\n});\\n'},\n",
              " {'name': 'src/reviews.ts',\n",
              "  'content': 'import {\\n  BranchDetails,\\n  BuilderResponse,\\n  CodeSuggestion,\\n  Review,\\n  processGitFilepath,\\n} from \"./constants\";\\nimport { Octokit } from \"@octokit/rest\";\\nimport { WebhookEventMap } from \"@octokit/webhooks-definitions/schema\";\\n\\nconst postGeneralReviewComment = async (\\n  octokit: Octokit,\\n  payload: WebhookEventMap[\"pull_request\"],\\n  review: string\\n) => {\\n  try {\\n    await octokit.request(\\n      \"POST /repos/{owner}/{repo}/issues/{issue_number}/comments\",\\n      {\\n        owner: payload.repository.owner.login,\\n        repo: payload.repository.name,\\n        issue_number: payload.pull_request.number,\\n        body: review,\\n        headers: {\\n          \"x-github-api-version\": \"2022-11-28\",\\n        },\\n      }\\n    );\\n  } catch (exc) {\\n    console.log(exc);\\n  }\\n};\\n\\nconst postInlineComment = async (\\n  octokit: Octokit,\\n  payload: WebhookEventMap[\"pull_request\"],\\n  suggestion: CodeSuggestion\\n) => {\\n  try {\\n    const line = suggestion.line_end;\\n    let startLine = null;\\n    if (suggestion.line_end != suggestion.line_start) {\\n      startLine = suggestion.line_start;\\n    }\\n    const suggestionBody = `${suggestion.comment}\\\\n\\\\`\\\\`\\\\`suggestion\\\\n${suggestion.correction}`;\\n\\n    await octokit.request(\\n      \"POST /repos/{owner}/{repo}/pulls/{pull_number}/comments\",\\n      {\\n        owner: payload.repository.owner.login,\\n        repo: payload.repository.name,\\n        pull_number: payload.pull_request.number,\\n        body: suggestionBody,\\n        commit_id: payload.pull_request.head.sha,\\n        path: suggestion.file,\\n        line: line,\\n        ...(startLine ? { start_line: startLine } : {}),\\n        // position: suggestion.line_start,\\n        // subject_type: \"line\",\\n        start_side: \"RIGHT\",\\n        side: \"RIGHT\",\\n        headers: {\\n          \"X-GitHub-Api-Version\": \"2022-11-28\",\\n        },\\n      }\\n    );\\n  } catch (exc) {\\n    console.log(exc);\\n  }\\n};\\n\\nexport const applyReview = async ({\\n  octokit,\\n  payload,\\n  review,\\n}: {\\n  octokit: Octokit;\\n  payload: WebhookEventMap[\"pull_request\"];\\n  review: Review;\\n}) => {\\n  let commentPromise = null;\\n  const comment = review.review?.comment;\\n  if (comment != null) {\\n    commentPromise = postGeneralReviewComment(octokit, payload, comment);\\n  }\\n  const suggestionPromises = review.suggestions.map((suggestion) =>\\n    postInlineComment(octokit, payload, suggestion)\\n  );\\n  await Promise.all([\\n    ...(commentPromise ? [commentPromise] : []),\\n    ...suggestionPromises,\\n  ]);\\n};\\n\\nconst addLineNumbers = (contents: string) => {\\n  const rawContents = String.raw`${contents}`;\\n  const prepended = rawContents\\n    .split(\"\\\\n\")\\n    .map((line, idx) => `${idx + 1}: ${line}`)\\n    .join(\"\\\\n\");\\n  return prepended;\\n};\\n\\nexport const getGitFile = async (\\n  octokit: Octokit,\\n  payload: WebhookEventMap[\"issues\"] | WebhookEventMap[\"pull_request\"],\\n  branch: BranchDetails,\\n  filepath: string\\n) => {\\n  try {\\n    const response = await octokit.request(\\n      \"GET /repos/{owner}/{repo}/contents/{path}\",\\n      {\\n        owner: payload.repository.owner.login,\\n        repo: payload.repository.name,\\n        path: filepath,\\n        ref: branch.name, // specify the branch name here\\n        headers: {\\n          \"X-GitHub-Api-Version\": \"2022-11-28\",\\n        },\\n      }\\n    );\\n    //@ts-ignore\\n    const decodedContent = Buffer.from(\\n      response.data.content,\\n      \"base64\"\\n    ).toString(\"utf8\");\\n    //@ts-ignore\\n    return { content: decodedContent, sha: response.data.sha };\\n  } catch (exc) {\\n    if (exc.status === 404) {\\n      return { content: null, sha: null };\\n    }\\n    console.log(exc);\\n    throw exc;\\n  }\\n};\\n\\nexport const getFileContents = async (\\n  octokit: Octokit,\\n  payload: WebhookEventMap[\"issues\"],\\n  branch: BranchDetails,\\n  filepath: string\\n) => {\\n  const gitFile = await getGitFile(\\n    octokit,\\n    payload,\\n    branch,\\n    processGitFilepath(filepath)\\n  );\\n  const fileWithLines = `# ${filepath}\\\\n${addLineNumbers(gitFile.content)}`;\\n  return { result: fileWithLines, functionString: `Opening file: ${filepath}` };\\n};\\n\\nexport const commentIssue = async (\\n  octokit: Octokit,\\n  payload: WebhookEventMap[\"issues\"],\\n  comment: string\\n) => {\\n  await octokit.rest.issues.createComment({\\n    owner: payload.repository.owner.login,\\n    repo: payload.repository.name,\\n    issue_number: payload.issue.number,\\n    body: comment,\\n  });\\n};\\n\\nexport const createBranch = async (\\n  octokit: Octokit,\\n  payload: WebhookEventMap[\"issues\"]\\n) => {\\n  let branchDetails = null;\\n  try {\\n    const title = payload.issue.title.replace(/\\\\s/g, \"-\").substring(0, 15);\\n\\n    const hash = Math.random().toString(36).substring(2, 7);\\n    const subName = `${title}-${hash}`.substring(0, 20);\\n    const branchName = `Code-Bot/${subName}`;\\n    // Get the default branch for the repository\\n    const { data: repo } = await octokit.rest.repos.get({\\n      owner: payload.repository.owner.login,\\n      repo: payload.repository.name,\\n    });\\n\\n    // Get the commit SHA of the default branch\\n    const { data: ref } = await octokit.rest.git.getRef({\\n      owner: payload.repository.owner.login,\\n      repo: payload.repository.name,\\n      ref: `heads/${repo.default_branch}`,\\n    });\\n\\n    // Create a new branch from the commit SHA\\n    const { data: newBranch } = await octokit.rest.git.createRef({\\n      owner: payload.repository.owner.login,\\n      repo: payload.repository.name,\\n      ref: `refs/heads/${branchName}`,\\n      sha: ref.object.sha,\\n    });\\n\\n    console.log(newBranch);\\n\\n    branchDetails = {\\n      name: branchName,\\n      sha: newBranch.object.sha,\\n      url: newBranch.url,\\n    };\\n    let branchUrl = `https://github.com/${payload.repository.owner.login}/${payload.repository.name}/tree/${branchName}`;\\n    const branchComment = `Branch created: [${branchName}](${branchUrl})`;\\n    await commentIssue(octokit, payload, branchComment);\\n\\n    console.log(`Branch ${branchName} created`);\\n  } catch (exc) {\\n    console.log(exc);\\n  }\\n  return branchDetails;\\n};\\n'},\n",
              " {'name': 'src/prompts.ts',\n",
              "  'content': 'import { encode, encodeChat } from \"gpt-tokenizer\";\\nimport type { ChatCompletionMessageParam } from \"groq-sdk/resources/chat/completions\";\\nimport type { PRFile } from \"./constants\";\\nimport {\\n  rawPatchStrategy,\\n  smarterContextPatchStrategy,\\n} from \"./context/review\";\\nimport { GROQ_MODEL, type GroqChatModel } from \"./llms/groq\";\\n\\nconst ModelsToTokenLimits: Record<GroqChatModel, number> = {\\n  \"mixtral-8x7b-32768\": 32768,\\n  \"gemma-7b-it\": 32768,\\n  \"llama3-70b-8192\": 8192,\\n  \"llama3-8b-8192\": 8192,\\n};\\n\\nexport const REVIEW_DIFF_PROMPT = `You are PR-Reviewer, a language model designed to review git pull requests.\\nYour task is to provide constructive and concise feedback for the PR, and also provide meaningful code suggestions.\\n\\nExample PR Diff input:\\n\\'\\n## src/file1.py\\n\\n@@ -12,5 +12,5 @@ def func1():\\ncode line that already existed in the file...\\ncode line that already existed in the file....\\n-code line that was removed in the PR\\n+new code line added in the PR\\n code line that already existed in the file...\\n code line that already existed in the file...\\n\\n@@ ... @@ def func2():\\n...\\n\\n\\n## src/file2.py\\n...\\n\\'\\n\\nThe review should focus on new code added in the PR (lines starting with \\'+\\'), and not on code that already existed in the file (lines starting with \\'-\\', or without prefix).\\n\\n- ONLY PROVIDE CODE SUGGESTIONS\\n- Focus on important suggestions like fixing code problems, improving performance, improving security, improving readability\\n- Avoid making suggestions that have already been implemented in the PR code. For example, if you want to add logs, or change a variable to const, or anything else, make sure it isn\\'t already in the PR code.\\n- Don\\'t suggest adding docstring, type hints, or comments.\\n- Suggestions should focus on improving the new code added in the PR (lines starting with \\'+\\')\\n- Do not say things like without seeing the full repo, or full code, or rest of the codebase. Comment only on the code you have!\\n\\nMake sure the provided code suggestions are in the same programming language.\\n\\nDon\\'t repeat the prompt in the answer, and avoid outputting the \\'type\\' and \\'description\\' fields.\\n\\nThink through your suggestions and make exceptional improvements.`;\\n\\nexport const XML_PR_REVIEW_PROMPT = `As the PR-Reviewer AI model, you are tasked to analyze git pull requests across any programming language and provide comprehensive and precise code enhancements. Keep your focus on the new code modifications indicated by \\'+\\' lines in the PR. Your feedback should hunt for code issues, opportunities for performance enhancement, security improvements, and ways to increase readability. \\n\\nEnsure your suggestions are novel and haven\\'t been previously incorporated in the \\'+\\' lines of the PR code. Refrain from proposing enhancements that add docstrings, type hints, or comments. Your recommendations should strictly target the \\'+\\' lines without suggesting the need for complete context such as the whole repo or codebase.\\n\\nYour code suggestions should match the programming language in the PR, steer clear of needless repetition or inclusion of \\'type\\' and \\'description\\' fields.\\n\\nFormulate thoughtful suggestions aimed at strengthening performance, security, and readability, and represent them in an XML format utilizing the tags: <review>, <code>, <suggestion>, <comment>, <type>, <describe>, <filename>. While multiple recommendations can be given, they should all reside within one <review> tag.\\n\\nAlso note, all your code suggestions should follow the valid Markdown syntax for GitHub, identifying the language they\\'re written in, and should be enclosed within backticks (\\\\`\\\\`\\\\`). \\n\\nDon\\'t hesitate to add as many constructive suggestions as are relevant to really improve the effectivity of the code.\\n\\nExample output:\\n\\\\`\\\\`\\\\`\\n<review>\\n  <suggestion>\\n    <describe>[Objective of the newly incorporated code]</describe>\\n    <type>[Category of the given suggestion such as performance, security, etc.]</type>\\n    <comment>[Guidance on enhancing the new code]</comment>\\n    <code>\\n    \\\\`\\\\`\\\\`[Programming Language]\\n    [Equivalent code amendment in the same language]\\n    \\\\`\\\\`\\\\`\\n    </code>\\n    <filename>[name of relevant file]</filename>\\n  </suggestion>\\n  <suggestion>\\n  ...\\n  </suggestion>\\n  ...\\n</review>\\n\\\\`\\\\`\\\\`\\n\\nNote: The \\'comment\\' and \\'describe\\' tags should elucidate the advice and why it’s given, while the \\'code\\' tag hosts the recommended code snippet within proper GitHub Markdown syntax. The \\'type\\' defines the suggestion\\'s category such as performance, security, readability, etc.`;\\n\\nexport const PR_SUGGESTION_TEMPLATE = `{COMMENT}\\n{ISSUE_LINK}\\n\\n{CODE}\\n`;\\n\\nconst assignLineNumbers = (diff: string) => {\\n  const lines = diff.split(\"\\\\n\");\\n  let newLine = 0;\\n  const lineNumbers = [];\\n\\n  for (const line of lines) {\\n    if (line.startsWith(\"@@\")) {\\n      // This is a chunk header. Parse the line numbers.\\n      const match = line.match(/@@ -\\\\d+,\\\\d+ \\\\+(\\\\d+),\\\\d+ @@/);\\n      newLine = parseInt(match[1]);\\n      lineNumbers.push(line); // keep chunk headers as is\\n    } else if (!line.startsWith(\"-\")) {\\n      // This is a line from the new file.\\n      lineNumbers.push(`${newLine++}: ${line}`);\\n    }\\n  }\\n\\n  return lineNumbers.join(\"\\\\n\");\\n};\\n\\nexport const buildSuggestionPrompt = (file: PRFile) => {\\n  const rawPatch = String.raw`${file.patch}`;\\n  const patchWithLines = assignLineNumbers(rawPatch);\\n  return `## ${file.filename}\\\\n\\\\n${patchWithLines}`;\\n};\\n\\nexport const buildPatchPrompt = (file: PRFile) => {\\n  if (file.old_contents == null) {\\n    return rawPatchStrategy(file);\\n  } else {\\n    return smarterContextPatchStrategy(file);\\n  }\\n};\\n\\nexport const getReviewPrompt = (diff: string): ChatCompletionMessageParam[] => {\\n  return [\\n    { role: \"system\", content: REVIEW_DIFF_PROMPT },\\n    { role: \"user\", content: diff },\\n  ];\\n};\\n\\nexport const getXMLReviewPrompt = (\\n  diff: string\\n): ChatCompletionMessageParam[] => {\\n  return [\\n    { role: \"system\", content: XML_PR_REVIEW_PROMPT },\\n    { role: \"user\", content: diff },\\n  ];\\n};\\n\\nexport const constructPrompt = (\\n  files: PRFile[],\\n  patchBuilder: (file: PRFile) => string,\\n  convoBuilder: (diff: string) => ChatCompletionMessageParam[]\\n) => {\\n  const patches = files.map((file) => patchBuilder(file));\\n  const diff = patches.join(\"\\\\n\");\\n  const convo = convoBuilder(diff);\\n  return convo;\\n};\\n\\nexport const getTokenLength = (blob: string) => {\\n  return encode(blob).length;\\n};\\n\\nexport const isConversationWithinLimit = (\\n  convo: any[],\\n  model: GroqChatModel = GROQ_MODEL\\n) => {\\n  // We don\\'t have the encoder for our Groq model, so we\\'re using\\n  // the one for gpt-3.5-turbo as a rough equivalent.\\n  const convoTokens = encodeChat(convo, \"gpt-3.5-turbo\").length;\\n  return convoTokens < ModelsToTokenLimits[model];\\n};\\n'},\n",
              " {'name': 'src/review-agent.ts',\n",
              "  'content': 'import { Octokit } from \"@octokit/rest\";\\nimport { WebhookEventMap } from \"@octokit/webhooks-definitions/schema\";\\nimport { ChatCompletionMessageParam } from \"groq-sdk/resources/chat/completions\";\\nimport * as xml2js from \"xml2js\";\\nimport type {\\n  BranchDetails,\\n  BuilderResponse,\\n  Builders,\\n  CodeSuggestion,\\n  PRFile,\\n  PRSuggestion,\\n} from \"./constants\";\\nimport { PRSuggestionImpl } from \"./data/PRSuggestionImpl\";\\nimport { generateChatCompletion } from \"./llms/chat\";\\nimport {\\n  PR_SUGGESTION_TEMPLATE,\\n  buildPatchPrompt,\\n  constructPrompt,\\n  getReviewPrompt,\\n  getTokenLength,\\n  getXMLReviewPrompt,\\n  isConversationWithinLimit,\\n} from \"./prompts\";\\nimport {\\n  INLINE_FIX_FUNCTION,\\n  getInlineFixPrompt,\\n} from \"./prompts/inline-prompt\";\\nimport { getGitFile } from \"./reviews\";\\n\\nexport const reviewDiff = async (messages: ChatCompletionMessageParam[]) => {\\n  const message = await generateChatCompletion({\\n    messages,\\n  });\\n  return message.content;\\n};\\n\\nexport const reviewFiles = async (\\n  files: PRFile[],\\n  patchBuilder: (file: PRFile) => string,\\n  convoBuilder: (diff: string) => ChatCompletionMessageParam[]\\n) => {\\n  const patches = files.map((file) => patchBuilder(file));\\n  const messages = convoBuilder(patches.join(\"\\\\n\"));\\n  const feedback = await reviewDiff(messages);\\n  return feedback;\\n};\\n\\nconst filterFile = (file: PRFile) => {\\n  const extensionsToIgnore = new Set<string>([\\n    \"pdf\",\\n    \"png\",\\n    \"jpg\",\\n    \"jpeg\",\\n    \"gif\",\\n    \"mp4\",\\n    \"mp3\",\\n    \"md\",\\n    \"json\",\\n    \"env\",\\n    \"toml\",\\n    \"svg\",\\n  ]);\\n  const filesToIgnore = new Set<string>([\\n    \"package-lock.json\",\\n    \"yarn.lock\",\\n    \".gitignore\",\\n    \"package.json\",\\n    \"tsconfig.json\",\\n    \"poetry.lock\",\\n    \"readme.md\",\\n  ]);\\n  const filename = file.filename.toLowerCase().split(\"/\").pop();\\n  if (filename && filesToIgnore.has(filename)) {\\n    console.log(`Filtering out ignored file: ${file.filename}`);\\n    return false;\\n  }\\n  const splitFilename = file.filename.toLowerCase().split(\".\");\\n  if (splitFilename.length <= 1) {\\n    console.log(`Filtering out file with no extension: ${file.filename}`);\\n    return false;\\n  }\\n  const extension = splitFilename.pop()?.toLowerCase();\\n  if (extension && extensionsToIgnore.has(extension)) {\\n    console.log(`Filtering out file with ignored extension: ${file.filename} (.${extension})`);\\n    return false;\\n  }\\n  return true;\\n};\\n\\nconst groupFilesByExtension = (files: PRFile[]): Map<string, PRFile[]> => {\\n  const filesByExtension: Map<string, PRFile[]> = new Map();\\n\\n  files.forEach((file) => {\\n    const extension = file.filename.split(\".\").pop()?.toLowerCase();\\n    if (extension) {\\n      if (!filesByExtension.has(extension)) {\\n        filesByExtension.set(extension, []);\\n      }\\n      filesByExtension.get(extension)?.push(file);\\n    }\\n  });\\n\\n  return filesByExtension;\\n};\\n\\n// all of the files here can be processed with the prompt at minimum\\nconst processWithinLimitFiles = (\\n  files: PRFile[],\\n  patchBuilder: (file: PRFile) => string,\\n  convoBuilder: (diff: string) => ChatCompletionMessageParam[]\\n) => {\\n  const processGroups: PRFile[][] = [];\\n  const convoWithinModelLimit = isConversationWithinLimit(\\n    constructPrompt(files, patchBuilder, convoBuilder)\\n  );\\n\\n  console.log(`Within model token limits: ${convoWithinModelLimit}`);\\n  if (!convoWithinModelLimit) {\\n    const grouped = groupFilesByExtension(files);\\n    for (const [extension, filesForExt] of grouped.entries()) {\\n      const extGroupWithinModelLimit = isConversationWithinLimit(\\n        constructPrompt(filesForExt, patchBuilder, convoBuilder)\\n      );\\n      if (extGroupWithinModelLimit) {\\n        processGroups.push(filesForExt);\\n      } else {\\n        // extension group exceeds model limit\\n        console.log(\\n          \"Processing files per extension that exceed model limit ...\"\\n        );\\n        let currentGroup: PRFile[] = [];\\n        filesForExt.sort((a, b) => a.patchTokenLength - b.patchTokenLength);\\n        filesForExt.forEach((file) => {\\n          const isPotentialGroupWithinLimit = isConversationWithinLimit(\\n            constructPrompt([...currentGroup, file], patchBuilder, convoBuilder)\\n          );\\n          if (isPotentialGroupWithinLimit) {\\n            currentGroup.push(file);\\n          } else {\\n            processGroups.push(currentGroup);\\n            currentGroup = [file];\\n          }\\n        });\\n        if (currentGroup.length > 0) {\\n          processGroups.push(currentGroup);\\n        }\\n      }\\n    }\\n  } else {\\n    processGroups.push(files);\\n  }\\n  return processGroups;\\n};\\n\\nconst stripRemovedLines = (originalFile: PRFile) => {\\n  // remove lines starting with a \\'-\\'\\n  const originalPatch = String.raw`${originalFile.patch}`;\\n  const strippedPatch = originalPatch\\n    .split(\"\\\\n\")\\n    .filter((line) => !line.startsWith(\"-\"))\\n    .join(\"\\\\n\");\\n  return { ...originalFile, patch: strippedPatch };\\n};\\n\\nconst processOutsideLimitFiles = (\\n  files: PRFile[],\\n  patchBuilder: (file: PRFile) => string,\\n  convoBuilder: (diff: string) => ChatCompletionMessageParam[]\\n) => {\\n  const processGroups: PRFile[][] = [];\\n  if (files.length == 0) {\\n    return processGroups;\\n  }\\n  files = files.map((file) => stripRemovedLines(file));\\n  const convoWithinModelLimit = isConversationWithinLimit(\\n    constructPrompt(files, patchBuilder, convoBuilder)\\n  );\\n  if (convoWithinModelLimit) {\\n    processGroups.push(files);\\n  } else {\\n    const exceedingLimits: PRFile[] = [];\\n    const withinLimits: PRFile[] = [];\\n    files.forEach((file) => {\\n      const isFileConvoWithinLimits = isConversationWithinLimit(\\n        constructPrompt([file], patchBuilder, convoBuilder)\\n      );\\n      if (isFileConvoWithinLimits) {\\n        withinLimits.push(file);\\n      } else {\\n        exceedingLimits.push(file);\\n      }\\n    });\\n    const withinLimitsGroup = processWithinLimitFiles(\\n      withinLimits,\\n      patchBuilder,\\n      convoBuilder\\n    );\\n    withinLimitsGroup.forEach((group) => {\\n      processGroups.push(group);\\n    });\\n    if (exceedingLimits.length > 0) {\\n      console.log(\"TODO: Need to further chunk large file changes.\");\\n      // throw \"Unimplemented\"\\n    }\\n  }\\n  return processGroups;\\n};\\n\\nconst processXMLSuggestions = async (feedbacks: string[]) => {\\n  const xmlParser = new xml2js.Parser();\\n  const parsedSuggestions = await Promise.all(\\n    feedbacks.map((fb) => {\\n      fb = fb\\n        .split(\"<code>\")\\n        .join(\"<code><![CDATA[\")\\n        .split(\"</code>\")\\n        .join(\"]]></code>\");\\n      console.log(fb);\\n      return xmlParser.parseStringPromise(fb);\\n    })\\n  );\\n  // gets suggestion arrays [[suggestion], [suggestion]], then flattens\\n  const allSuggestions = parsedSuggestions\\n    .map((sug) => sug.review.suggestion)\\n    .flat(1);\\n  const suggestions: PRSuggestion[] = allSuggestions.map((rawSuggestion) => {\\n    const lines = rawSuggestion.code[0].trim().split(\"\\\\n\");\\n    lines[0] = lines[0].trim();\\n    lines[lines.length - 1] = lines[lines.length - 1].trim();\\n    const code = lines.join(\"\\\\n\");\\n\\n    return new PRSuggestionImpl(\\n      rawSuggestion.describe[0],\\n      rawSuggestion.type[0],\\n      rawSuggestion.comment[0],\\n      code,\\n      rawSuggestion.filename[0]\\n    );\\n  });\\n  return suggestions;\\n};\\n\\nconst generateGithubIssueUrl = (\\n  owner: string,\\n  repoName: string,\\n  title: string,\\n  body: string,\\n  codeblock?: string\\n) => {\\n  const encodedTitle = encodeURIComponent(title);\\n  const encodedBody = encodeURIComponent(body);\\n  const encodedCodeBlock = codeblock\\n    ? encodeURIComponent(`\\\\n${codeblock}\\\\n`)\\n    : \"\";\\n\\n  let url = `https://github.com/${owner}/${repoName}/issues/new?title=${encodedTitle}&body=${encodedBody}${encodedCodeBlock}`;\\n\\n  if (url.length > 2048) {\\n    url = `https://github.com/${owner}/${repoName}/issues/new?title=${encodedTitle}&body=${encodedBody}`;\\n  }\\n  return `[Create Issue](${url})`;\\n};\\n\\nexport const dedupSuggestions = (\\n  suggestions: PRSuggestion[]\\n): PRSuggestion[] => {\\n  const suggestionsMap = new Map<string, PRSuggestion>();\\n  suggestions.forEach((suggestion) => {\\n    suggestionsMap.set(suggestion.identity(), suggestion);\\n  });\\n  return Array.from(suggestionsMap.values());\\n};\\n\\nconst convertPRSuggestionToComment = (\\n  owner: string,\\n  repo: string,\\n  suggestions: PRSuggestion[]\\n): string[] => {\\n  const suggestionsMap = new Map<string, PRSuggestion[]>();\\n  suggestions.forEach((suggestion) => {\\n    if (!suggestionsMap.has(suggestion.filename)) {\\n      suggestionsMap.set(suggestion.filename, []);\\n    }\\n    suggestionsMap.get(suggestion.filename).push(suggestion);\\n  });\\n  const comments: string[] = [];\\n  for (let [filename, suggestions] of suggestionsMap) {\\n    const temp = [`## ${filename}\\\\n`];\\n    suggestions.forEach((suggestion: PRSuggestion) => {\\n      const issueLink = generateGithubIssueUrl(\\n        owner,\\n        repo,\\n        suggestion.describe,\\n        suggestion.comment,\\n        suggestion.code\\n      );\\n      temp.push(\\n        PR_SUGGESTION_TEMPLATE.replace(\"{COMMENT}\", suggestion.comment)\\n          .replace(\"{CODE}\", suggestion.code)\\n          .replace(\"{ISSUE_LINK}\", issueLink)\\n      );\\n    });\\n    comments.push(temp.join(\"\\\\n\"));\\n  }\\n  return comments;\\n};\\n\\nconst xmlResponseBuilder = async (\\n  owner: string,\\n  repoName: string,\\n  feedbacks: string[]\\n): Promise<BuilderResponse> => {\\n  console.log(\"IN XML RESPONSE BUILDER\");\\n  const parsedXMLSuggestions = await processXMLSuggestions(feedbacks);\\n  const comments = convertPRSuggestionToComment(\\n    owner,\\n    repoName,\\n    dedupSuggestions(parsedXMLSuggestions)\\n  );\\n  const commentBlob = comments.join(\"\\\\n\");\\n  return { comment: commentBlob, structuredComments: parsedXMLSuggestions };\\n};\\n\\nconst curriedXmlResponseBuilder = (owner: string, repoName: string) => {\\n  return (feedbacks: string[]) =>\\n    xmlResponseBuilder(owner, repoName, feedbacks);\\n};\\n\\nconst basicResponseBuilder = async (\\n  feedbacks: string[]\\n): Promise<BuilderResponse> => {\\n  console.log(\"IN BASIC RESPONSE BUILDER\");\\n  const commentBlob = feedbacks.join(\"\\\\n\");\\n  return { comment: commentBlob, structuredComments: [] };\\n};\\n\\nexport const reviewChanges = async (\\n  files: PRFile[],\\n  convoBuilder: (diff: string) => ChatCompletionMessageParam[],\\n  responseBuilder: (responses: string[]) => Promise<BuilderResponse>\\n) => {\\n  const patchBuilder = buildPatchPrompt;\\n  const filteredFiles = files.filter((file) => filterFile(file));\\n  filteredFiles.map((file) => {\\n    file.patchTokenLength = getTokenLength(patchBuilder(file));\\n  });\\n  // further subdivide if necessary, maybe group files by common extension?\\n  const patchesWithinModelLimit: PRFile[] = [];\\n  // these single file patches are larger than the full model context\\n  const patchesOutsideModelLimit: PRFile[] = [];\\n\\n  filteredFiles.forEach((file) => {\\n    const patchWithPromptWithinLimit = isConversationWithinLimit(\\n      constructPrompt([file], patchBuilder, convoBuilder)\\n    );\\n    if (patchWithPromptWithinLimit) {\\n      patchesWithinModelLimit.push(file);\\n    } else {\\n      patchesOutsideModelLimit.push(file);\\n    }\\n  });\\n\\n  console.log(`files within limits: ${patchesWithinModelLimit.length}`);\\n  const withinLimitsPatchGroups = processWithinLimitFiles(\\n    patchesWithinModelLimit,\\n    patchBuilder,\\n    convoBuilder\\n  );\\n  const exceedingLimitsPatchGroups = processOutsideLimitFiles(\\n    patchesOutsideModelLimit,\\n    patchBuilder,\\n    convoBuilder\\n  );\\n  console.log(`${withinLimitsPatchGroups.length} within limits groups.`);\\n  console.log(\\n    `${patchesOutsideModelLimit.length} files outside limit, skipping them.`\\n  );\\n\\n  const groups = [...withinLimitsPatchGroups, ...exceedingLimitsPatchGroups];\\n\\n  const feedbacks = await Promise.all(\\n    groups.map((patchGroup) => {\\n      return reviewFiles(patchGroup, patchBuilder, convoBuilder);\\n    })\\n  );\\n  try {\\n    return await responseBuilder(feedbacks);\\n  } catch (exc) {\\n    console.log(\"XML parsing error\");\\n    console.log(exc);\\n    throw exc;\\n  }\\n};\\n\\nconst indentCodeFix = (\\n  file: string,\\n  code: string,\\n  lineStart: number\\n): string => {\\n  const fileLines = file.split(\"\\\\n\");\\n  const firstLine = fileLines[lineStart - 1];\\n  const codeLines = code.split(\"\\\\n\");\\n  const indentation = firstLine.match(/^(\\\\s*)/)[0];\\n  const indentedCodeLines = codeLines.map((line) => indentation + line);\\n  return indentedCodeLines.join(\"\\\\n\");\\n};\\n\\nconst isCodeSuggestionNew = (\\n  contents: string,\\n  suggestion: CodeSuggestion\\n): boolean => {\\n  const fileLines = contents.split(\"\\\\n\");\\n  const targetLines = fileLines\\n    .slice(suggestion.line_start - 1, suggestion.line_end)\\n    .join(\"\\\\n\");\\n  if (targetLines.trim() == suggestion.correction.trim()) {\\n    // same as existing code.\\n    return false;\\n  }\\n  return true;\\n};\\n\\nexport const generateInlineComments = async (\\n  suggestion: PRSuggestion,\\n  file: PRFile\\n): Promise<CodeSuggestion> => {\\n  try {\\n    const messages = getInlineFixPrompt(file.current_contents, suggestion);\\n    const { function_call } = await generateChatCompletion({\\n      messages,\\n      functions: [INLINE_FIX_FUNCTION],\\n      function_call: { name: INLINE_FIX_FUNCTION.name },\\n    });\\n    if (!function_call) {\\n      throw new Error(\"No function call found\");\\n    }\\n    const args = JSON.parse(function_call.arguments);\\n    const initialCode = String.raw`${args[\"code\"]}`;\\n    const indentedCode = indentCodeFix(\\n      file.current_contents,\\n      initialCode,\\n      args[\"lineStart\"]\\n    );\\n    const codeFix = {\\n      file: suggestion.filename,\\n      line_start: args[\"lineStart\"],\\n      line_end: args[\"lineEnd\"],\\n      correction: indentedCode,\\n      comment: args[\"comment\"],\\n    };\\n    if (isCodeSuggestionNew(file.current_contents, codeFix)) {\\n      return codeFix;\\n    }\\n    return null;\\n  } catch (exc) {\\n    console.log(exc);\\n    return null;\\n  }\\n};\\n\\nconst preprocessFile = async (\\n  octokit: Octokit,\\n  payload: WebhookEventMap[\"pull_request\"],\\n  file: PRFile\\n) => {\\n  const { base, head } = payload.pull_request;\\n  const baseBranch: BranchDetails = {\\n    name: base.ref,\\n    sha: base.sha,\\n    url: payload.pull_request.url,\\n  };\\n  const currentBranch: BranchDetails = {\\n    name: head.ref,\\n    sha: head.sha,\\n    url: payload.pull_request.url,\\n  };\\n  // Handle scenario where file does not exist!!\\n  const [oldContents, currentContents] = await Promise.all([\\n    getGitFile(octokit, payload, baseBranch, file.filename),\\n    getGitFile(octokit, payload, currentBranch, file.filename),\\n  ]);\\n\\n  if (oldContents.content != null) {\\n    file.old_contents = String.raw`${oldContents.content}`;\\n  } else {\\n    file.old_contents = null;\\n  }\\n\\n  if (currentContents.content != null) {\\n    file.current_contents = String.raw`${currentContents.content}`;\\n  } else {\\n    file.current_contents = null;\\n  }\\n};\\n\\nconst reviewChangesRetry = async (files: PRFile[], builders: Builders[]) => {\\n  for (const { convoBuilder, responseBuilder } of builders) {\\n    try {\\n      console.log(`Trying with convoBuilder: ${convoBuilder.name}.`);\\n      return await reviewChanges(files, convoBuilder, responseBuilder);\\n    } catch (error) {\\n      console.log(\\n        `Error with convoBuilder: ${convoBuilder.name}, trying next one. Error: ${error}`\\n      );\\n    }\\n  }\\n  throw new Error(\"All convoBuilders failed.\");\\n};\\n\\nexport const processPullRequest = async (\\n  octokit: Octokit,\\n  payload: WebhookEventMap[\"pull_request\"],\\n  files: PRFile[],\\n  includeSuggestions = false\\n) => {\\n  console.dir({ files }, { depth: null });\\n  const filteredFiles = files.filter((file) => filterFile(file));\\n  console.dir({ filteredFiles }, { depth: null });\\n  if (filteredFiles.length == 0) {\\n    console.log(\"Nothing to comment on, all files were filtered out. The PR Agent does not support the following file types: pdf, png, jpg, jpeg, gif, mp4, mp3, md, json, env, toml, svg, package-lock.json, yarn.lock, .gitignore, package.json, tsconfig.json, poetry.lock, readme.md\");\\n    return {\\n      review: null,\\n      suggestions: [],\\n    };\\n  }\\n  await Promise.all(\\n    filteredFiles.map((file) => {\\n      return preprocessFile(octokit, payload, file);\\n    })\\n  );\\n  const owner = payload.repository.owner.login;\\n  const repoName = payload.repository.name;\\n  const curriedXMLResponseBuilder = curriedXmlResponseBuilder(owner, repoName);\\n  if (includeSuggestions) {\\n    const reviewComments = await reviewChangesRetry(filteredFiles, [\\n      {\\n        convoBuilder: getXMLReviewPrompt,\\n        responseBuilder: curriedXMLResponseBuilder,\\n      },\\n      {\\n        convoBuilder: getReviewPrompt,\\n        responseBuilder: basicResponseBuilder,\\n      },\\n    ]);\\n    let inlineComments: CodeSuggestion[] = [];\\n    if (reviewComments.structuredComments.length > 0) {\\n      console.log(\"STARTING INLINE COMMENT PROCESSING\");\\n      inlineComments = await Promise.all(\\n        reviewComments.structuredComments.map((suggestion) => {\\n          // find relevant file\\n          const file = files.find(\\n            (file) => file.filename === suggestion.filename\\n          );\\n          if (file == null) {\\n            return null;\\n          }\\n          return generateInlineComments(suggestion, file);\\n        })\\n      );\\n    }\\n    const filteredInlineComments = inlineComments.filter(\\n      (comment) => comment !== null\\n    );\\n    return {\\n      review: reviewComments,\\n      suggestions: filteredInlineComments,\\n    };\\n  } else {\\n    const [review] = await Promise.all([\\n      reviewChangesRetry(filteredFiles, [\\n        {\\n          convoBuilder: getXMLReviewPrompt,\\n          responseBuilder: curriedXMLResponseBuilder,\\n        },\\n        {\\n          convoBuilder: getReviewPrompt,\\n          responseBuilder: basicResponseBuilder,\\n        },\\n      ]),\\n    ]);\\n\\n    return {\\n      review,\\n      suggestions: [],\\n    };\\n  }\\n};\\n'},\n",
              " {'name': 'src/constants.ts',\n",
              "  'content': 'import { Node } from \"@babel/traverse\";\\nimport { JavascriptParser } from \"./context/language/javascript-parser\";\\nimport { ChatCompletionMessageParam } from \"groq-sdk/resources/chat/completions\";\\n\\nexport interface PRFile {\\n  sha: string;\\n  filename: string;\\n  status:\\n    | \"added\"\\n    | \"removed\"\\n    | \"renamed\"\\n    | \"changed\"\\n    | \"modified\"\\n    | \"copied\"\\n    | \"unchanged\";\\n  additions: number;\\n  deletions: number;\\n  changes: number;\\n  blob_url: string;\\n  raw_url: string;\\n  contents_url: string;\\n  patch?: string;\\n  previous_filename?: string;\\n  patchTokenLength?: number;\\n  old_contents?: string;\\n  current_contents?: string;\\n}\\n\\nexport interface BuilderResponse {\\n  comment: string;\\n  structuredComments: any[];\\n}\\n\\nexport interface Builders {\\n  convoBuilder: (diff: string) => ChatCompletionMessageParam[];\\n  responseBuilder: (feedbacks: string[]) => Promise<BuilderResponse>;\\n}\\n\\nexport interface PatchInfo {\\n  hunks: {\\n    oldStart: number;\\n    oldLines: number;\\n    newStart: number;\\n    newLines: number;\\n    lines: string[];\\n  }[];\\n}\\n\\nexport interface PRSuggestion {\\n  describe: string;\\n  type: string;\\n  comment: string;\\n  code: string;\\n  filename: string;\\n  toString: () => string;\\n  identity: () => string;\\n}\\n\\nexport interface CodeSuggestion {\\n  file: string;\\n  line_start: number;\\n  line_end: number;\\n  correction: string;\\n  comment: string;\\n}\\n\\nexport interface ChatMessage {\\n  role: string;\\n  content: string;\\n}\\n\\nexport interface Review {\\n  review: BuilderResponse;\\n  suggestions: CodeSuggestion[];\\n}\\n\\nexport interface BranchDetails {\\n  name: string;\\n  sha: string;\\n  url: string;\\n}\\n\\nexport const sleep = async (ms: number) => {\\n  return new Promise((resolve) => setTimeout(resolve, ms));\\n};\\n\\nexport const processGitFilepath = (filepath: string) => {\\n  // Remove the leading \\'/\\' if it exists\\n  return filepath.startsWith(\"/\") ? filepath.slice(1) : filepath;\\n};\\n\\nexport interface EnclosingContext {\\n  enclosingContext: Node | null;\\n}\\n\\nexport interface AbstractParser {\\n  findEnclosingContext(\\n    file: string,\\n    lineStart: number,\\n    lineEnd: number\\n  ): EnclosingContext;\\n  dryRun(file: string): { valid: boolean; error: string };\\n}\\n\\nconst EXTENSIONS_TO_PARSERS: Map<string, AbstractParser> = new Map([\\n  [\"ts\", new JavascriptParser()],\\n  [\"tsx\", new JavascriptParser()],\\n  [\"js\", new JavascriptParser()],\\n  [\"jsx\", new JavascriptParser()],\\n]);\\n\\nexport const getParserForExtension = (filename: string) => {\\n  const fileExtension = filename.split(\".\").pop().toLowerCase();\\n  return EXTENSIONS_TO_PARSERS.get(fileExtension) || null;\\n};\\n\\nexport const assignLineNumbers = (contents: string): string => {\\n  const lines = contents.split(\"\\\\n\");\\n  let lineNumber = 1;\\n  const linesWithNumbers = lines.map((line) => {\\n    const numberedLine = `${lineNumber}: ${line}`;\\n    lineNumber++;\\n    return numberedLine;\\n  });\\n  return linesWithNumbers.join(\"\\\\n\");\\n};\\n'},\n",
              " {'name': 'src/env.ts',\n",
              "  'content': 'import * as dotenv from \"dotenv\";\\nimport { createPrivateKey } from \"crypto\";\\nimport chalk from \"chalk\";\\n\\ndotenv.config();\\n\\nexport const env = {\\n  GITHUB_APP_ID: process.env.GITHUB_APP_ID,\\n  GITHUB_PRIVATE_KEY: process.env.GITHUB_PRIVATE_KEY,\\n  GITHUB_WEBHOOK_SECRET: process.env.GITHUB_WEBHOOK_SECRET,\\n  GROQ_API_KEY: process.env.GROQ_API_KEY,\\n} as const;\\n\\nlet valid = true;\\n\\nfor (const key in env) {\\n  if (!env[key as keyof typeof env]) {\\n    console.log(\\n      chalk.red(\"✖\") +\\n        chalk.gray(\" Missing required env var: \") +\\n        chalk.bold(`process.env.${key}`)\\n    );\\n    valid = false;\\n  }\\n}\\n\\ntry {\\n  createPrivateKey(env.GITHUB_PRIVATE_KEY);\\n} catch (error) {\\n  console.log(\\n    chalk.red(\\n      \"\\\\n✖ Invalid GitHub private key format for \" +\\n        chalk.bold(`process.env.GITHUB_PRIVATE_KEY`) +\\n        \"\\\\n\"\\n    ) +\\n      chalk.gray(\"  • Must start with: \") +\\n      chalk.bold(\"-----BEGIN RSA PRIVATE KEY-----\\\\n\") +\\n      chalk.gray(\"  • Must end with:   \") +\\n      chalk.bold(\"-----END RSA PRIVATE KEY-----\\\\n\")\\n  );\\n  valid = false;\\n}\\n\\nif (!valid) {\\n  console.log(\\n    chalk.yellow(\"\\\\n⚠ \") +\\n      chalk.bold(\"Please check your .env file and try again.\\\\n\")\\n  );\\n  process.exit(1);\\n}\\n'},\n",
              " {'name': 'src/prompts/inline-prompt.ts',\n",
              "  'content': 'import { ChatCompletionMessageParam } from \"groq-sdk/resources/chat/completions\";\\nimport { PRSuggestion } from \"../constants\";\\n\\nexport const INLINE_FIX_PROMPT = `In this task, you are provided with a code suggestion in XML format, along with the corresponding file content. Your task is to radiate from this suggestion and draft a precise code fix. Here\\'s how your input will look:\\n\\n\\\\`\\\\`\\\\`xml\\n  <suggestion>\\n    <describe>Your Description Here</describe>\\n    <type>Your Type Here</type>\\n    <comment>Your Suggestions Here</comment>\\n    <code>Original Code Here</code>\\n    <filename>File Name Here</filename>\\n  </suggestion>\\n\\\\`\\\\`\\\\`\\n\\n{file}\\n\\nThe \\'comment\\' field contains specific code modification instructions. Based on these instructions, you\\'re required to formulate a precise code fix. Bear in mind that the fix must include only the lines between the starting line (linestart) and ending line (lineend) where the changes are applied.\\n\\nThe adjusted code doesn\\'t necessarily need to be standalone valid code, but when incorporated into the corresponding file, it must result in valid, functional code, without errors. Ensure to include only the specific lines affected by the modifications. Avoid including placeholders such as \\'rest of code...\\'\\n\\nPlease interpret the given directions and apply the necessary changes to the provided suggestion and file content. Make the modifications unambiguous and appropriate for utilizing in an inline suggestion on GitHub.`;\\n\\nexport const INLINE_FIX_FUNCTION = {\\n  name: \"fix\",\\n  description: \"The code fix to address the suggestion and rectify the issue\",\\n  parameters: {\\n    type: \"object\",\\n    properties: {\\n      comment: {\\n        type: \"string\",\\n        description: \"Why this change improves the code\",\\n      },\\n      code: {\\n        type: \"string\",\\n        description: \"Modified Code Snippet\",\\n      },\\n      lineStart: {\\n        type: \"number\",\\n        description: \"Starting Line Number\",\\n      },\\n      lineEnd: {\\n        type: \"number\",\\n        description: \"Ending Line Number\",\\n      },\\n    },\\n  },\\n  required: [\"action\"],\\n};\\n\\nconst INLINE_USER_MESSAGE_TEMPLATE = `{SUGGESTION}\\n\\n{FILE}`;\\n\\nconst assignFullLineNumers = (contents: string): string => {\\n  const lines = contents.split(\"\\\\n\");\\n  let lineNumber = 1;\\n  const linesWithNumbers = lines.map((line) => {\\n    const numberedLine = `${lineNumber}: ${line}`;\\n    lineNumber++;\\n    return numberedLine;\\n  });\\n  return linesWithNumbers.join(\"\\\\n\");\\n};\\n\\nexport const getInlineFixPrompt = (\\n  fileContents: string,\\n  suggestion: PRSuggestion\\n): ChatCompletionMessageParam[] => {\\n  const userMessage = INLINE_USER_MESSAGE_TEMPLATE.replace(\\n    \"{SUGGESTION}\",\\n    suggestion.toString()\\n  ).replace(\"{FILE}\", assignFullLineNumers(fileContents));\\n  return [\\n    { role: \"system\", content: INLINE_FIX_PROMPT },\\n    { role: \"user\", content: userMessage },\\n  ];\\n};\\n'},\n",
              " {'name': 'src/llms/groq.ts',\n",
              "  'content': 'import { Groq } from \"groq-sdk\";\\nimport { env } from \"../env\";\\nimport { ChatCompletionCreateParamsBase } from \"groq-sdk/resources/chat/completions\";\\n\\nexport const groq = new Groq({\\n  apiKey: env.GROQ_API_KEY,\\n});\\n\\nexport type GroqChatModel = ChatCompletionCreateParamsBase[\"model\"];\\n\\nexport const GROQ_MODEL: GroqChatModel = \"mixtral-8x7b-32768\";\\n'},\n",
              " {'name': 'src/llms/chat.ts',\n",
              "  'content': 'import { ChatCompletionCreateParamsNonStreaming } from \"groq-sdk/resources/chat/completions\";\\nimport { groq, GROQ_MODEL } from \"./groq\";\\n\\nexport const generateChatCompletion = async (\\n  options: Omit<ChatCompletionCreateParamsNonStreaming, \"model\">\\n) => {\\n  const response = await groq.chat.completions.create({\\n    model: GROQ_MODEL,\\n    temperature: 0,\\n    ...options,\\n  });\\n  return response.choices[0].message;\\n};\\n'},\n",
              " {'name': 'src/context/review.ts',\n",
              "  'content': 'import {\\n  AbstractParser,\\n  PRFile,\\n  PatchInfo,\\n  getParserForExtension,\\n} from \"../constants\";\\nimport * as diff from \"diff\";\\nimport { JavascriptParser } from \"./language/javascript-parser\";\\nimport { Node } from \"@babel/traverse\";\\n\\nconst expandHunk = (\\n  contents: string,\\n  hunk: diff.Hunk,\\n  linesAbove: number = 5,\\n  linesBelow: number = 5\\n) => {\\n  const fileLines = contents.split(\"\\\\n\");\\n  const curExpansion: string[] = [];\\n  const start = Math.max(0, hunk.oldStart - 1 - linesAbove);\\n  const end = Math.min(\\n    fileLines.length,\\n    hunk.oldStart - 1 + hunk.oldLines + linesBelow\\n  );\\n\\n  for (let i = start; i < hunk.oldStart - 1; i++) {\\n    curExpansion.push(fileLines[i]);\\n  }\\n\\n  curExpansion.push(\\n    `@@ -${hunk.oldStart},${hunk.oldLines} +${hunk.newStart},${hunk.newLines} @@`\\n  );\\n  hunk.lines.forEach((line) => {\\n    if (!curExpansion.includes(line)) {\\n      curExpansion.push(line);\\n    }\\n  });\\n\\n  for (let i = hunk.oldStart - 1 + hunk.oldLines; i < end; i++) {\\n    curExpansion.push(fileLines[i]);\\n  }\\n  return curExpansion.join(\"\\\\n\");\\n};\\n\\nconst expandFileLines = (\\n  file: PRFile,\\n  linesAbove: number = 5,\\n  linesBelow: number = 5\\n) => {\\n  const fileLines = file.old_contents.split(\"\\\\n\");\\n  const patches: PatchInfo[] = diff.parsePatch(file.patch);\\n  const expandedLines: string[][] = [];\\n  patches.forEach((patch) => {\\n    patch.hunks.forEach((hunk) => {\\n      const curExpansion: string[] = [];\\n      const start = Math.max(0, hunk.oldStart - 1 - linesAbove);\\n      const end = Math.min(\\n        fileLines.length,\\n        hunk.oldStart - 1 + hunk.oldLines + linesBelow\\n      );\\n\\n      for (let i = start; i < hunk.oldStart - 1; i++) {\\n        curExpansion.push(fileLines[i]);\\n      }\\n\\n      curExpansion.push(\\n        `@@ -${hunk.oldStart},${hunk.oldLines} +${hunk.newStart},${hunk.newLines} @@`\\n      );\\n      hunk.lines.forEach((line) => {\\n        if (!curExpansion.includes(line)) {\\n          curExpansion.push(line);\\n        }\\n      });\\n\\n      for (let i = hunk.oldStart - 1 + hunk.oldLines; i < end; i++) {\\n        curExpansion.push(fileLines[i]);\\n      }\\n      expandedLines.push(curExpansion);\\n    });\\n  });\\n\\n  return expandedLines;\\n};\\n\\nexport const expandedPatchStrategy = (file: PRFile) => {\\n  const expandedPatches = expandFileLines(file);\\n  const expansions = expandedPatches\\n    .map((patchLines) => patchLines.join(\"\\\\n\"))\\n    .join(\"\\\\n\\\\n\");\\n  return `## ${file.filename}\\\\n\\\\n${expansions}`;\\n};\\n\\nexport const rawPatchStrategy = (file: PRFile) => {\\n  return `## ${file.filename}\\\\n\\\\n${file.patch}`;\\n};\\n\\nconst trimHunk = (hunk: diff.Hunk): diff.Hunk => {\\n  const startIdx = hunk.lines.findIndex(\\n    (line) => line.startsWith(\"+\") || line.startsWith(\"-\")\\n  );\\n  const endIdx = hunk.lines\\n    .slice()\\n    .reverse()\\n    .findIndex((line) => line.startsWith(\"+\") || line.startsWith(\"-\"));\\n  const editLines = hunk.lines.slice(startIdx, hunk.lines.length - endIdx);\\n  return { ...hunk, lines: editLines, newStart: startIdx + hunk.newStart };\\n};\\n\\nconst buildingScopeString = (\\n  currentFile: string,\\n  scope: Node,\\n  hunk: diff.Hunk\\n) => {\\n  const res: string[] = [];\\n  const trimmedHunk = trimHunk(hunk);\\n  const functionStartLine = scope.loc.start.line;\\n  const functionEndLine = scope.loc.end.line;\\n  const updatedFileLines = currentFile.split(\"\\\\n\");\\n  // Extract the lines of the function\\n  const functionContext = updatedFileLines.slice(\\n    functionStartLine - 1,\\n    functionEndLine\\n  );\\n  // Calculate the index where the changes should be injected into the function\\n  const injectionIdx =\\n    hunk.newStart -\\n    functionStartLine +\\n    hunk.lines.findIndex(\\n      (line) => line.startsWith(\"+\") || line.startsWith(\"-\")\\n    );\\n  // Count the number of lines that should be dropped from the function\\n  const dropCount = trimmedHunk.lines.filter(\\n    (line) => !line.startsWith(\"-\")\\n  ).length;\\n\\n  const hunkHeader = `@@ -${hunk.oldStart},${hunk.oldLines} +${hunk.newStart},${hunk.newLines} @@`;\\n  // Inject the changes into the function, dropping the necessary lines\\n  functionContext.splice(injectionIdx, dropCount, ...trimmedHunk.lines);\\n\\n  res.push(functionContext.join(\"\\\\n\"));\\n  res.unshift(hunkHeader);\\n  return res;\\n};\\n\\n/*\\nline nums are 0 index, file is 1 index\\n*/\\nconst combineHunks = (\\n  file: string,\\n  overlappingHunks: diff.Hunk[]\\n): diff.Hunk => {\\n  if (!overlappingHunks || overlappingHunks.length === 0) {\\n    throw \"Overlapping hunks are empty, this should never happen.\";\\n  }\\n  const sortedHunks = overlappingHunks.sort((a, b) => a.newStart - b.newStart);\\n  const fileLines = file.split(\"\\\\n\");\\n  let lastHunkEnd = sortedHunks[0].newStart + sortedHunks[0].newLines;\\n\\n  const combinedHunk: diff.Hunk = {\\n    oldStart: sortedHunks[0].oldStart,\\n    oldLines: sortedHunks[0].oldLines,\\n    newStart: sortedHunks[0].newStart,\\n    newLines: sortedHunks[0].newLines,\\n    lines: [...sortedHunks[0].lines],\\n    linedelimiters: [...sortedHunks[0].linedelimiters],\\n  };\\n\\n  for (let i = 1; i < sortedHunks.length; i++) {\\n    const hunk = sortedHunks[i];\\n\\n    // If there\\'s a gap between the last hunk and this one, add the lines in between\\n    if (hunk.newStart > lastHunkEnd) {\\n      combinedHunk.lines.push(\\n        ...fileLines.slice(lastHunkEnd - 1, hunk.newStart - 1)\\n      );\\n      combinedHunk.newLines += hunk.newStart - lastHunkEnd;\\n    }\\n\\n    combinedHunk.oldLines += hunk.oldLines;\\n    combinedHunk.newLines += hunk.newLines;\\n    combinedHunk.lines.push(...hunk.lines);\\n    combinedHunk.linedelimiters.push(...hunk.linedelimiters);\\n\\n    lastHunkEnd = hunk.newStart + hunk.newLines;\\n  }\\n  return combinedHunk;\\n};\\n\\nconst diffContextPerHunk = (file: PRFile, parser: AbstractParser) => {\\n  const updatedFile = diff.applyPatch(file.old_contents, file.patch);\\n  const patches = diff.parsePatch(file.patch);\\n  if (!updatedFile || typeof updatedFile !== \"string\") {\\n    console.log(\"APPLYING PATCH ERROR - FALLINGBACK\");\\n    throw \"THIS SHOULD NOT HAPPEN!\";\\n  }\\n\\n  const hunks: diff.Hunk[] = [];\\n  const order: number[] = [];\\n  const scopeRangeHunkMap = new Map<string, diff.Hunk[]>();\\n  const scopeRangeNodeMap = new Map<string, Node>();\\n  const expandStrategy: diff.Hunk[] = [];\\n\\n  patches.forEach((p) => {\\n    p.hunks.forEach((hunk) => {\\n      hunks.push(hunk);\\n    });\\n  });\\n\\n  hunks.forEach((hunk, idx) => {\\n    try {\\n      const trimmedHunk = trimHunk(hunk);\\n      const insertions = hunk.lines.filter((line) =>\\n        line.startsWith(\"+\")\\n      ).length;\\n      const lineStart = trimmedHunk.newStart;\\n      const lineEnd = lineStart + insertions;\\n      const largestEnclosingFunction = parser.findEnclosingContext(\\n        updatedFile,\\n        lineStart,\\n        lineEnd\\n      ).enclosingContext;\\n\\n      if (largestEnclosingFunction) {\\n        const enclosingRangeKey = `${largestEnclosingFunction.loc.start.line} -> ${largestEnclosingFunction.loc.end.line}`;\\n        let existingHunks = scopeRangeHunkMap.get(enclosingRangeKey) || [];\\n        existingHunks.push(hunk);\\n        scopeRangeHunkMap.set(enclosingRangeKey, existingHunks);\\n        scopeRangeNodeMap.set(enclosingRangeKey, largestEnclosingFunction);\\n      } else {\\n        throw \"No enclosing function.\";\\n      }\\n      order.push(idx);\\n    } catch (exc) {\\n      console.log(file.filename);\\n      console.log(\"NORMAL STRATEGY\");\\n      console.log(exc);\\n      expandStrategy.push(hunk);\\n      order.push(idx);\\n    }\\n  });\\n\\n  const scopeStategy: [string, diff.Hunk][] = []; // holds map range key and combined hunk: [[key, hunk]]\\n  for (const [range, hunks] of scopeRangeHunkMap.entries()) {\\n    const combinedHunk = combineHunks(updatedFile, hunks);\\n    scopeStategy.push([range, combinedHunk]);\\n  }\\n\\n  const contexts: string[] = [];\\n  scopeStategy.forEach(([rangeKey, hunk]) => {\\n    const context = buildingScopeString(\\n      updatedFile,\\n      scopeRangeNodeMap.get(rangeKey),\\n      hunk\\n    ).join(\"\\\\n\");\\n    contexts.push(context);\\n  });\\n  expandStrategy.forEach((hunk) => {\\n    const context = expandHunk(file.old_contents, hunk);\\n    contexts.push(context);\\n  });\\n  return contexts;\\n};\\n\\nconst functionContextPatchStrategy = (\\n  file: PRFile,\\n  parser: AbstractParser\\n): string => {\\n  let res = null;\\n  try {\\n    const contextChunks = diffContextPerHunk(file, parser);\\n    res = `## ${file.filename}\\\\n\\\\n${contextChunks.join(\"\\\\n\\\\n\")}`;\\n  } catch (exc) {\\n    console.log(exc);\\n    res = expandedPatchStrategy(file);\\n  }\\n  return res;\\n};\\n\\nexport const smarterContextPatchStrategy = (file: PRFile) => {\\n  const parser: AbstractParser = getParserForExtension(file.filename);\\n  if (parser != null) {\\n    return functionContextPatchStrategy(file, parser);\\n  } else {\\n    return expandedPatchStrategy(file);\\n  }\\n};\\n'},\n",
              " {'name': 'src/context/language/python-parser.ts',\n",
              "  'content': 'import { AbstractParser, EnclosingContext } from \"../../constants\";\\nexport class PythonParser implements AbstractParser {\\n  findEnclosingContext(\\n    file: string,\\n    lineStart: number,\\n    lineEnd: number\\n  ): EnclosingContext {\\n    // TODO: Implement this method for Python\\n    return null;\\n  }\\n  dryRun(file: string): { valid: boolean; error: string } {\\n    // TODO: Implement this method for Python\\n    return { valid: false, error: \"Not implemented yet\" };\\n  }\\n}\\n'},\n",
              " {'name': 'src/context/language/javascript-parser.ts',\n",
              "  'content': 'import { AbstractParser, EnclosingContext } from \"../../constants\";\\nimport * as parser from \"@babel/parser\";\\nimport traverse, { NodePath, Node } from \"@babel/traverse\";\\n\\nconst processNode = (\\n  path: NodePath<Node>,\\n  lineStart: number,\\n  lineEnd: number,\\n  largestSize: number,\\n  largestEnclosingContext: Node | null\\n) => {\\n  const { start, end } = path.node.loc;\\n  if (start.line <= lineStart && lineEnd <= end.line) {\\n    const size = end.line - start.line;\\n    if (size > largestSize) {\\n      largestSize = size;\\n      largestEnclosingContext = path.node;\\n    }\\n  }\\n  return { largestSize, largestEnclosingContext };\\n};\\n\\nexport class JavascriptParser implements AbstractParser {\\n  findEnclosingContext(\\n    file: string,\\n    lineStart: number,\\n    lineEnd: number\\n  ): EnclosingContext {\\n    const ast = parser.parse(file, {\\n      sourceType: \"module\",\\n      plugins: [\"jsx\", \"typescript\"], // To allow JSX and TypeScript\\n    });\\n    let largestEnclosingContext: Node = null;\\n    let largestSize = 0;\\n    traverse(ast, {\\n      Function(path) {\\n        ({ largestSize, largestEnclosingContext } = processNode(\\n          path,\\n          lineStart,\\n          lineEnd,\\n          largestSize,\\n          largestEnclosingContext\\n        ));\\n      },\\n      TSInterfaceDeclaration(path) {\\n        ({ largestSize, largestEnclosingContext } = processNode(\\n          path,\\n          lineStart,\\n          lineEnd,\\n          largestSize,\\n          largestEnclosingContext\\n        ));\\n      },\\n    });\\n    return {\\n      enclosingContext: largestEnclosingContext,\\n    } as EnclosingContext;\\n  }\\n\\n  dryRun(file: string): { valid: boolean; error: string } {\\n    try {\\n      const ast = parser.parse(file, {\\n        sourceType: \"module\",\\n        plugins: [\"jsx\", \"typescript\"], // To allow JSX and TypeScript\\n      });\\n      return {\\n        valid: true,\\n        error: \"\",\\n      };\\n    } catch (exc) {\\n      return {\\n        valid: false,\\n        error: exc,\\n      };\\n    }\\n  }\\n}\\n'},\n",
              " {'name': 'src/data/PRSuggestionImpl.ts',\n",
              "  'content': 'import { PRSuggestion } from \"../constants\";\\n\\nexport class PRSuggestionImpl implements PRSuggestion {\\n  describe: string;\\n  type: string;\\n  comment: string;\\n  code: string;\\n  filename: string;\\n\\n  constructor(\\n    describe: string,\\n    type: string,\\n    comment: string,\\n    code: string,\\n    filename: string\\n  ) {\\n    this.describe = describe;\\n    this.type = type;\\n    this.comment = comment;\\n    this.code = code;\\n    this.filename = filename;\\n  }\\n\\n  toString(): string {\\n    const xmlElements = [\\n      `<suggestion>`,\\n      `  <describe>${this.describe}</describe>`,\\n      `  <type>${this.type}</type>`,\\n      `  <comment>${this.comment}</comment>`,\\n      `  <code>${this.code}</code>`,\\n      `  <filename>${this.filename}</filename>`,\\n      `</suggestion>`,\\n    ];\\n    return xmlElements.join(\"\\\\n\");\\n  }\\n\\n  identity(): string {\\n    return `${this.filename}:${this.comment}`;\\n  }\\n}\\n'}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "files_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTHEOUgp1Nmv"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pRz7UnvJoL-d"
      },
      "outputs": [],
      "source": [
        "def get_huggingface_embeddings(text, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    return model.encode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "oe-7UwHGvCno"
      },
      "outputs": [],
      "source": [
        "text = \"I like running\"\n",
        "\n",
        "embedding = get_huggingface_embeddings(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "s8W8qQxr-Xfi",
        "outputId": "11612c3a-a971-437a-c937-1b6ba85caa67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-7.04788268e-02,  1.94333866e-02, -1.86341424e-02,  4.69544753e-02,\n",
              "        3.61323655e-02,  5.14774658e-02, -9.41505358e-02,  1.51251704e-02,\n",
              "       -2.15133047e-03, -1.39002576e-02,  1.19065330e-03,  9.69313737e-03,\n",
              "        1.96244307e-02,  1.96997877e-02,  1.16340872e-02, -5.16801290e-02,\n",
              "       -4.76255380e-02, -4.00664844e-03,  9.41014290e-03, -1.29923569e-02,\n",
              "        8.50989018e-05,  7.24124210e-03, -8.99291970e-03, -6.52526096e-02,\n",
              "       -1.28775183e-02, -1.80495866e-02,  3.14601250e-02, -4.64244150e-02,\n",
              "        1.11613853e-03,  6.29375875e-02, -3.89754698e-02, -5.87115176e-02,\n",
              "        2.55934969e-02, -5.96262142e-03,  1.46683931e-06,  1.16431098e-02,\n",
              "       -2.85385642e-03,  2.02579331e-02,  2.59376634e-02, -1.00705689e-02,\n",
              "        6.89758211e-02, -3.88326054e-03, -3.99554819e-02,  7.85138924e-03,\n",
              "       -6.83246106e-02,  5.25738969e-02,  5.47029823e-02, -4.41773087e-02,\n",
              "       -2.81809531e-02,  1.19776474e-02, -1.52425775e-02,  2.51070913e-02,\n",
              "       -7.15806559e-02, -2.06886306e-02,  1.10534579e-02,  3.56975459e-02,\n",
              "        7.63529865e-03, -3.18390131e-02,  8.77301171e-02,  2.07744222e-02,\n",
              "        8.81027710e-03, -6.22895807e-02, -5.59348054e-03,  2.38923468e-02,\n",
              "        3.81694082e-03,  1.63814723e-02,  4.06055525e-02,  9.60358302e-04,\n",
              "        2.21771747e-02, -4.41919360e-03, -4.47414964e-02, -7.86486827e-03,\n",
              "        2.21529324e-02,  8.46277270e-03,  7.27370149e-03, -3.97236906e-02,\n",
              "       -3.57103744e-03,  5.87973483e-02,  1.15466444e-02, -7.72484951e-03,\n",
              "        3.45609672e-02,  6.33578971e-02,  1.21478708e-02,  5.62698906e-03,\n",
              "       -1.09443394e-03, -8.16466380e-03,  7.41912425e-03, -3.72232832e-02,\n",
              "        5.18994927e-02,  4.52220663e-02, -1.46914897e-02, -8.51786695e-03,\n",
              "        6.27890404e-04, -7.05070607e-03, -1.70562807e-02, -7.77456770e-03,\n",
              "       -3.20316479e-02, -5.99154877e-03,  2.68312059e-02, -8.12638830e-03,\n",
              "        1.82095226e-02,  1.07036335e-02,  6.32183347e-03, -8.27492913e-04,\n",
              "       -7.05016311e-04, -4.34801728e-02,  1.52780619e-02, -3.01753748e-02,\n",
              "       -2.91535202e-02,  1.57784950e-02, -7.81678036e-03, -1.99067593e-02,\n",
              "        3.37998718e-02,  7.04375654e-02, -7.50351176e-02,  4.12706332e-03,\n",
              "       -5.25229014e-02,  4.45351899e-02,  2.37590377e-03, -2.12836899e-02,\n",
              "       -5.46411090e-02, -2.50012167e-02,  2.27824450e-02,  3.67899761e-02,\n",
              "        1.99967232e-02, -4.48204465e-02, -2.33086701e-02,  5.79815470e-02,\n",
              "       -2.66358256e-02, -2.31645219e-02, -2.88926847e-02, -1.29380135e-03,\n",
              "       -9.71604418e-03, -3.36333662e-02,  4.99516539e-02,  5.40098117e-04,\n",
              "       -1.48069288e-03, -2.53484976e-02,  6.69857161e-03, -4.18751203e-02,\n",
              "        1.00977942e-02, -5.82159832e-02,  3.47055774e-03, -2.08524764e-02,\n",
              "        7.55150290e-03, -1.33914221e-02,  8.78425222e-03,  1.82720162e-02,\n",
              "       -1.89119857e-02, -3.07431091e-02,  2.24920381e-02,  3.80206928e-02,\n",
              "       -1.39213735e-02, -1.17687881e-02,  2.85458546e-02, -1.30993081e-02,\n",
              "        6.73169792e-02,  2.93340851e-02,  1.86988078e-02,  6.58536982e-03,\n",
              "        1.78831480e-02, -7.24356575e-03, -4.35241535e-02, -7.47614540e-03,\n",
              "       -2.52659023e-02, -4.55647372e-02,  5.96143045e-02, -1.44935763e-02,\n",
              "        2.02233940e-02,  1.80877873e-03,  6.88484833e-02, -8.45475197e-02,\n",
              "       -4.82157841e-02,  2.24887114e-02,  5.91600165e-02,  4.69720066e-02,\n",
              "        2.03517228e-02, -1.52107058e-02, -2.33864915e-02, -8.59186724e-02,\n",
              "       -1.78992841e-02, -2.53199302e-02,  5.41228168e-02, -2.23989245e-02,\n",
              "        1.36091104e-02,  1.81144625e-02, -6.57477975e-03,  6.31625131e-02,\n",
              "       -4.40965965e-02, -3.56280617e-02,  1.57912783e-02, -3.67900776e-03,\n",
              "        6.39739931e-02, -8.92088108e-04,  2.14736927e-02,  2.42294017e-02,\n",
              "        4.04700562e-02, -1.70990955e-02,  4.59530838e-02, -4.53464082e-03,\n",
              "       -8.30866210e-03,  2.21399572e-02, -4.04494889e-02, -1.47972570e-03,\n",
              "       -4.18259464e-02, -3.91691811e-02,  3.98955774e-03,  2.02213228e-02,\n",
              "        7.80331865e-02, -1.57180522e-02,  2.39106994e-02, -5.45100588e-03,\n",
              "        6.62764758e-02,  1.89877655e-02,  1.25487074e-02, -3.69676761e-02,\n",
              "        3.60203385e-02, -3.20824198e-02,  5.53362945e-04,  5.30735105e-02,\n",
              "       -2.35245824e-02,  1.96409598e-02,  1.68137196e-02,  5.73650971e-02,\n",
              "       -4.42413688e-02, -1.13903363e-04,  5.48365973e-02,  4.71885130e-02,\n",
              "        5.92107698e-02, -1.67959053e-02,  5.48425782e-03, -2.62896586e-02,\n",
              "        8.40416364e-03, -6.82736933e-03,  1.56520624e-02,  1.05132200e-02,\n",
              "        5.31163700e-02, -3.50911953e-02, -5.91810187e-03,  1.39760366e-02,\n",
              "        4.96762358e-02,  1.38504319e-02,  9.55141056e-03,  2.34698411e-02,\n",
              "       -5.01801353e-03, -2.09189840e-02,  1.69525780e-02,  2.91200317e-02,\n",
              "       -4.69824225e-02,  2.29986869e-02, -4.90644462e-02, -2.34395713e-02,\n",
              "        2.09469534e-02,  4.89134826e-02,  9.32871457e-03,  1.74086746e-02,\n",
              "       -4.05121706e-02, -3.95299755e-02, -3.48175950e-02, -6.98427483e-02,\n",
              "        9.11881179e-02, -1.56271607e-02, -4.68986556e-02,  3.18024084e-02,\n",
              "        2.51773708e-02,  1.51930191e-02, -1.16601214e-02, -8.27602856e-03,\n",
              "       -1.49055086e-02, -2.19998453e-02,  1.70183126e-02,  5.34977997e-03,\n",
              "        3.98756415e-02,  1.99717134e-02,  6.11203862e-03,  4.97207977e-02,\n",
              "       -5.73114865e-03, -2.10969076e-02, -2.00946219e-02,  8.57996847e-03,\n",
              "       -1.19707836e-02,  4.28173244e-02,  1.15936510e-02,  3.56728323e-02,\n",
              "        2.75478791e-02, -3.54828835e-02, -8.47323681e-04,  1.86554727e-03,\n",
              "        4.26523685e-02, -8.30126461e-04,  9.36400983e-03,  3.42154391e-02,\n",
              "       -2.38540694e-02,  4.15415643e-03,  1.64184789e-03, -6.63760025e-03,\n",
              "       -3.91316190e-02, -2.37366185e-02, -3.09604071e-02, -6.33870661e-02,\n",
              "        1.33938976e-02, -2.80302111e-02,  3.45134665e-03,  2.15655323e-02,\n",
              "        9.04735830e-03,  5.34532266e-03, -2.09043063e-02, -3.58208129e-03,\n",
              "       -1.42474538e-02,  4.94647510e-02,  3.89120989e-02, -1.91171858e-02,\n",
              "       -2.20483243e-02, -7.44958296e-02,  4.43464443e-02,  6.02065548e-02,\n",
              "       -4.59768053e-04,  1.29242009e-03,  3.09064202e-02,  2.20488142e-02,\n",
              "       -1.25720873e-02,  6.83796708e-04,  1.14433449e-02,  1.23446425e-02,\n",
              "       -2.86014881e-02, -3.40075195e-02, -1.53420186e-02, -1.70094166e-02,\n",
              "        2.94025503e-02, -6.36860216e-03, -1.25333006e-02, -7.33178167e-04,\n",
              "        7.23030493e-02,  4.87555154e-02, -1.73591971e-02, -3.68770584e-02,\n",
              "       -3.02025415e-02,  1.40584316e-02, -1.67602915e-02, -6.48408383e-03,\n",
              "        3.85595784e-02, -2.83381483e-03, -2.07341034e-02, -2.58746240e-02,\n",
              "       -3.17148492e-02, -2.78756302e-02, -7.44388700e-02,  1.17350137e-02,\n",
              "       -1.51605364e-02, -5.57643548e-02,  3.63914110e-02, -2.09139683e-03,\n",
              "        1.44762788e-02,  1.99627262e-02,  2.89312433e-02,  3.15455161e-02,\n",
              "        5.51998615e-02,  1.39284402e-03, -3.96359153e-02,  2.05004625e-02,\n",
              "        6.39183745e-02,  7.43686631e-02, -1.53889693e-02,  6.60866871e-02,\n",
              "       -2.42512319e-02, -1.20049184e-02,  5.72583452e-02, -1.70745589e-02,\n",
              "       -6.97179958e-02,  5.02663106e-02, -2.25852821e-02,  9.10286326e-03,\n",
              "       -1.01506300e-01, -1.16730928e-02, -2.31392961e-02,  6.96709827e-02,\n",
              "        2.47116648e-02,  2.79354323e-02,  8.32065418e-02, -5.17854281e-03,\n",
              "        2.30230074e-02, -1.25103882e-02, -1.17263459e-02, -4.34533507e-03,\n",
              "        3.99456695e-02, -3.12809981e-02,  7.16591701e-02,  3.09742279e-02,\n",
              "       -8.18688720e-02,  1.28821237e-02,  7.30063319e-02, -6.89289495e-02,\n",
              "       -1.69185009e-02, -6.96540028e-02,  2.92193312e-02, -2.94877663e-02,\n",
              "       -6.40573129e-02, -4.24507931e-02,  1.02255819e-02,  1.25593264e-02,\n",
              "        6.61236933e-03,  4.91319820e-02, -1.90333799e-02, -6.13994449e-02,\n",
              "       -6.39671236e-02,  5.78848319e-03,  5.59781007e-02,  3.34066600e-02,\n",
              "        4.86004306e-03, -1.27515905e-02, -2.30179708e-02, -3.38933640e-03,\n",
              "       -3.91696254e-03, -3.18568088e-02,  4.20126654e-02,  2.89298454e-03,\n",
              "        1.96746141e-02,  1.96221638e-02,  5.47936447e-02, -4.33366969e-02,\n",
              "       -5.40702380e-02,  1.73873827e-02,  1.07197769e-01,  1.38885584e-02,\n",
              "       -7.35163875e-03, -1.40998801e-02,  5.49086630e-02,  4.94563319e-02,\n",
              "       -1.93028226e-02, -6.28918856e-02,  3.24070156e-02,  4.59265109e-04,\n",
              "       -1.33984741e-02,  7.78660476e-02, -7.86848087e-03, -1.03931218e-01,\n",
              "       -5.42383380e-02,  2.26626154e-02, -2.16532983e-02, -5.13760075e-02,\n",
              "       -2.08660774e-02,  4.61733527e-02,  4.92642261e-02, -4.18032669e-02,\n",
              "        3.01037487e-02,  2.77683977e-02, -5.23851253e-02, -5.02985381e-02,\n",
              "       -7.19792992e-02,  4.20873165e-02,  7.00829225e-03,  7.00225234e-02,\n",
              "       -1.21813891e-02,  4.76323776e-02,  3.02654020e-02,  5.17798634e-03,\n",
              "        4.74375524e-02, -1.10406116e-01,  2.25578900e-02,  2.43525710e-02,\n",
              "        4.53788526e-02,  1.39224725e-02,  4.03538868e-02, -1.31246250e-03,\n",
              "        2.15130970e-02, -1.98300127e-02, -3.91672514e-02, -4.73112911e-02,\n",
              "        4.97059301e-02, -4.14171517e-02, -3.77675928e-02, -3.88689972e-02,\n",
              "       -4.02100310e-02,  5.86881442e-03, -5.92052676e-02,  8.42449591e-02,\n",
              "       -8.43751617e-03, -7.28472620e-02, -3.20445672e-02, -2.80435104e-02,\n",
              "       -6.18447214e-02,  4.52910997e-02,  1.00412453e-02, -2.27205642e-02,\n",
              "        1.16333868e-02,  9.32550803e-03, -3.99930682e-03, -3.73739600e-02,\n",
              "       -3.64108235e-02, -1.57456677e-02,  3.28446366e-02, -3.26153189e-02,\n",
              "       -5.40435947e-02, -5.08470312e-02, -2.19378527e-02,  8.71153921e-02,\n",
              "       -1.74693242e-02, -5.68780564e-02,  1.99351343e-03,  4.82465066e-02,\n",
              "       -4.97148186e-02, -2.54712300e-03,  2.19378378e-02,  1.16106104e-02,\n",
              "        7.71617517e-02, -3.25433016e-02,  5.48689102e-04, -5.93384402e-03,\n",
              "       -2.52622496e-02,  2.15653218e-02,  2.21075397e-03, -3.20494995e-02,\n",
              "        6.94685010e-03,  3.49412709e-02, -2.54618358e-02,  7.50929266e-02,\n",
              "        5.21727977e-03, -3.83299589e-02, -1.36180036e-02, -3.19272205e-02,\n",
              "        1.14225568e-02,  1.58724247e-03,  1.81650408e-02,  2.27658786e-02,\n",
              "       -1.27892280e-02,  3.84871773e-02, -1.08279486e-03, -4.73704450e-02,\n",
              "       -1.67163573e-02,  1.38974665e-02,  5.62306829e-02, -2.68490426e-03,\n",
              "        5.57815693e-02, -7.22645149e-02, -2.24823728e-02,  3.54665853e-02,\n",
              "        1.42619237e-02,  8.30370784e-02, -2.41624936e-03,  3.16991322e-02,\n",
              "       -1.15490295e-02,  3.88707686e-03, -5.24598500e-03, -2.60551441e-02,\n",
              "        1.29123554e-02, -1.56723820e-02,  6.57815561e-02,  2.38347817e-02,\n",
              "       -3.30957323e-02, -2.44016871e-02, -8.38062633e-03,  1.14908698e-03,\n",
              "        4.37334403e-02, -4.74685095e-02, -9.45419818e-02, -5.91542920e-33,\n",
              "        3.30030429e-03,  3.96445952e-02, -1.51662854e-03, -2.31703334e-02,\n",
              "       -7.18485788e-02, -1.47296656e-02,  5.24859056e-02,  4.37986255e-02,\n",
              "        5.03953174e-02, -6.16001617e-03,  1.64663699e-02, -3.01418528e-02,\n",
              "       -3.54766264e-03, -3.49040590e-02,  2.83657499e-02, -1.97837595e-02,\n",
              "        2.91163735e-02, -5.49358781e-03,  1.49489229e-03, -5.71951568e-02,\n",
              "       -9.89637710e-03, -2.45100241e-02,  9.45533719e-03,  2.18002182e-02,\n",
              "       -1.75263472e-02,  5.74542321e-02, -5.60086034e-03, -3.21219228e-02,\n",
              "       -2.35681869e-02,  4.15652804e-02, -4.33003856e-03, -5.93543537e-02,\n",
              "       -2.29760278e-02,  1.09423865e-02, -5.86775644e-03,  8.28297958e-02,\n",
              "       -3.72453816e-02,  1.65277664e-02, -4.57557337e-03,  1.49256671e-02,\n",
              "       -5.48436912e-03, -8.66588876e-02,  4.86667454e-02, -3.50562260e-02,\n",
              "        2.60234950e-03, -3.92286442e-02,  4.92223278e-02,  8.06255452e-03,\n",
              "        4.23370190e-02,  1.99774224e-02, -2.00865474e-02, -1.36591997e-02,\n",
              "       -2.98182927e-02,  3.01239616e-03, -2.09685713e-02, -9.43706185e-02,\n",
              "        3.88822407e-02,  3.13550867e-02, -4.03047800e-02, -4.48071351e-03,\n",
              "       -8.44902396e-02,  3.03610452e-02, -3.39902379e-02,  5.73828537e-03,\n",
              "       -2.02538650e-02, -6.21293392e-03, -1.80076621e-02, -1.43541135e-02,\n",
              "        9.43789165e-03,  6.20732084e-03, -1.36492411e-02,  2.91110370e-02,\n",
              "        8.14592000e-03, -2.34256200e-02,  4.47810702e-02, -5.35876676e-03,\n",
              "       -4.11365069e-02, -1.36338752e-02, -2.94446629e-02, -2.38549486e-02,\n",
              "        1.36106266e-02, -3.28741409e-03, -5.46396375e-02, -9.52198729e-03,\n",
              "        5.46841212e-02, -8.54008249e-05, -3.48852836e-02, -1.60656124e-02,\n",
              "       -3.62546183e-02,  1.81192458e-02,  2.27950737e-02, -1.20481495e-02,\n",
              "       -6.83544874e-02,  2.58695632e-02,  1.44625185e-02,  1.62813459e-02,\n",
              "       -3.66383828e-02, -2.33234540e-02, -2.42937189e-02,  2.85853017e-02,\n",
              "       -1.39358277e-02,  7.71863619e-03, -3.45796123e-02,  2.80473512e-02,\n",
              "        9.46810376e-03, -4.21059504e-02, -5.60742524e-03,  1.29821561e-02,\n",
              "        8.12665373e-03,  4.24522981e-02, -1.88456625e-02,  3.96130159e-02,\n",
              "        6.06210344e-03,  4.35422640e-04,  5.31011969e-02,  2.42750309e-02,\n",
              "       -2.08797287e-02,  2.77207606e-02, -8.99105370e-02, -8.40794891e-02,\n",
              "       -6.76279562e-03, -5.46283834e-03, -2.04162356e-02,  8.07579549e-04,\n",
              "        2.68538408e-02,  1.15557853e-03, -5.50433155e-03,  3.06708682e-02,\n",
              "       -5.79993054e-02, -1.57854222e-02, -1.45974215e-02,  5.26958369e-02,\n",
              "        2.13851507e-07, -3.43880430e-02, -3.81274670e-02, -3.07054725e-02,\n",
              "        2.67131571e-02, -2.78821737e-02,  3.88597921e-02,  4.98165842e-03,\n",
              "        3.81035246e-02, -2.03758106e-02,  8.24826360e-02, -3.36542614e-02,\n",
              "        8.06348806e-04,  3.95396389e-02,  3.39669399e-02,  6.58278912e-02,\n",
              "       -6.82147294e-02, -4.26881947e-03, -6.21654280e-03,  1.69904307e-02,\n",
              "       -5.96147403e-02, -4.59494330e-02,  4.21392098e-02,  3.80836911e-02,\n",
              "        3.64746489e-02, -6.45955931e-03,  2.33981982e-02,  1.13755185e-02,\n",
              "       -5.73579967e-02,  5.97977638e-02,  6.96774945e-03, -1.90750156e-02,\n",
              "        1.08648073e-02, -2.86654979e-02, -4.76020053e-02, -3.52129340e-02,\n",
              "       -3.04710194e-02,  3.37775052e-02,  4.79674339e-02,  4.68017310e-02,\n",
              "        7.90653452e-02, -6.29864261e-02,  8.16390961e-02, -7.43190525e-03,\n",
              "        1.52886072e-02,  3.24059231e-03,  1.35437353e-02,  3.32849473e-02,\n",
              "       -1.05304094e-02, -1.73983853e-02, -1.24285021e-03, -5.04106954e-02,\n",
              "       -3.17308964e-04,  2.34872028e-02,  1.70494784e-02,  2.91661825e-02,\n",
              "       -4.63550612e-02,  2.07221834e-03, -3.60319414e-03,  3.24953981e-02,\n",
              "       -1.38269606e-04, -4.27365266e-02,  6.39410466e-02, -1.71472747e-02,\n",
              "        1.21376306e-01,  6.21141903e-02,  1.57268550e-02, -3.68573293e-02,\n",
              "        1.11955689e-34, -2.41201613e-02, -6.96519436e-03, -1.52328145e-02,\n",
              "       -1.33241555e-02,  1.98338125e-02, -2.53730398e-02, -2.19981242e-02,\n",
              "       -1.45072555e-02,  2.99908519e-02,  4.44530956e-02, -1.19132996e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = \"I like to run\"\n",
        "\n",
        "embedding2 = get_huggingface_embeddings(text2)"
      ],
      "metadata": {
        "id": "Rwg05YhWLG8S"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zKnu9SOzLG-3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FBZfK4uYLjt5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "QyUKZzIIK1dM"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity_embeddings(embedding1, embedding2):\n",
        "  \"\"\"Calculates the cosine similarity between two embeddings.\n",
        "\n",
        "  Args:\n",
        "    embedding1: The first embedding as a NumPy array.\n",
        "    embedding2: The second embedding as a NumPy array.\n",
        "\n",
        "  Returns:\n",
        "    The cosine similarity between the two embeddings.\n",
        "  \"\"\"\n",
        "  dot_product = np.dot(embedding1, embedding2)\n",
        "  magnitude1 = np.linalg.norm(embedding1)\n",
        "  magnitude2 = np.linalg.norm(embedding2)\n",
        "\n",
        "  if magnitude1 == 0 or magnitude2 == 0:\n",
        "    return 0\n",
        "\n",
        "  return dot_product / (magnitude1 * magnitude2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "N6JWrh6VK1fg"
      },
      "outputs": [],
      "source": [
        "cosine_similarity = cosine_similarity_embeddings(embedding, embedding2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity"
      ],
      "metadata": {
        "id": "ig2T98vTLauL",
        "outputId": "5271ffd4-c084-47a4-d383-3fcd4e5d254d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float32(0.93396336)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6D2hKUrWLawu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Q792y-0LazU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umKbNfk3aBOL"
      },
      "source": [
        "# Setting up Pinecone\n",
        "**1. Create an account on [Pinecone.io](https://app.pinecone.io/)**\n",
        "\n",
        "**2. Create a new index called \"codebase-rag\" and set the dimensions to 768. Leave the rest of the settings as they are.**\n",
        "\n",
        "![Screenshot 2024-11-24 at 10 58 50 PM](https://github.com/user-attachments/assets/f5fda046-4087-432a-a8c2-86e061005238)\n",
        "\n",
        "\n",
        "\n",
        "**3. Create an API Key for Pinecone**\n",
        "\n",
        "![Screenshot 2024-11-24 at 10 44 37 PM](https://github.com/user-attachments/assets/e7feacc6-2bd1-472a-82e5-659f65624a88)\n",
        "\n",
        "\n",
        "**4. Store your Pinecone API Key within Google Colab's secrets section, and then enable access to it (see the blue checkmark)**\n",
        "\n",
        "![Screenshot 2024-11-24 at 10 45 25 PM](https://github.com/user-attachments/assets/eaf73083-0b5f-4d17-9e0c-eab84f91b0bc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "y05YK2IjaGgm"
      },
      "outputs": [],
      "source": [
        "# Set the PINECONE_API_KEY as an environment variable\n",
        "pinecone_api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=userdata.get(\"PINECONE_API_KEY\"),)\n",
        "\n",
        "# Connect to your Pinecone index\n",
        "pinecone_index = pc.Index(\"codebase-rag\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "OQN1SdEQbwDI",
        "outputId": "b19f29b6-dc78-4c1b-fe65-3859c247c6f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-5982ffb8f713>:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  vectorstore = PineconeVectorStore(index_name=\"codebase-rag\", embedding=HuggingFaceEmbeddings())\n",
            "<ipython-input-34-5982ffb8f713>:1: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  vectorstore = PineconeVectorStore(index_name=\"codebase-rag\", embedding=HuggingFaceEmbeddings())\n"
          ]
        }
      ],
      "source": [
        "vectorstore = PineconeVectorStore(index_name=\"codebase-rag\", embedding=HuggingFaceEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "tDAB_siIb93B",
        "outputId": "46ce05a2-ccab-4692-cbf5-7e23d3d57bf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-ab9f44c9e140>:17: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  embedding=HuggingFaceEmbeddings(),\n"
          ]
        }
      ],
      "source": [
        "# Insert the codebase embeddings into Pinecone\n",
        "\n",
        "documents = []\n",
        "\n",
        "for file in files_content:\n",
        "    doc = Document(\n",
        "        page_content=f\"{file['name']}\\n\\n{file['content']}\",\n",
        "        metadata={\"source\": file['name']}\n",
        "\n",
        "    )\n",
        "\n",
        "    documents.append(doc)\n",
        "\n",
        "\n",
        "vectorstore = PineconeVectorStore.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=HuggingFaceEmbeddings(),\n",
        "    index_name=\"codebase-rag\",\n",
        "    namespace=\"https://github.com/CoderAgent/SecureAgent\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGuQiFQmd4HZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e75xrBVCrRL6"
      },
      "source": [
        "# Perform RAG\n",
        "\n",
        "1. Get your OpenRouter API Key [here](https://openrouter.ai/settings/keys)\n",
        "\n",
        "2. Paste your OpenRouter Key into your Google Colab secrets, and make sure to enable permissions for it\n",
        "\n",
        "![Image](https://github.com/user-attachments/assets/bd64c5aa-952e-4a1e-9ac0-01d8fe93aaa1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "K9DJQMc_nrsZ"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=userdata.get(\"OPENROUTER_API_KEY\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "MqJtdpK_qNut"
      },
      "outputs": [],
      "source": [
        "query = \"How is the javascript parser used?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "hxQHkgNSLEnH"
      },
      "outputs": [],
      "source": [
        "query_embedding = get_huggingface_embeddings(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "qpsGsYUXLEpx",
        "outputId": "011889c1-ef86-43d3-9972-df54f962ae7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5.71991131e-02, -3.48081291e-02, -3.27215418e-02,  5.29010817e-02,\n",
              "       -3.88054959e-02,  2.21033078e-02,  1.60938818e-02, -1.00735975e-02,\n",
              "        3.04608904e-02, -6.25874922e-02,  2.71743089e-02,  3.67565900e-02,\n",
              "        5.69119453e-02,  5.45354113e-02,  4.02487814e-02, -4.73744869e-02,\n",
              "        3.59910820e-03,  6.65521948e-03,  1.47536704e-02,  3.57538760e-02,\n",
              "        1.81228686e-02,  1.24748386e-02, -2.07926128e-02,  6.99328259e-02,\n",
              "       -1.78905539e-02, -1.98267922e-02, -8.77424423e-03, -4.04769043e-03,\n",
              "       -4.82026935e-02, -1.55368652e-02, -6.26485124e-02, -6.66373223e-03,\n",
              "        1.43068153e-02, -4.92968187e-02,  1.30648721e-06, -2.02893210e-03,\n",
              "       -4.47639599e-02,  2.07317546e-02, -2.80540297e-03,  1.37846796e-02,\n",
              "        4.11506603e-03,  6.87661488e-03, -2.91272085e-02, -6.68382505e-03,\n",
              "        2.94112526e-02, -4.13797721e-02,  3.90248671e-02, -5.73173016e-02,\n",
              "        3.29415090e-02,  1.95522374e-03, -7.05714687e-04, -2.74959207e-02,\n",
              "        8.47589783e-03,  5.72881177e-02,  1.15182819e-02, -2.74837762e-02,\n",
              "        9.83639061e-03, -3.90402637e-02,  2.65231058e-02,  2.21770331e-02,\n",
              "       -3.45997736e-02, -1.54186934e-02,  4.95924754e-03, -1.39193246e-02,\n",
              "        2.68028304e-03,  6.40870398e-03, -4.75133844e-02, -5.65078110e-02,\n",
              "        4.45124984e-04, -3.50493230e-02,  3.05134077e-02, -1.38917025e-02,\n",
              "        5.74368164e-02,  3.41166444e-02, -2.11743731e-02,  8.92364606e-02,\n",
              "       -2.50644702e-02, -4.00944464e-02, -2.53773164e-02,  2.24428754e-02,\n",
              "        2.05555819e-02, -1.37278633e-02, -4.36093891e-03, -3.98067497e-02,\n",
              "        1.22444024e-02,  6.88756406e-02, -1.91624984e-02,  1.02390936e-02,\n",
              "        2.36052368e-02,  3.67535241e-02, -3.08089629e-02, -9.60418060e-02,\n",
              "        3.46700586e-02,  7.60558015e-03,  9.76804271e-02, -1.58347022e-02,\n",
              "        6.36186078e-03, -4.67259996e-02,  1.72872264e-02, -3.31959799e-02,\n",
              "        1.39393723e-02,  5.40790111e-02, -6.83548599e-02,  2.75306888e-02,\n",
              "        2.55090147e-02,  5.65291941e-03,  4.33611162e-02, -3.92891839e-02,\n",
              "       -1.34463422e-03,  2.44514365e-02,  2.43255477e-02, -5.87054379e-02,\n",
              "       -2.45560892e-02,  8.56442470e-03, -1.87520776e-02,  3.01436987e-02,\n",
              "        3.30266990e-02,  2.43946873e-02,  1.19082564e-02,  3.93456295e-02,\n",
              "       -8.27320199e-03, -2.96888910e-02,  2.99564097e-02,  4.83968370e-02,\n",
              "       -3.96414399e-02,  5.54087311e-02, -5.52443974e-02, -4.09533456e-02,\n",
              "        2.51762141e-02, -7.45351091e-02,  2.15652455e-02,  6.81989919e-03,\n",
              "        2.51630731e-02, -2.66167130e-02, -7.87202176e-03,  2.03645229e-02,\n",
              "       -4.96322289e-02,  5.16786873e-02, -3.97936068e-03, -4.66504693e-02,\n",
              "       -1.27714081e-02,  5.39381765e-02,  6.07716627e-02, -5.20693325e-02,\n",
              "       -9.14145168e-03,  3.92798632e-02, -3.82390544e-02, -2.54382212e-02,\n",
              "       -1.19422507e-02,  2.77046878e-02, -2.28422638e-02,  1.93142500e-02,\n",
              "       -1.73437539e-02, -3.91559973e-02,  2.76857801e-03, -2.68673766e-02,\n",
              "       -4.06523468e-03,  2.71099471e-02,  5.50326379e-03, -8.65715183e-03,\n",
              "       -5.38523309e-03, -2.38758381e-02, -1.67614557e-02, -4.30299081e-02,\n",
              "       -2.02624667e-02, -1.52831962e-02, -3.04900017e-02, -2.98172352e-03,\n",
              "       -2.20250301e-02,  4.97563323e-03,  3.35392952e-02,  8.65502004e-03,\n",
              "       -3.52158844e-02, -2.41796877e-02, -5.46162017e-02,  6.46919459e-02,\n",
              "        3.09137329e-02,  6.81637824e-02,  6.54886588e-02,  3.34176458e-02,\n",
              "        7.92475268e-02,  6.95155486e-02, -2.44963896e-02, -2.03490388e-02,\n",
              "        4.01198715e-02,  2.72981985e-03, -7.68630356e-02,  7.44913220e-02,\n",
              "        4.25300933e-02, -3.43394279e-02,  7.25540216e-04,  5.14009818e-02,\n",
              "        2.04829704e-02, -1.03368303e-02, -3.25191095e-02, -9.89327300e-03,\n",
              "        3.73155028e-02,  1.45622957e-02, -3.11909313e-03,  1.36054363e-02,\n",
              "        2.61460710e-02, -3.69705968e-02, -7.29956552e-02,  6.29785433e-02,\n",
              "        4.84144129e-02,  2.19919141e-02, -1.09012667e-02,  3.46254325e-03,\n",
              "       -3.20377983e-02, -4.76485267e-02, -2.28098873e-03,  5.27650770e-03,\n",
              "       -5.92173003e-02,  7.27547379e-03, -1.52387209e-02,  3.88087449e-03,\n",
              "       -5.07637579e-03,  7.63851330e-02, -8.62326548e-02, -1.99151938e-04,\n",
              "       -2.91267354e-02, -3.06969285e-02,  3.16414014e-02,  2.48523126e-03,\n",
              "       -2.64980271e-02, -1.47979911e-02, -2.47980990e-02,  1.41937938e-02,\n",
              "       -4.73740362e-02,  3.34852899e-04, -2.39294395e-02,  1.47233473e-03,\n",
              "        1.51686436e-02,  1.65915210e-03,  6.51667565e-02,  3.18878926e-02,\n",
              "       -5.19753061e-02, -3.91487293e-02,  4.75128032e-02, -2.75391862e-02,\n",
              "       -2.13983115e-02,  2.49021351e-02,  2.59907432e-02, -8.86957441e-03,\n",
              "        2.82779820e-02, -3.33999172e-02,  1.11997565e-02,  5.82844391e-03,\n",
              "       -2.11793110e-02, -4.00041156e-02,  4.85287094e-03,  1.39900222e-02,\n",
              "        4.17894125e-03, -7.78923556e-03, -4.07452658e-02, -3.08802747e-03,\n",
              "        1.24430219e-02, -1.28500471e-02,  3.19465213e-02,  1.55629152e-02,\n",
              "        3.76610681e-02,  1.50284786e-02, -3.08945472e-03,  1.38458060e-02,\n",
              "       -3.01544759e-02,  7.57221738e-03, -3.40416506e-02, -5.21370471e-02,\n",
              "       -5.93046322e-02,  3.19491476e-02,  5.71038807e-03,  2.03530136e-02,\n",
              "        1.42997690e-02,  5.41717596e-02,  3.86603060e-03, -2.02749372e-02,\n",
              "        1.42188957e-02, -3.46890390e-02, -2.56205779e-02,  1.79553963e-02,\n",
              "        1.24495886e-02, -7.16773942e-02, -2.37138453e-03, -1.46819595e-02,\n",
              "       -5.17199859e-02,  4.68135327e-02, -1.01810284e-02, -2.03419961e-02,\n",
              "        3.34763601e-02,  4.86822166e-02,  2.60386113e-02,  8.65800083e-02,\n",
              "        2.55843587e-02, -1.48231238e-02,  1.54763730e-02, -2.24735308e-02,\n",
              "       -3.67613025e-02,  8.40102658e-02, -4.15653773e-02,  3.55002917e-02,\n",
              "       -2.45448556e-02, -5.36360256e-02, -3.72972004e-02,  1.25069078e-02,\n",
              "        3.31733413e-02, -1.56354103e-02, -6.76281452e-02, -7.84973875e-02,\n",
              "       -2.89759338e-02,  1.41797945e-01,  5.50606847e-02,  2.96505950e-02,\n",
              "       -1.88852102e-03,  2.57555721e-03, -2.04364732e-02,  3.05336528e-02,\n",
              "       -4.73619532e-03, -1.05110267e-02, -5.13510071e-02,  4.21647727e-02,\n",
              "        1.20367277e-02,  2.16573868e-02,  6.30189255e-02, -9.46695283e-02,\n",
              "       -2.55720410e-02, -3.23980600e-02,  3.68338898e-02,  4.76690708e-03,\n",
              "        1.15047703e-02,  1.00437272e-02, -2.41284966e-02, -3.10911331e-02,\n",
              "       -1.58535689e-02,  4.08831537e-02, -2.18220912e-02,  3.53736952e-02,\n",
              "       -2.94169020e-02, -4.26899754e-02,  7.75681483e-03,  8.99861101e-03,\n",
              "       -2.52819378e-02, -1.36864875e-02,  2.02667806e-02, -4.14673798e-02,\n",
              "        7.43439887e-03,  2.87979525e-02,  3.36821824e-02,  3.12989801e-02,\n",
              "       -2.69457046e-02, -1.73054133e-02,  3.86038050e-02,  3.35292108e-02,\n",
              "        1.68660711e-02, -2.47896067e-04, -6.19834326e-02, -5.70357293e-02,\n",
              "       -2.00222079e-02, -2.88821049e-02, -7.29324296e-03,  2.31045317e-02,\n",
              "        3.86906713e-02,  4.99564670e-02, -4.24653217e-02, -5.99629246e-02,\n",
              "       -4.41746227e-02, -1.52639868e-02,  5.46780489e-02, -1.19739389e-02,\n",
              "       -5.69641776e-03,  5.25579266e-02,  1.24874180e-02, -2.42425166e-02,\n",
              "       -2.83974223e-02,  1.83277465e-02, -7.52052292e-03, -2.72396039e-02,\n",
              "       -6.37565833e-03, -1.87853426e-02, -3.64199877e-02,  4.44277711e-02,\n",
              "       -4.84144548e-03, -2.59571001e-02, -2.72887591e-02,  5.13039194e-02,\n",
              "        4.55947854e-02, -1.83453914e-02, -9.67897847e-03,  4.89505343e-02,\n",
              "        1.05741043e-02,  2.34286338e-02, -1.59939250e-03,  1.21629843e-02,\n",
              "        1.74357519e-02,  8.69955420e-02, -2.49097049e-02,  3.48292887e-02,\n",
              "        7.69620994e-03,  3.07778567e-02,  6.87526632e-03,  4.10034023e-02,\n",
              "        5.58804767e-03, -6.51644766e-02, -2.15925626e-03, -2.64191367e-02,\n",
              "       -1.73870847e-02, -3.32179256e-02,  7.04814717e-02,  5.56455599e-03,\n",
              "        1.39858108e-02,  2.12791506e-02,  1.75066441e-02, -4.26362120e-02,\n",
              "        1.15706418e-02, -2.65381970e-02, -2.20380276e-02,  1.49483699e-02,\n",
              "       -9.76180751e-03,  2.02453099e-02,  3.22943483e-03,  1.96187962e-02,\n",
              "        2.59395596e-03, -7.23197125e-03,  7.51051353e-03, -4.48049456e-02,\n",
              "        6.28838614e-02,  2.21251380e-02,  5.62744541e-03, -3.55769205e-03,\n",
              "       -1.79370958e-02,  7.44422302e-02,  1.67561416e-02, -7.53110349e-02,\n",
              "       -1.13453552e-01,  2.72909198e-02, -5.47077619e-02, -1.50530171e-02,\n",
              "       -1.25812963e-02,  4.01325598e-02, -5.40130809e-02,  3.55191971e-03,\n",
              "        3.12822424e-02, -1.87432691e-02, -3.57308611e-02,  5.76704694e-03,\n",
              "       -6.83805346e-02,  3.80889550e-02, -1.63622538e-03,  1.38717145e-02,\n",
              "        1.73450122e-03,  5.04001230e-02, -1.97166257e-04,  2.09847298e-02,\n",
              "       -2.53032111e-02,  1.00756595e-02, -5.74005255e-03, -3.91292982e-02,\n",
              "       -4.11761478e-02,  1.97189506e-02,  7.77617039e-04,  3.66704650e-02,\n",
              "       -2.75941435e-02,  1.59035921e-02, -1.70494076e-02, -1.63443610e-02,\n",
              "       -9.65072121e-03, -9.38471183e-02,  4.11776192e-02, -9.08073038e-02,\n",
              "       -2.68844273e-02, -4.85741254e-03,  4.88195904e-02,  4.30380553e-02,\n",
              "       -3.16761471e-02, -5.54440450e-03, -5.39558660e-03, -1.32693946e-02,\n",
              "       -2.58765593e-02, -5.53768016e-02,  1.50012951e-02,  1.90001950e-02,\n",
              "        2.13003922e-02,  4.67846803e-02,  1.58928297e-02,  7.79179260e-02,\n",
              "        5.40408492e-02, -4.86199837e-03, -6.92401733e-03, -1.13790100e-02,\n",
              "       -3.90905850e-02, -2.86201760e-02,  2.77783405e-02, -3.61350179e-03,\n",
              "        8.33226666e-02,  6.88894242e-02, -3.57933640e-02,  2.55738031e-02,\n",
              "       -4.98848082e-03,  7.69988000e-02,  1.79184191e-02, -4.02382575e-02,\n",
              "        5.87942544e-03,  1.80999599e-02, -4.69399523e-03, -2.60235686e-02,\n",
              "        4.01100107e-02, -1.45588315e-03, -4.86016460e-02,  1.99401472e-02,\n",
              "        4.31803800e-03,  7.88210798e-03,  3.08645014e-02, -1.94113683e-02,\n",
              "       -2.27649808e-02,  2.96580996e-02,  6.04185015e-02, -3.13249528e-02,\n",
              "        2.39116941e-02, -7.98480026e-03,  4.28687260e-02,  2.43713427e-02,\n",
              "        6.16064575e-03,  9.98261385e-03, -2.26695333e-02, -5.75594418e-03,\n",
              "        2.52199657e-02, -3.63932997e-02, -2.26824116e-02, -3.48698646e-02,\n",
              "       -1.28334031e-01,  5.05821928e-02,  1.45999258e-02, -4.99502048e-02,\n",
              "       -1.16854310e-02, -5.10387644e-02, -4.39276407e-03, -1.76934954e-02,\n",
              "        1.85698103e-02,  4.24964577e-02,  3.62612517e-03, -8.21787491e-03,\n",
              "        1.52608445e-02, -3.99135798e-02, -1.72168817e-02, -2.55729314e-02,\n",
              "        2.00702455e-02, -2.86136828e-02,  5.72780706e-03,  3.12902592e-02,\n",
              "        4.64497730e-02, -3.95247228e-02, -3.20839658e-02,  9.26095843e-02,\n",
              "        2.89518703e-02, -4.08141920e-03, -1.50173055e-02, -4.41565012e-33,\n",
              "       -4.74333391e-02,  3.32583189e-02,  1.03685381e-02,  5.93765788e-02,\n",
              "        1.88487619e-02, -7.81426951e-02, -2.78754104e-02, -5.63828312e-02,\n",
              "        1.78628378e-02, -1.60493776e-02,  4.21900675e-02,  8.91905744e-03,\n",
              "        1.28873170e-03,  2.24172082e-02,  1.51028158e-02, -5.00620417e-02,\n",
              "        1.40005089e-02, -5.46086160e-03, -2.26302817e-02, -6.18816689e-02,\n",
              "        5.18094487e-02,  1.19468691e-02, -1.44731235e-02,  3.07529178e-02,\n",
              "       -2.58049890e-02, -9.86220874e-03,  2.09785393e-03,  8.37041531e-03,\n",
              "        6.02251180e-02,  6.98747709e-02,  2.91903857e-02, -2.45108344e-02,\n",
              "       -7.29592564e-03, -2.38358825e-02, -1.74369421e-02,  6.75049201e-02,\n",
              "       -1.03483006e-01, -2.44491789e-02,  5.89062497e-02,  8.48937780e-02,\n",
              "       -2.22708918e-02, -2.91215051e-02,  6.03993908e-02, -1.43617103e-02,\n",
              "       -4.30093408e-02, -3.15463059e-02, -2.00793147e-02, -2.32756212e-02,\n",
              "        5.45023754e-03, -2.29356028e-02,  2.18587089e-02,  1.31977256e-02,\n",
              "       -5.85779324e-02,  3.66184227e-02,  8.90511274e-02,  7.60766789e-02,\n",
              "        2.29219161e-02, -8.89664236e-03,  2.57415250e-02, -3.64604443e-02,\n",
              "       -3.63916866e-02,  3.28687914e-02,  1.30067132e-02,  2.96794567e-02,\n",
              "        4.47977241e-03,  2.02063043e-02,  2.59186665e-04, -4.30257842e-02,\n",
              "       -2.88880952e-02,  8.59236270e-02, -8.61762092e-04, -9.64332465e-03,\n",
              "       -2.20969543e-02, -1.97867006e-02, -4.89864219e-03, -3.50129697e-03,\n",
              "       -4.05743672e-03,  2.54365280e-02, -2.47489265e-03,  6.20073229e-02,\n",
              "       -1.31220398e-02,  4.32156632e-03,  3.89345400e-02, -2.68731900e-02,\n",
              "       -2.84163542e-02, -2.78223753e-02, -2.92000175e-02, -2.83088572e-02,\n",
              "       -2.04930920e-02, -3.39918807e-02, -8.12643557e-04,  4.55642343e-02,\n",
              "       -1.17630716e-02, -4.73280139e-02,  4.34421673e-02,  2.05987338e-02,\n",
              "       -5.64046577e-02,  1.54250693e-02,  5.22709498e-03, -1.38512496e-02,\n",
              "        8.95439647e-03, -2.38714553e-02, -5.65449893e-02, -5.69092631e-02,\n",
              "        4.25568409e-02,  1.24976775e-02, -2.92897727e-02, -3.88705954e-02,\n",
              "       -4.52911370e-02, -4.93761734e-04, -2.76485737e-02,  4.34354581e-02,\n",
              "       -1.40210036e-02, -1.12787308e-02,  4.59000990e-02,  1.39154075e-02,\n",
              "       -2.20193826e-02, -5.88837592e-03, -6.79737516e-03, -7.00293761e-03,\n",
              "        5.42924274e-04, -4.00976241e-02, -1.60106067e-02,  1.95955653e-02,\n",
              "       -5.37425019e-02, -3.62902172e-02,  2.77884919e-02,  6.25473168e-03,\n",
              "        3.46342064e-02,  3.21694314e-02,  8.12013634e-03,  2.52433438e-02,\n",
              "        1.90433298e-07,  4.38987985e-02,  2.80597527e-02,  2.45935358e-02,\n",
              "       -1.02043822e-02,  6.41568378e-02,  1.43293105e-02, -1.70972012e-02,\n",
              "        3.34620811e-02,  3.23122367e-02, -5.29283993e-02, -5.55978827e-02,\n",
              "       -2.66577434e-02,  3.09817977e-02,  5.11521772e-02, -4.48862389e-02,\n",
              "        1.54932030e-02, -2.53988262e-02, -2.83563472e-02,  1.72856252e-03,\n",
              "        9.29574966e-02,  1.55642899e-02,  2.24222075e-02, -1.63166504e-02,\n",
              "        5.49935643e-03,  3.24896388e-02, -5.44768050e-02, -6.08727382e-03,\n",
              "        6.90511148e-03, -5.46010351e-03,  1.10500483e-02, -5.91143481e-02,\n",
              "       -5.12277521e-03,  1.05564753e-02, -3.12648481e-03,  2.27596075e-03,\n",
              "       -1.17113227e-02,  7.53145525e-03,  8.07938427e-02,  2.84501240e-02,\n",
              "        9.90494117e-02,  2.89266426e-02, -1.05028056e-01, -1.28771700e-02,\n",
              "        1.14156175e-02,  2.62483805e-02, -5.94671303e-03, -3.32753882e-02,\n",
              "       -1.81747675e-02, -1.53868729e-02, -5.29382192e-02, -6.13175407e-02,\n",
              "       -7.93049298e-03, -3.12193646e-03, -2.62025241e-02, -1.39061101e-02,\n",
              "       -4.54897135e-02,  2.37891916e-02,  9.08095762e-02, -2.70755142e-02,\n",
              "        6.52734190e-02, -4.87189144e-02, -2.26315837e-02, -3.94732170e-02,\n",
              "        1.39855575e-02,  1.24080165e-03,  8.30419809e-02, -1.35176163e-02,\n",
              "        1.63212477e-34,  1.51399104e-02, -1.50091005e-02, -1.88204721e-02,\n",
              "       -2.25235280e-02,  1.63258407e-02, -4.47282335e-03, -2.58178059e-02,\n",
              "        3.47461849e-02, -7.84784369e-03, -1.07982252e-02, -5.36752772e-03],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "query_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ibfr-LsiLEsE"
      },
      "outputs": [],
      "source": [
        "top_matches = pinecone_index.query(vector=query_embedding.tolist(),\n",
        "                                   top_k=5,\n",
        "                                   include_metadata=True,\n",
        "                                   namespace=\"https://github.com/CoderAgent/SecureAgent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "7FwqRviPqkHy",
        "outputId": "b6688545-d01c-48a9-aab2-fcb02d3ca721",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'matches': [{'id': 'b9f222f1-c02e-4537-acc5-028de03e1b35',\n",
              "              'metadata': {'source': 'src/context/language/javascript-parser.ts',\n",
              "                           'text': 'src/context/language/javascript-parser.ts\\n'\n",
              "                                   '\\n'\n",
              "                                   'import { AbstractParser, EnclosingContext '\n",
              "                                   '} from \"../../constants\";\\n'\n",
              "                                   'import * as parser from \"@babel/parser\";\\n'\n",
              "                                   'import traverse, { NodePath, Node } from '\n",
              "                                   '\"@babel/traverse\";\\n'\n",
              "                                   '\\n'\n",
              "                                   'const processNode = (\\n'\n",
              "                                   '  path: NodePath<Node>,\\n'\n",
              "                                   '  lineStart: number,\\n'\n",
              "                                   '  lineEnd: number,\\n'\n",
              "                                   '  largestSize: number,\\n'\n",
              "                                   '  largestEnclosingContext: Node | null\\n'\n",
              "                                   ') => {\\n'\n",
              "                                   '  const { start, end } = path.node.loc;\\n'\n",
              "                                   '  if (start.line <= lineStart && lineEnd '\n",
              "                                   '<= end.line) {\\n'\n",
              "                                   '    const size = end.line - start.line;\\n'\n",
              "                                   '    if (size > largestSize) {\\n'\n",
              "                                   '      largestSize = size;\\n'\n",
              "                                   '      largestEnclosingContext = '\n",
              "                                   'path.node;\\n'\n",
              "                                   '    }\\n'\n",
              "                                   '  }\\n'\n",
              "                                   '  return { largestSize, '\n",
              "                                   'largestEnclosingContext };\\n'\n",
              "                                   '};\\n'\n",
              "                                   '\\n'\n",
              "                                   'export class JavascriptParser implements '\n",
              "                                   'AbstractParser {\\n'\n",
              "                                   '  findEnclosingContext(\\n'\n",
              "                                   '    file: string,\\n'\n",
              "                                   '    lineStart: number,\\n'\n",
              "                                   '    lineEnd: number\\n'\n",
              "                                   '  ): EnclosingContext {\\n'\n",
              "                                   '    const ast = parser.parse(file, {\\n'\n",
              "                                   '      sourceType: \"module\",\\n'\n",
              "                                   '      plugins: [\"jsx\", \"typescript\"], // '\n",
              "                                   'To allow JSX and TypeScript\\n'\n",
              "                                   '    });\\n'\n",
              "                                   '    let largestEnclosingContext: Node = '\n",
              "                                   'null;\\n'\n",
              "                                   '    let largestSize = 0;\\n'\n",
              "                                   '    traverse(ast, {\\n'\n",
              "                                   '      Function(path) {\\n'\n",
              "                                   '        ({ largestSize, '\n",
              "                                   'largestEnclosingContext } = processNode(\\n'\n",
              "                                   '          path,\\n'\n",
              "                                   '          lineStart,\\n'\n",
              "                                   '          lineEnd,\\n'\n",
              "                                   '          largestSize,\\n'\n",
              "                                   '          largestEnclosingContext\\n'\n",
              "                                   '        ));\\n'\n",
              "                                   '      },\\n'\n",
              "                                   '      TSInterfaceDeclaration(path) {\\n'\n",
              "                                   '        ({ largestSize, '\n",
              "                                   'largestEnclosingContext } = processNode(\\n'\n",
              "                                   '          path,\\n'\n",
              "                                   '          lineStart,\\n'\n",
              "                                   '          lineEnd,\\n'\n",
              "                                   '          largestSize,\\n'\n",
              "                                   '          largestEnclosingContext\\n'\n",
              "                                   '        ));\\n'\n",
              "                                   '      },\\n'\n",
              "                                   '    });\\n'\n",
              "                                   '    return {\\n'\n",
              "                                   '      enclosingContext: '\n",
              "                                   'largestEnclosingContext,\\n'\n",
              "                                   '    } as EnclosingContext;\\n'\n",
              "                                   '  }\\n'\n",
              "                                   '\\n'\n",
              "                                   '  dryRun(file: string): { valid: boolean; '\n",
              "                                   'error: string } {\\n'\n",
              "                                   '    try {\\n'\n",
              "                                   '      const ast = parser.parse(file, {\\n'\n",
              "                                   '        sourceType: \"module\",\\n'\n",
              "                                   '        plugins: [\"jsx\", \"typescript\"], // '\n",
              "                                   'To allow JSX and TypeScript\\n'\n",
              "                                   '      });\\n'\n",
              "                                   '      return {\\n'\n",
              "                                   '        valid: true,\\n'\n",
              "                                   '        error: \"\",\\n'\n",
              "                                   '      };\\n'\n",
              "                                   '    } catch (exc) {\\n'\n",
              "                                   '      return {\\n'\n",
              "                                   '        valid: false,\\n'\n",
              "                                   '        error: exc,\\n'\n",
              "                                   '      };\\n'\n",
              "                                   '    }\\n'\n",
              "                                   '  }\\n'\n",
              "                                   '}\\n'},\n",
              "              'score': 0.422816962,\n",
              "              'values': []},\n",
              "             {'id': 'a87acf8d-a5aa-4194-b6c6-26fe06f64c69',\n",
              "              'metadata': {'source': 'src/context/language/python-parser.ts',\n",
              "                           'text': 'src/context/language/python-parser.ts\\n'\n",
              "                                   '\\n'\n",
              "                                   'import { AbstractParser, EnclosingContext '\n",
              "                                   '} from \"../../constants\";\\n'\n",
              "                                   'export class PythonParser implements '\n",
              "                                   'AbstractParser {\\n'\n",
              "                                   '  findEnclosingContext(\\n'\n",
              "                                   '    file: string,\\n'\n",
              "                                   '    lineStart: number,\\n'\n",
              "                                   '    lineEnd: number\\n'\n",
              "                                   '  ): EnclosingContext {\\n'\n",
              "                                   '    // TODO: Implement this method for '\n",
              "                                   'Python\\n'\n",
              "                                   '    return null;\\n'\n",
              "                                   '  }\\n'\n",
              "                                   '  dryRun(file: string): { valid: boolean; '\n",
              "                                   'error: string } {\\n'\n",
              "                                   '    // TODO: Implement this method for '\n",
              "                                   'Python\\n'\n",
              "                                   '    return { valid: false, error: \"Not '\n",
              "                                   'implemented yet\" };\\n'\n",
              "                                   '  }\\n'\n",
              "                                   '}\\n'},\n",
              "              'score': 0.273299128,\n",
              "              'values': []},\n",
              "             {'id': '808c4f25-1895-4e2d-9404-4f4fd53eee2a',\n",
              "              'metadata': {'source': 'src/context/review.ts',\n",
              "                           'text': 'src/context/review.ts\\n'\n",
              "                                   '\\n'\n",
              "                                   'import {\\n'\n",
              "                                   '  AbstractParser,\\n'\n",
              "                                   '  PRFile,\\n'\n",
              "                                   '  PatchInfo,\\n'\n",
              "                                   '  getParserForExtension,\\n'\n",
              "                                   '} from \"../constants\";\\n'\n",
              "                                   'import * as diff from \"diff\";\\n'\n",
              "                                   'import { JavascriptParser } from '\n",
              "                                   '\"./language/javascript-parser\";\\n'\n",
              "                                   'import { Node } from \"@babel/traverse\";\\n'\n",
              "                                   '\\n'\n",
              "                                   'const expandHunk = (\\n'\n",
              "                                   '  contents: string,\\n'\n",
              "                                   '  hunk: diff.Hunk,\\n'\n",
              "                                   '  linesAbove: number = 5,\\n'\n",
              "                                   '  linesBelow: number = 5\\n'\n",
              "                                   ') => {\\n'\n",
              "                                   '  const fileLines = '\n",
              "                                   'contents.split(\"\\\\n\");\\n'\n",
              "                                   '  const curExpansion: string[] = [];\\n'\n",
              "                                   '  const start = Math.max(0, hunk.oldStart '\n",
              "                                   '- 1 - linesAbove);\\n'\n",
              "                                   '  const end = Math.min(\\n'\n",
              "                                   '    fileLines.length,\\n'\n",
              "                                   '    hunk.oldStart - 1 + hunk.oldLines + '\n",
              "                                   'linesBelow\\n'\n",
              "                                   '  );\\n'\n",
              "                                   '\\n'\n",
              "                                   '  for (let i = start; i < hunk.oldStart - '\n",
              "                                   '1; i++) {\\n'\n",
              "                                   '    curExpansion.push(fileLines[i]);\\n'\n",
              "                                   '  }\\n'\n",
              "                                   '\\n'\n",
              "                                   '  curExpansion.push(\\n'\n",
              "                                   '    `@@ -${hunk.oldStart},${hunk.oldLines} '\n",
              "                                   '+${hunk.newStart},${hunk.newLines} @@`\\n'\n",
              "                                   '  );\\n'\n",
              "                                   '  hunk.lines.forEach((line) => {\\n'\n",
              "                                   '    if (!curExpansion.includes(line)) {\\n'\n",
              "                                   '      curExpansion.push(line);\\n'\n",
              "                                   '    }\\n'\n",
              "                                   '  });\\n'\n",
              "                                   '\\n'\n",
              "                                   '  for (let i = hunk.oldStart - 1 + '\n",
              "                                   'hunk.oldLines; i < end; i++) {\\n'\n",
              "                                   '    curExpansion.push(fileLines[i]);\\n'\n",
              "                                   '  }\\n'\n",
              "                                   '  return curExpansion.join(\"\\\\n\");\\n'\n",
              "                                   '};\\n'\n",
              "                                   '\\n'\n",
              "                                   'const expandFileLines = (\\n'\n",
              "                                   '  file: PRFile,\\n'\n",
              "                                   '  linesAbove: number = 5,\\n'\n",
              "                                   '  linesBelow: number = 5\\n'\n",
              "                                   ') => {\\n'\n",
              "                                   '  const fileLines = '\n",
              "                                   'file.old_contents.split(\"\\\\n\");\\n'\n",
              "                                   '  const patches: PatchInfo[] = '\n",
              "                                   'diff.parsePatch(file.patch);\\n'\n",
              "                                   '  const expandedLines: string[][] = [];\\n'\n",
              "                                   '  patches.forEach((patch) => {\\n'\n",
              "                                   '    patch.hunks.forEach((hunk) => {\\n'\n",
              "                                   '      const curExpansion: string[] = [];\\n'\n",
              "                                   '      const start = Math.max(0, '\n",
              "                                   'hunk.oldStart - 1 - linesAbove);\\n'\n",
              "                                   '      const end = Math.min(\\n'\n",
              "                                   '        fileLines.length,\\n'\n",
              "                                   '        hunk.oldStart - 1 + hunk.oldLines '\n",
              "                                   '+ linesBelow\\n'\n",
              "                                   '      );\\n'\n",
              "                                   '\\n'\n",
              "                                   '      for (let i = start; i < '\n",
              "                                   'hunk.oldStart - 1; i++) {\\n'\n",
              "                                   '        curExpansion.push(fileLines[i]);\\n'\n",
              "                                   '      }\\n'\n",
              "                                   '\\n'\n",
              "                                   '      curExpansion.push(\\n'\n",
              "                                   '        `@@ '\n",
              "                                   '-${hunk.oldStart},${hunk.oldLines} '\n",
              "                                   '+${hunk.newStart},${hunk.newLines} @@`\\n'\n",
              "                                   '      );\\n'\n",
              "                                   '      hunk.lines.forEach((line) => {\\n'\n",
              "                                   '        if (!curExpansion.includes(line)) '\n",
              "                                   '{\\n'\n",
              "                                   '          curExpansion.push(line);\\n'\n",
              "                                   '        }\\n'\n",
              "                                   '      });\\n'\n",
              "                                   '\\n'\n",
              "                                   '      for (let i = hunk.oldStart - 1 + '\n",
              "                                   'hunk.oldLines; i < end; i++) {\\n'\n",
              "                                   '        curExpansion.push(fileLines[i]);\\n'\n",
              "                                   '      }\\n'\n",
              "                                   '      expandedLines.push(curExpansion);\\n'\n",
              "                                   '    });\\n'\n",
              "                                   '  });\\n'\n",
              "                                   '\\n'\n",
              "                                   '  return expandedLines;\\n'\n",
              "                                   '};\\n'\n",
              "                                   '\\n'\n",
              "                                   'export const expandedPatchStrategy = '\n",
              "                                   '(file: PRFile) => {\\n'\n",
              "                                   '  const expandedPatches = '\n",
              "                                   'expandFileLines(file);\\n'\n",
              "                                   '  const expansions = expandedPatches\\n'\n",
              "                                   '    .map((patchLines) => '\n",
              "                                   'patchLines.join(\"\\\\n\"))\\n'\n",
              "                                   '    .join(\"\\\\n\\\\n\");\\n'\n",
              "                                   '  return `## '\n",
              "                                   '${file.filename}\\\\n\\\\n${expansions}`;\\n'\n",
              "                                   '};\\n'\n",
              "                                   '\\n'\n",
              "                                   'export const rawPatchStrategy = (file: '\n",
              "                                   'PRFile) => {\\n'\n",
              "                                   '  return `## '\n",
              "                                   '${file.filename}\\\\n\\\\n${file.patch}`;\\n'\n",
              "                                   '};\\n'\n",
              "                                   '\\n'\n",
              "                                   'const trimHunk = (hunk: diff.Hunk): '\n",
              "                                   'diff.Hunk => {\\n'\n",
              "                                   '  const startIdx = hunk.lines.findIndex(\\n'\n",
              "                                   '    (line) => line.startsWith(\"+\") || '\n",
              "                                   'line.startsWith(\"-\")\\n'\n",
              "                                   '  );\\n'\n",
              "                                   '  const endIdx = hunk.lines\\n'\n",
              "                                   '    .slice()\\n'\n",
              "                                   '    .reverse()\\n'\n",
              "                                   '    .findIndex((line) => '\n",
              "                                   'line.startsWith(\"+\") || '\n",
              "                                   'line.startsWith(\"-\"));\\n'\n",
              "                                   '  const editLines = '\n",
              "                                   'hunk.lines.slice(startIdx, '\n",
              "                                   'hunk.lines.length - endIdx);\\n'\n",
              "                                   '  return { ...hunk, lines: editLines, '\n",
              "                                   'newStart: startIdx + hunk.newStart };\\n'\n",
              "                                   '};\\n'\n",
              "                                   '\\n'\n",
              "                                   'const buildingScopeString = (\\n'\n",
              "                                   '  currentFile: string,\\n'\n",
              "                                   '  scope: Node,\\n'\n",
              "                                   '  hunk: diff.Hunk\\n'\n",
              "                                   ') => {\\n'\n",
              "                                   '  const res: string[] = [];\\n'\n",
              "                                   '  const trimmedHunk = trimHunk(hunk);\\n'\n",
              "                                   '  const functionStartLine = '\n",
              "                                   'scope.loc.start.line;\\n'\n",
              "                                   '  const functionEndLine = '\n",
              "                                   'scope.loc.end.line;\\n'\n",
              "                                   '  const updatedFileLines = '\n",
              "                                   'currentFile.split(\"\\\\n\");\\n'\n",
              "                                   '  // Extract the lines of the function\\n'\n",
              "                                   '  const functionContext = '\n",
              "                                   'updatedFileLines.slice(\\n'\n",
              "                                   '    functionStartLine - 1,\\n'\n",
              "                                   '    functionEndLine\\n'\n",
              "                                   '  );\\n'\n",
              "                                   '  // Calculate the index where the changes '\n",
              "                                   'should be injected into the function\\n'\n",
              "                                   '  const injectionIdx =\\n'\n",
              "                                   '    hunk.newStart -\\n'\n",
              "                                   '    functionStartLine +\\n'\n",
              "                                   '    hunk.lines.findIndex(\\n'\n",
              "                                   '      (line) => line.startsWith(\"+\") || '\n",
              "                                   'line.startsWith(\"-\")\\n'\n",
              "                                   '    );\\n'\n",
              "                                   '  // Count the number of lines that should '\n",
              "                                   'be dropped from the function\\n'\n",
              "                                   '  const dropCount = '\n",
              "                                   'trimmedHunk.lines.filter(\\n'\n",
              "                                   '    (line) => !line.startsWith(\"-\")\\n'\n",
              "                                   '  ).length;\\n'\n",
              "                                   '\\n'\n",
              "                                   '  const hunkHeader = `@@ '\n",
              "                                   '-${hunk.oldStart},${hunk.oldLines} '\n",
              "                                   '+${hunk.newStart},${hunk.newLines} @@`;\\n'\n",
              "                                   '  // Inject the changes into the function, '\n",
              "                                   'dropping the necessary lines\\n'\n",
              "                                   '  functionContext.splice(injectionIdx, '\n",
              "                                   'dropCount, ...trimmedHunk.lines);\\n'\n",
              "                                   '\\n'\n",
              "                                   '  res.push(functionContext.join(\"\\\\n\"));\\n'\n",
              "                                   '  res.unshift(hunkHeader);\\n'\n",
              "                                   '  return res;\\n'\n",
              "                                   '};\\n'\n",
              "                                   '\\n'\n",
              "                                   '/*\\n'\n",
              "                                   'line nums are 0 index, file is 1 index\\n'\n",
              "                                   '*/\\n'\n",
              "                                   'const combineHunks = (\\n'\n",
              "                                   '  file: string,\\n'\n",
              "                                   '  overlappingHunks: diff.Hunk[]\\n'\n",
              "                                   '): diff.Hunk => {\\n'\n",
              "                                   '  if (!overlappingHunks || '\n",
              "                                   'overlappingHunks.length === 0) {\\n'\n",
              "                                   '    throw \"Overlapping hunks are empty, '\n",
              "                                   'this should never happen.\";\\n'\n",
              "                                   '  }\\n'\n",
              "                                   '  const sortedHunks = '\n",
              "                                   'overlappingHunks.sort((a, b) => a.newStart '\n",
              "                                   '- b.newStart);\\n'\n",
              "                                   '  const fileLines = file.split(\"\\\\n\");\\n'\n",
              "                                   '  let lastHunkEnd = '\n",
              "                                   'sortedHunks[0].newStart + '\n",
              "                                   'sortedHunks[0].newLines;\\n'\n",
              "                                   '\\n'\n",
              "                                   '  const combinedHunk: diff.Hunk = {\\n'\n",
              "                                   '    oldStart: sortedHunks[0].oldStart,\\n'\n",
              "                                   '    oldLines: sortedHunks[0].oldLines,\\n'\n",
              "                                   '    newStart: sortedHunks[0].newStart,\\n'\n",
              "                                   '    newLines: sortedHunks[0].newLines,\\n'\n",
              "                                   '    lines: [...sortedHunks[0].lines],\\n'\n",
              "                                   '    linedelimiters: '\n",
              "                                   '[...sortedHunks[0].linedelimiters],\\n'\n",
              "                                   '  };\\n'\n",
              "                                   '\\n'\n",
              "                                   '  for (let i = 1; i < sortedHunks.length; '\n",
              "                                   'i++) {\\n'\n",
              "                                   '    const hunk = sortedHunks[i];\\n'\n",
              "                                   '\\n'\n",
              "                                   \"    // If there's a gap between the last \"\n",
              "                                   'hunk and this one, add the lines in '\n",
              "                                   'between\\n'\n",
              "                                   '    if (hunk.newStart > lastHunkEnd) {\\n'\n",
              "                                   '      combinedHunk.lines.push(\\n'\n",
              "                                   '        ...fileLines.slice(lastHunkEnd - '\n",
              "                                   '1, hunk.newStart - 1)\\n'\n",
              "                                   '      );\\n'\n",
              "                                   '      combinedHunk.newLines += '\n",
              "                                   'hunk.newStart - lastHunkEnd;\\n'\n",
              "                                   '    }\\n'\n",
              "                                   '\\n'\n",
              "                                   '    combinedHunk.oldLines += '\n",
              "                                   'hunk.oldLines;\\n'\n",
              "                                   '    combinedHunk.newLines += '\n",
              "                                   'hunk.newLines;\\n'\n",
              "                                   '    '\n",
              "                                   'combinedHunk.lines.push(...hunk.lines);\\n'\n",
              "                                   '    '\n",
              "                                   'combinedHunk.linedelimiters.push(...hunk.linedelimiters);\\n'\n",
              "                                   '\\n'\n",
              "                                   '    lastHunkEnd = hunk.newStart + '\n",
              "                                   'hunk.newLines;\\n'\n",
              "                                   '  }\\n'\n",
              "                                   '  return combinedHunk;\\n'\n",
              "                                   '};\\n'\n",
              "                                   '\\n'\n",
              "                                   'const diffContextPerHunk = (file: PRFile, '\n",
              "                                   'parser: AbstractParser) => {\\n'\n",
              "                                   '  const updatedFile = '\n",
              "                                   'diff.applyPatch(file.old_contents, '\n",
              "                                   'file.patch);\\n'\n",
              "                                   '  const patches = '\n",
              "                                   'diff.parsePatch(file.patch);\\n'\n",
              "                                   '  if (!updatedFile || typeof updatedFile '\n",
              "                                   '!== \"string\") {\\n'\n",
              "                                   '    console.log(\"APPLYING PATCH ERROR - '\n",
              "                                   'FALLINGBACK\");\\n'\n",
              "                                   '    throw \"THIS SHOULD NOT HAPPEN!\";\\n'\n",
              "                                   '  }\\n'\n",
              "                                   '\\n'\n",
              "                                   '  const hunks: diff.Hunk[] = [];\\n'\n",
              "                                   '  const order: number[] = [];\\n'\n",
              "                                   '  const scopeRangeHunkMap = new '\n",
              "                                   'Map<string, diff.Hunk[]>();\\n'\n",
              "                                   '  const scopeRangeNodeMap = new '\n",
              "                                   'Map<string, Node>();\\n'\n",
              "                                   '  const expandStrategy: diff.Hunk[] = [];\\n'\n",
              "                                   '\\n'\n",
              "                                   '  patches.forEach((p) => {\\n'\n",
              "                                   '    p.hunks.forEach((hunk) => {\\n'\n",
              "                                   '      hunks.push(hunk);\\n'\n",
              "                                   '    });\\n'\n",
              "                                   '  });\\n'\n",
              "                                   '\\n'\n",
              "                                   '  hunks.forEach((hunk, idx) => {\\n'\n",
              "                                   '    try {\\n'\n",
              "                                   '      const trimmedHunk = trimHunk(hunk);\\n'\n",
              "                                   '      const insertions = '\n",
              "                                   'hunk.lines.filter((line) =>\\n'\n",
              "                                   '        line.startsWith(\"+\")\\n'\n",
              "                                   '      ).length;\\n'\n",
              "                                   '      const lineStart = '\n",
              "                                   'trimmedHunk.newStart;\\n'\n",
              "                                   '      const lineEnd = lineStart + '\n",
              "                                   'insertions;\\n'\n",
              "                                   '      const largestEnclosingFunction = '\n",
              "                                   'parser.findEnclosingContext(\\n'\n",
              "                                   '        updatedFile,\\n'\n",
              "                                   '        lineStart,\\n'\n",
              "                                   '        lineEnd\\n'\n",
              "                                   '      ).enclosingContext;\\n'\n",
              "                                   '\\n'\n",
              "                                   '      if (largestEnclosingFunction) {\\n'\n",
              "                                   '        const enclosingRangeKey = '\n",
              "                                   '`${largestEnclosingFunction.loc.start.line} '\n",
              "                                   '-> '\n",
              "                                   '${largestEnclosingFunction.loc.end.line}`;\\n'\n",
              "                                   '        let existingHunks = '\n",
              "                                   'scopeRangeHunkMap.get(enclosingRangeKey) '\n",
              "                                   '|| [];\\n'\n",
              "                                   '        existingHunks.push(hunk);\\n'\n",
              "                                   '        '\n",
              "                                   'scopeRangeHunkMap.set(enclosingRangeKey, '\n",
              "                                   'existingHunks);\\n'\n",
              "                                   '        '\n",
              "                                   'scopeRangeNodeMap.set(enclosingRangeKey, '\n",
              "                                   'largestEnclosingFunction);\\n'\n",
              "                                   '      } else {\\n'\n",
              "                                   '        throw \"No enclosing function.\";\\n'\n",
              "                                   '      }\\n'\n",
              "                                   '      order.push(idx);\\n'\n",
              "                                   '    } catch (exc) {\\n'\n",
              "                                   '      console.log(file.filename);\\n'\n",
              "                                   '      console.log(\"NORMAL STRATEGY\");\\n'\n",
              "                                   '      console.log(exc);\\n'\n",
              "                                   '      expandStrategy.push(hunk);\\n'\n",
              "                                   '      order.push(idx);\\n'\n",
              "                                   '    }\\n'\n",
              "                                   '  });\\n'\n",
              "                                   '\\n'\n",
              "                                   '  const scopeStategy: [string, '\n",
              "                                   'diff.Hunk][] = []; // holds map range key '\n",
              "                                   'and combined hunk: [[key, hunk]]\\n'\n",
              "                                   '  for (const [range, hunks] of '\n",
              "                                   'scopeRangeHunkMap.entries()) {\\n'\n",
              "                                   '    const combinedHunk = '\n",
              "                                   'combineHunks(updatedFile, hunks);\\n'\n",
              "                                   '    scopeStategy.push([range, '\n",
              "                                   'combinedHunk]);\\n'\n",
              "                                   '  }\\n'\n",
              "                                   '\\n'\n",
              "                                   '  const contexts: string[] = [];\\n'\n",
              "                                   '  scopeStategy.forEach(([rangeKey, hunk]) '\n",
              "                                   '=> {\\n'\n",
              "                                   '    const context = buildingScopeString(\\n'\n",
              "                                   '      updatedFile,\\n'\n",
              "                                   '      scopeRangeNodeMap.get(rangeKey),\\n'\n",
              "                                   '      hunk\\n'\n",
              "                                   '    ).join(\"\\\\n\");\\n'\n",
              "                                   '    contexts.push(context);\\n'\n",
              "                                   '  });\\n'\n",
              "                                   '  expandStrategy.forEach((hunk) => {\\n'\n",
              "                                   '    const context = '\n",
              "                                   'expandHunk(file.old_contents, hunk);\\n'\n",
              "                                   '    contexts.push(context);\\n'\n",
              "                                   '  });\\n'\n",
              "                                   '  return contexts;\\n'\n",
              "                                   '};\\n'\n",
              "                                   '\\n'\n",
              "                                   'const functionContextPatchStrategy = (\\n'\n",
              "                                   '  file: PRFile,\\n'\n",
              "                                   '  parser: AbstractParser\\n'\n",
              "                                   '): string => {\\n'\n",
              "                                   '  let res = null;\\n'\n",
              "                                   '  try {\\n'\n",
              "                                   '    const contextChunks = '\n",
              "                                   'diffContextPerHunk(file, parser);\\n'\n",
              "                                   '    res = `## '\n",
              "                                   '${file.filename}\\\\n\\\\n${contextChunks.join(\"\\\\n\\\\n\")}`;\\n'\n",
              "                                   '  } catch (exc) {\\n'\n",
              "                                   '    console.log(exc);\\n'\n",
              "                                   '    res = expandedPatchStrategy(file);\\n'\n",
              "                                   '  }\\n'\n",
              "                                   '  return res;\\n'\n",
              "                                   '};\\n'\n",
              "                                   '\\n'\n",
              "                                   'export const smarterContextPatchStrategy = '\n",
              "                                   '(file: PRFile) => {\\n'\n",
              "                                   '  const parser: AbstractParser = '\n",
              "                                   'getParserForExtension(file.filename);\\n'\n",
              "                                   '  if (parser != null) {\\n'\n",
              "                                   '    return '\n",
              "                                   'functionContextPatchStrategy(file, '\n",
              "                                   'parser);\\n'\n",
              "                                   '  } else {\\n'\n",
              "                                   '    return expandedPatchStrategy(file);\\n'\n",
              "                                   '  }\\n'\n",
              "                                   '};\\n'},\n",
              "              'score': 0.244099587,\n",
              "              'values': []},\n",
              "             {'id': '39503051-7d12-4a81-9288-e3c4814df539',\n",
              "              'metadata': {'source': 'src/constants.ts',\n",
              "                           'text': 'src/constants.ts\\n'\n",
              "                                   '\\n'\n",
              "                                   'import { Node } from \"@babel/traverse\";\\n'\n",
              "                                   'import { JavascriptParser } from '\n",
              "                                   '\"./context/language/javascript-parser\";\\n'\n",
              "                                   'import { ChatCompletionMessageParam } from '\n",
              "                                   '\"groq-sdk/resources/chat/completions\";\\n'\n",
              "                                   '\\n'\n",
              "                                   'export interface PRFile {\\n'\n",
              "                                   '  sha: string;\\n'\n",
              "                                   '  filename: string;\\n'\n",
              "                                   '  status:\\n'\n",
              "                                   '    | \"added\"\\n'\n",
              "                                   '    | \"removed\"\\n'\n",
              "                                   '    | \"renamed\"\\n'\n",
              "                                   '    | \"changed\"\\n'\n",
              "                                   '    | \"modified\"\\n'\n",
              "                                   '    | \"copied\"\\n'\n",
              "                                   '    | \"unchanged\";\\n'\n",
              "                                   '  additions: number;\\n'\n",
              "                                   '  deletions: number;\\n'\n",
              "                                   '  changes: number;\\n'\n",
              "                                   '  blob_url: string;\\n'\n",
              "                                   '  raw_url: string;\\n'\n",
              "                                   '  contents_url: string;\\n'\n",
              "                                   '  patch?: string;\\n'\n",
              "                                   '  previous_filename?: string;\\n'\n",
              "                                   '  patchTokenLength?: number;\\n'\n",
              "                                   '  old_contents?: string;\\n'\n",
              "                                   '  current_contents?: string;\\n'\n",
              "                                   '}\\n'\n",
              "                                   '\\n'\n",
              "                                   'export interface BuilderResponse {\\n'\n",
              "                                   '  comment: string;\\n'\n",
              "                                   '  structuredComments: any[];\\n'\n",
              "                                   '}\\n'\n",
              "                                   '\\n'\n",
              "                                   'export interface Builders {\\n'\n",
              "                                   '  convoBuilder: (diff: string) => '\n",
              "                                   'ChatCompletionMessageParam[];\\n'\n",
              "                                   '  responseBuilder: (feedbacks: string[]) '\n",
              "                                   '=> Promise<BuilderResponse>;\\n'\n",
              "                                   '}\\n'\n",
              "                                   '\\n'\n",
              "                                   'export interface PatchInfo {\\n'\n",
              "                                   '  hunks: {\\n'\n",
              "                                   '    oldStart: number;\\n'\n",
              "                                   '    oldLines: number;\\n'\n",
              "                                   '    newStart: number;\\n'\n",
              "                                   '    newLines: number;\\n'\n",
              "                                   '    lines: string[];\\n'\n",
              "                                   '  }[];\\n'\n",
              "                                   '}\\n'\n",
              "                                   '\\n'\n",
              "                                   'export interface PRSuggestion {\\n'\n",
              "                                   '  describe: string;\\n'\n",
              "                                   '  type: string;\\n'\n",
              "                                   '  comment: string;\\n'\n",
              "                                   '  code: string;\\n'\n",
              "                                   '  filename: string;\\n'\n",
              "                                   '  toString: () => string;\\n'\n",
              "                                   '  identity: () => string;\\n'\n",
              "                                   '}\\n'\n",
              "                                   '\\n'\n",
              "                                   'export interface CodeSuggestion {\\n'\n",
              "                                   '  file: string;\\n'\n",
              "                                   '  line_start: number;\\n'\n",
              "                                   '  line_end: number;\\n'\n",
              "                                   '  correction: string;\\n'\n",
              "                                   '  comment: string;\\n'\n",
              "                                   '}\\n'\n",
              "                                   '\\n'\n",
              "                                   'export interface ChatMessage {\\n'\n",
              "                                   '  role: string;\\n'\n",
              "                                   '  content: string;\\n'\n",
              "                                   '}\\n'\n",
              "                                   '\\n'\n",
              "                                   'export interface Review {\\n'\n",
              "                                   '  review: BuilderResponse;\\n'\n",
              "                                   '  suggestions: CodeSuggestion[];\\n'\n",
              "                                   '}\\n'\n",
              "                                   '\\n'\n",
              "                                   'export interface BranchDetails {\\n'\n",
              "                                   '  name: string;\\n'\n",
              "                                   '  sha: string;\\n'\n",
              "                                   '  url: string;\\n'\n",
              "                                   '}\\n'\n",
              "                                   '\\n'\n",
              "                                   'export const sleep = async (ms: number) => '\n",
              "                                   '{\\n'\n",
              "                                   '  return new Promise((resolve) => '\n",
              "                                   'setTimeout(resolve, ms));\\n'\n",
              "                                   '};\\n'\n",
              "                                   '\\n'\n",
              "                                   'export const processGitFilepath = '\n",
              "                                   '(filepath: string) => {\\n'\n",
              "                                   \"  // Remove the leading '/' if it exists\\n\"\n",
              "                                   '  return filepath.startsWith(\"/\") ? '\n",
              "                                   'filepath.slice(1) : filepath;\\n'\n",
              "                                   '};\\n'\n",
              "                                   '\\n'\n",
              "                                   'export interface EnclosingContext {\\n'\n",
              "                                   '  enclosingContext: Node | null;\\n'\n",
              "                                   '}\\n'\n",
              "                                   '\\n'\n",
              "                                   'export interface AbstractParser {\\n'\n",
              "                                   '  findEnclosingContext(\\n'\n",
              "                                   '    file: string,\\n'\n",
              "                                   '    lineStart: number,\\n'\n",
              "                                   '    lineEnd: number\\n'\n",
              "                                   '  ): EnclosingContext;\\n'\n",
              "                                   '  dryRun(file: string): { valid: boolean; '\n",
              "                                   'error: string };\\n'\n",
              "                                   '}\\n'\n",
              "                                   '\\n'\n",
              "                                   'const EXTENSIONS_TO_PARSERS: Map<string, '\n",
              "                                   'AbstractParser> = new Map([\\n'\n",
              "                                   '  [\"ts\", new JavascriptParser()],\\n'\n",
              "                                   '  [\"tsx\", new JavascriptParser()],\\n'\n",
              "                                   '  [\"js\", new JavascriptParser()],\\n'\n",
              "                                   '  [\"jsx\", new JavascriptParser()],\\n'\n",
              "                                   ']);\\n'\n",
              "                                   '\\n'\n",
              "                                   'export const getParserForExtension = '\n",
              "                                   '(filename: string) => {\\n'\n",
              "                                   '  const fileExtension = '\n",
              "                                   'filename.split(\".\").pop().toLowerCase();\\n'\n",
              "                                   '  return '\n",
              "                                   'EXTENSIONS_TO_PARSERS.get(fileExtension) '\n",
              "                                   '|| null;\\n'\n",
              "                                   '};\\n'\n",
              "                                   '\\n'\n",
              "                                   'export const assignLineNumbers = '\n",
              "                                   '(contents: string): string => {\\n'\n",
              "                                   '  const lines = contents.split(\"\\\\n\");\\n'\n",
              "                                   '  let lineNumber = 1;\\n'\n",
              "                                   '  const linesWithNumbers = '\n",
              "                                   'lines.map((line) => {\\n'\n",
              "                                   '    const numberedLine = `${lineNumber}: '\n",
              "                                   '${line}`;\\n'\n",
              "                                   '    lineNumber++;\\n'\n",
              "                                   '    return numberedLine;\\n'\n",
              "                                   '  });\\n'\n",
              "                                   '  return linesWithNumbers.join(\"\\\\n\");\\n'\n",
              "                                   '};\\n'},\n",
              "              'score': 0.19671075,\n",
              "              'values': []},\n",
              "             {'id': 'b8c28503-8e92-4430-97de-a4a202795e4f',\n",
              "              'metadata': {'source': 'src/data/PRSuggestionImpl.ts',\n",
              "                           'text': 'src/data/PRSuggestionImpl.ts\\n'\n",
              "                                   '\\n'\n",
              "                                   'import { PRSuggestion } from '\n",
              "                                   '\"../constants\";\\n'\n",
              "                                   '\\n'\n",
              "                                   'export class PRSuggestionImpl implements '\n",
              "                                   'PRSuggestion {\\n'\n",
              "                                   '  describe: string;\\n'\n",
              "                                   '  type: string;\\n'\n",
              "                                   '  comment: string;\\n'\n",
              "                                   '  code: string;\\n'\n",
              "                                   '  filename: string;\\n'\n",
              "                                   '\\n'\n",
              "                                   '  constructor(\\n'\n",
              "                                   '    describe: string,\\n'\n",
              "                                   '    type: string,\\n'\n",
              "                                   '    comment: string,\\n'\n",
              "                                   '    code: string,\\n'\n",
              "                                   '    filename: string\\n'\n",
              "                                   '  ) {\\n'\n",
              "                                   '    this.describe = describe;\\n'\n",
              "                                   '    this.type = type;\\n'\n",
              "                                   '    this.comment = comment;\\n'\n",
              "                                   '    this.code = code;\\n'\n",
              "                                   '    this.filename = filename;\\n'\n",
              "                                   '  }\\n'\n",
              "                                   '\\n'\n",
              "                                   '  toString(): string {\\n'\n",
              "                                   '    const xmlElements = [\\n'\n",
              "                                   '      `<suggestion>`,\\n'\n",
              "                                   '      `  '\n",
              "                                   '<describe>${this.describe}</describe>`,\\n'\n",
              "                                   '      `  <type>${this.type}</type>`,\\n'\n",
              "                                   '      `  '\n",
              "                                   '<comment>${this.comment}</comment>`,\\n'\n",
              "                                   '      `  <code>${this.code}</code>`,\\n'\n",
              "                                   '      `  '\n",
              "                                   '<filename>${this.filename}</filename>`,\\n'\n",
              "                                   '      `</suggestion>`,\\n'\n",
              "                                   '    ];\\n'\n",
              "                                   '    return xmlElements.join(\"\\\\n\");\\n'\n",
              "                                   '  }\\n'\n",
              "                                   '\\n'\n",
              "                                   '  identity(): string {\\n'\n",
              "                                   '    return '\n",
              "                                   '`${this.filename}:${this.comment}`;\\n'\n",
              "                                   '  }\\n'\n",
              "                                   '}\\n'},\n",
              "              'score': 0.167432636,\n",
              "              'values': []}],\n",
              " 'namespace': 'https://github.com/CoderAgent/SecureAgent',\n",
              " 'usage': {'read_units': 6}}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "top_matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "LDKAXsu1qkKF"
      },
      "outputs": [],
      "source": [
        "context = [item['metadata']['text'] for item in top_matches['matches']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context"
      ],
      "metadata": {
        "id": "NT5MEtjWQFpt",
        "outputId": "82a8c60f-ab3d-4fac-8e1b-aa66ca93232a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['src/context/language/javascript-parser.ts\\n\\nimport { AbstractParser, EnclosingContext } from \"../../constants\";\\nimport * as parser from \"@babel/parser\";\\nimport traverse, { NodePath, Node } from \"@babel/traverse\";\\n\\nconst processNode = (\\n  path: NodePath<Node>,\\n  lineStart: number,\\n  lineEnd: number,\\n  largestSize: number,\\n  largestEnclosingContext: Node | null\\n) => {\\n  const { start, end } = path.node.loc;\\n  if (start.line <= lineStart && lineEnd <= end.line) {\\n    const size = end.line - start.line;\\n    if (size > largestSize) {\\n      largestSize = size;\\n      largestEnclosingContext = path.node;\\n    }\\n  }\\n  return { largestSize, largestEnclosingContext };\\n};\\n\\nexport class JavascriptParser implements AbstractParser {\\n  findEnclosingContext(\\n    file: string,\\n    lineStart: number,\\n    lineEnd: number\\n  ): EnclosingContext {\\n    const ast = parser.parse(file, {\\n      sourceType: \"module\",\\n      plugins: [\"jsx\", \"typescript\"], // To allow JSX and TypeScript\\n    });\\n    let largestEnclosingContext: Node = null;\\n    let largestSize = 0;\\n    traverse(ast, {\\n      Function(path) {\\n        ({ largestSize, largestEnclosingContext } = processNode(\\n          path,\\n          lineStart,\\n          lineEnd,\\n          largestSize,\\n          largestEnclosingContext\\n        ));\\n      },\\n      TSInterfaceDeclaration(path) {\\n        ({ largestSize, largestEnclosingContext } = processNode(\\n          path,\\n          lineStart,\\n          lineEnd,\\n          largestSize,\\n          largestEnclosingContext\\n        ));\\n      },\\n    });\\n    return {\\n      enclosingContext: largestEnclosingContext,\\n    } as EnclosingContext;\\n  }\\n\\n  dryRun(file: string): { valid: boolean; error: string } {\\n    try {\\n      const ast = parser.parse(file, {\\n        sourceType: \"module\",\\n        plugins: [\"jsx\", \"typescript\"], // To allow JSX and TypeScript\\n      });\\n      return {\\n        valid: true,\\n        error: \"\",\\n      };\\n    } catch (exc) {\\n      return {\\n        valid: false,\\n        error: exc,\\n      };\\n    }\\n  }\\n}\\n',\n",
              " 'src/context/language/python-parser.ts\\n\\nimport { AbstractParser, EnclosingContext } from \"../../constants\";\\nexport class PythonParser implements AbstractParser {\\n  findEnclosingContext(\\n    file: string,\\n    lineStart: number,\\n    lineEnd: number\\n  ): EnclosingContext {\\n    // TODO: Implement this method for Python\\n    return null;\\n  }\\n  dryRun(file: string): { valid: boolean; error: string } {\\n    // TODO: Implement this method for Python\\n    return { valid: false, error: \"Not implemented yet\" };\\n  }\\n}\\n',\n",
              " 'src/context/review.ts\\n\\nimport {\\n  AbstractParser,\\n  PRFile,\\n  PatchInfo,\\n  getParserForExtension,\\n} from \"../constants\";\\nimport * as diff from \"diff\";\\nimport { JavascriptParser } from \"./language/javascript-parser\";\\nimport { Node } from \"@babel/traverse\";\\n\\nconst expandHunk = (\\n  contents: string,\\n  hunk: diff.Hunk,\\n  linesAbove: number = 5,\\n  linesBelow: number = 5\\n) => {\\n  const fileLines = contents.split(\"\\\\n\");\\n  const curExpansion: string[] = [];\\n  const start = Math.max(0, hunk.oldStart - 1 - linesAbove);\\n  const end = Math.min(\\n    fileLines.length,\\n    hunk.oldStart - 1 + hunk.oldLines + linesBelow\\n  );\\n\\n  for (let i = start; i < hunk.oldStart - 1; i++) {\\n    curExpansion.push(fileLines[i]);\\n  }\\n\\n  curExpansion.push(\\n    `@@ -${hunk.oldStart},${hunk.oldLines} +${hunk.newStart},${hunk.newLines} @@`\\n  );\\n  hunk.lines.forEach((line) => {\\n    if (!curExpansion.includes(line)) {\\n      curExpansion.push(line);\\n    }\\n  });\\n\\n  for (let i = hunk.oldStart - 1 + hunk.oldLines; i < end; i++) {\\n    curExpansion.push(fileLines[i]);\\n  }\\n  return curExpansion.join(\"\\\\n\");\\n};\\n\\nconst expandFileLines = (\\n  file: PRFile,\\n  linesAbove: number = 5,\\n  linesBelow: number = 5\\n) => {\\n  const fileLines = file.old_contents.split(\"\\\\n\");\\n  const patches: PatchInfo[] = diff.parsePatch(file.patch);\\n  const expandedLines: string[][] = [];\\n  patches.forEach((patch) => {\\n    patch.hunks.forEach((hunk) => {\\n      const curExpansion: string[] = [];\\n      const start = Math.max(0, hunk.oldStart - 1 - linesAbove);\\n      const end = Math.min(\\n        fileLines.length,\\n        hunk.oldStart - 1 + hunk.oldLines + linesBelow\\n      );\\n\\n      for (let i = start; i < hunk.oldStart - 1; i++) {\\n        curExpansion.push(fileLines[i]);\\n      }\\n\\n      curExpansion.push(\\n        `@@ -${hunk.oldStart},${hunk.oldLines} +${hunk.newStart},${hunk.newLines} @@`\\n      );\\n      hunk.lines.forEach((line) => {\\n        if (!curExpansion.includes(line)) {\\n          curExpansion.push(line);\\n        }\\n      });\\n\\n      for (let i = hunk.oldStart - 1 + hunk.oldLines; i < end; i++) {\\n        curExpansion.push(fileLines[i]);\\n      }\\n      expandedLines.push(curExpansion);\\n    });\\n  });\\n\\n  return expandedLines;\\n};\\n\\nexport const expandedPatchStrategy = (file: PRFile) => {\\n  const expandedPatches = expandFileLines(file);\\n  const expansions = expandedPatches\\n    .map((patchLines) => patchLines.join(\"\\\\n\"))\\n    .join(\"\\\\n\\\\n\");\\n  return `## ${file.filename}\\\\n\\\\n${expansions}`;\\n};\\n\\nexport const rawPatchStrategy = (file: PRFile) => {\\n  return `## ${file.filename}\\\\n\\\\n${file.patch}`;\\n};\\n\\nconst trimHunk = (hunk: diff.Hunk): diff.Hunk => {\\n  const startIdx = hunk.lines.findIndex(\\n    (line) => line.startsWith(\"+\") || line.startsWith(\"-\")\\n  );\\n  const endIdx = hunk.lines\\n    .slice()\\n    .reverse()\\n    .findIndex((line) => line.startsWith(\"+\") || line.startsWith(\"-\"));\\n  const editLines = hunk.lines.slice(startIdx, hunk.lines.length - endIdx);\\n  return { ...hunk, lines: editLines, newStart: startIdx + hunk.newStart };\\n};\\n\\nconst buildingScopeString = (\\n  currentFile: string,\\n  scope: Node,\\n  hunk: diff.Hunk\\n) => {\\n  const res: string[] = [];\\n  const trimmedHunk = trimHunk(hunk);\\n  const functionStartLine = scope.loc.start.line;\\n  const functionEndLine = scope.loc.end.line;\\n  const updatedFileLines = currentFile.split(\"\\\\n\");\\n  // Extract the lines of the function\\n  const functionContext = updatedFileLines.slice(\\n    functionStartLine - 1,\\n    functionEndLine\\n  );\\n  // Calculate the index where the changes should be injected into the function\\n  const injectionIdx =\\n    hunk.newStart -\\n    functionStartLine +\\n    hunk.lines.findIndex(\\n      (line) => line.startsWith(\"+\") || line.startsWith(\"-\")\\n    );\\n  // Count the number of lines that should be dropped from the function\\n  const dropCount = trimmedHunk.lines.filter(\\n    (line) => !line.startsWith(\"-\")\\n  ).length;\\n\\n  const hunkHeader = `@@ -${hunk.oldStart},${hunk.oldLines} +${hunk.newStart},${hunk.newLines} @@`;\\n  // Inject the changes into the function, dropping the necessary lines\\n  functionContext.splice(injectionIdx, dropCount, ...trimmedHunk.lines);\\n\\n  res.push(functionContext.join(\"\\\\n\"));\\n  res.unshift(hunkHeader);\\n  return res;\\n};\\n\\n/*\\nline nums are 0 index, file is 1 index\\n*/\\nconst combineHunks = (\\n  file: string,\\n  overlappingHunks: diff.Hunk[]\\n): diff.Hunk => {\\n  if (!overlappingHunks || overlappingHunks.length === 0) {\\n    throw \"Overlapping hunks are empty, this should never happen.\";\\n  }\\n  const sortedHunks = overlappingHunks.sort((a, b) => a.newStart - b.newStart);\\n  const fileLines = file.split(\"\\\\n\");\\n  let lastHunkEnd = sortedHunks[0].newStart + sortedHunks[0].newLines;\\n\\n  const combinedHunk: diff.Hunk = {\\n    oldStart: sortedHunks[0].oldStart,\\n    oldLines: sortedHunks[0].oldLines,\\n    newStart: sortedHunks[0].newStart,\\n    newLines: sortedHunks[0].newLines,\\n    lines: [...sortedHunks[0].lines],\\n    linedelimiters: [...sortedHunks[0].linedelimiters],\\n  };\\n\\n  for (let i = 1; i < sortedHunks.length; i++) {\\n    const hunk = sortedHunks[i];\\n\\n    // If there\\'s a gap between the last hunk and this one, add the lines in between\\n    if (hunk.newStart > lastHunkEnd) {\\n      combinedHunk.lines.push(\\n        ...fileLines.slice(lastHunkEnd - 1, hunk.newStart - 1)\\n      );\\n      combinedHunk.newLines += hunk.newStart - lastHunkEnd;\\n    }\\n\\n    combinedHunk.oldLines += hunk.oldLines;\\n    combinedHunk.newLines += hunk.newLines;\\n    combinedHunk.lines.push(...hunk.lines);\\n    combinedHunk.linedelimiters.push(...hunk.linedelimiters);\\n\\n    lastHunkEnd = hunk.newStart + hunk.newLines;\\n  }\\n  return combinedHunk;\\n};\\n\\nconst diffContextPerHunk = (file: PRFile, parser: AbstractParser) => {\\n  const updatedFile = diff.applyPatch(file.old_contents, file.patch);\\n  const patches = diff.parsePatch(file.patch);\\n  if (!updatedFile || typeof updatedFile !== \"string\") {\\n    console.log(\"APPLYING PATCH ERROR - FALLINGBACK\");\\n    throw \"THIS SHOULD NOT HAPPEN!\";\\n  }\\n\\n  const hunks: diff.Hunk[] = [];\\n  const order: number[] = [];\\n  const scopeRangeHunkMap = new Map<string, diff.Hunk[]>();\\n  const scopeRangeNodeMap = new Map<string, Node>();\\n  const expandStrategy: diff.Hunk[] = [];\\n\\n  patches.forEach((p) => {\\n    p.hunks.forEach((hunk) => {\\n      hunks.push(hunk);\\n    });\\n  });\\n\\n  hunks.forEach((hunk, idx) => {\\n    try {\\n      const trimmedHunk = trimHunk(hunk);\\n      const insertions = hunk.lines.filter((line) =>\\n        line.startsWith(\"+\")\\n      ).length;\\n      const lineStart = trimmedHunk.newStart;\\n      const lineEnd = lineStart + insertions;\\n      const largestEnclosingFunction = parser.findEnclosingContext(\\n        updatedFile,\\n        lineStart,\\n        lineEnd\\n      ).enclosingContext;\\n\\n      if (largestEnclosingFunction) {\\n        const enclosingRangeKey = `${largestEnclosingFunction.loc.start.line} -> ${largestEnclosingFunction.loc.end.line}`;\\n        let existingHunks = scopeRangeHunkMap.get(enclosingRangeKey) || [];\\n        existingHunks.push(hunk);\\n        scopeRangeHunkMap.set(enclosingRangeKey, existingHunks);\\n        scopeRangeNodeMap.set(enclosingRangeKey, largestEnclosingFunction);\\n      } else {\\n        throw \"No enclosing function.\";\\n      }\\n      order.push(idx);\\n    } catch (exc) {\\n      console.log(file.filename);\\n      console.log(\"NORMAL STRATEGY\");\\n      console.log(exc);\\n      expandStrategy.push(hunk);\\n      order.push(idx);\\n    }\\n  });\\n\\n  const scopeStategy: [string, diff.Hunk][] = []; // holds map range key and combined hunk: [[key, hunk]]\\n  for (const [range, hunks] of scopeRangeHunkMap.entries()) {\\n    const combinedHunk = combineHunks(updatedFile, hunks);\\n    scopeStategy.push([range, combinedHunk]);\\n  }\\n\\n  const contexts: string[] = [];\\n  scopeStategy.forEach(([rangeKey, hunk]) => {\\n    const context = buildingScopeString(\\n      updatedFile,\\n      scopeRangeNodeMap.get(rangeKey),\\n      hunk\\n    ).join(\"\\\\n\");\\n    contexts.push(context);\\n  });\\n  expandStrategy.forEach((hunk) => {\\n    const context = expandHunk(file.old_contents, hunk);\\n    contexts.push(context);\\n  });\\n  return contexts;\\n};\\n\\nconst functionContextPatchStrategy = (\\n  file: PRFile,\\n  parser: AbstractParser\\n): string => {\\n  let res = null;\\n  try {\\n    const contextChunks = diffContextPerHunk(file, parser);\\n    res = `## ${file.filename}\\\\n\\\\n${contextChunks.join(\"\\\\n\\\\n\")}`;\\n  } catch (exc) {\\n    console.log(exc);\\n    res = expandedPatchStrategy(file);\\n  }\\n  return res;\\n};\\n\\nexport const smarterContextPatchStrategy = (file: PRFile) => {\\n  const parser: AbstractParser = getParserForExtension(file.filename);\\n  if (parser != null) {\\n    return functionContextPatchStrategy(file, parser);\\n  } else {\\n    return expandedPatchStrategy(file);\\n  }\\n};\\n',\n",
              " 'src/constants.ts\\n\\nimport { Node } from \"@babel/traverse\";\\nimport { JavascriptParser } from \"./context/language/javascript-parser\";\\nimport { ChatCompletionMessageParam } from \"groq-sdk/resources/chat/completions\";\\n\\nexport interface PRFile {\\n  sha: string;\\n  filename: string;\\n  status:\\n    | \"added\"\\n    | \"removed\"\\n    | \"renamed\"\\n    | \"changed\"\\n    | \"modified\"\\n    | \"copied\"\\n    | \"unchanged\";\\n  additions: number;\\n  deletions: number;\\n  changes: number;\\n  blob_url: string;\\n  raw_url: string;\\n  contents_url: string;\\n  patch?: string;\\n  previous_filename?: string;\\n  patchTokenLength?: number;\\n  old_contents?: string;\\n  current_contents?: string;\\n}\\n\\nexport interface BuilderResponse {\\n  comment: string;\\n  structuredComments: any[];\\n}\\n\\nexport interface Builders {\\n  convoBuilder: (diff: string) => ChatCompletionMessageParam[];\\n  responseBuilder: (feedbacks: string[]) => Promise<BuilderResponse>;\\n}\\n\\nexport interface PatchInfo {\\n  hunks: {\\n    oldStart: number;\\n    oldLines: number;\\n    newStart: number;\\n    newLines: number;\\n    lines: string[];\\n  }[];\\n}\\n\\nexport interface PRSuggestion {\\n  describe: string;\\n  type: string;\\n  comment: string;\\n  code: string;\\n  filename: string;\\n  toString: () => string;\\n  identity: () => string;\\n}\\n\\nexport interface CodeSuggestion {\\n  file: string;\\n  line_start: number;\\n  line_end: number;\\n  correction: string;\\n  comment: string;\\n}\\n\\nexport interface ChatMessage {\\n  role: string;\\n  content: string;\\n}\\n\\nexport interface Review {\\n  review: BuilderResponse;\\n  suggestions: CodeSuggestion[];\\n}\\n\\nexport interface BranchDetails {\\n  name: string;\\n  sha: string;\\n  url: string;\\n}\\n\\nexport const sleep = async (ms: number) => {\\n  return new Promise((resolve) => setTimeout(resolve, ms));\\n};\\n\\nexport const processGitFilepath = (filepath: string) => {\\n  // Remove the leading \\'/\\' if it exists\\n  return filepath.startsWith(\"/\") ? filepath.slice(1) : filepath;\\n};\\n\\nexport interface EnclosingContext {\\n  enclosingContext: Node | null;\\n}\\n\\nexport interface AbstractParser {\\n  findEnclosingContext(\\n    file: string,\\n    lineStart: number,\\n    lineEnd: number\\n  ): EnclosingContext;\\n  dryRun(file: string): { valid: boolean; error: string };\\n}\\n\\nconst EXTENSIONS_TO_PARSERS: Map<string, AbstractParser> = new Map([\\n  [\"ts\", new JavascriptParser()],\\n  [\"tsx\", new JavascriptParser()],\\n  [\"js\", new JavascriptParser()],\\n  [\"jsx\", new JavascriptParser()],\\n]);\\n\\nexport const getParserForExtension = (filename: string) => {\\n  const fileExtension = filename.split(\".\").pop().toLowerCase();\\n  return EXTENSIONS_TO_PARSERS.get(fileExtension) || null;\\n};\\n\\nexport const assignLineNumbers = (contents: string): string => {\\n  const lines = contents.split(\"\\\\n\");\\n  let lineNumber = 1;\\n  const linesWithNumbers = lines.map((line) => {\\n    const numberedLine = `${lineNumber}: ${line}`;\\n    lineNumber++;\\n    return numberedLine;\\n  });\\n  return linesWithNumbers.join(\"\\\\n\");\\n};\\n',\n",
              " 'src/data/PRSuggestionImpl.ts\\n\\nimport { PRSuggestion } from \"../constants\";\\n\\nexport class PRSuggestionImpl implements PRSuggestion {\\n  describe: string;\\n  type: string;\\n  comment: string;\\n  code: string;\\n  filename: string;\\n\\n  constructor(\\n    describe: string,\\n    type: string,\\n    comment: string,\\n    code: string,\\n    filename: string\\n  ) {\\n    this.describe = describe;\\n    this.type = type;\\n    this.comment = comment;\\n    this.code = code;\\n    this.filename = filename;\\n  }\\n\\n  toString(): string {\\n    const xmlElements = [\\n      `<suggestion>`,\\n      `  <describe>${this.describe}</describe>`,\\n      `  <type>${this.type}</type>`,\\n      `  <comment>${this.comment}</comment>`,\\n      `  <code>${this.code}</code>`,\\n      `  <filename>${this.filename}</filename>`,\\n      `</suggestion>`,\\n    ];\\n    return xmlElements.join(\"\\\\n\");\\n  }\\n\\n  identity(): string {\\n    return `${this.filename}:${this.comment}`;\\n  }\\n}\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augumented_query = \"<CONTEXT>\\n\" + \"\\n\\n-----------------\\n\\n\".join(context) + \"\\n--------------\\n</CONTEXT>\\n\\n\\n\\nMY QUESTION:\\n\" + query"
      ],
      "metadata": {
        "id": "5YBMAMRzQFr_"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(augumented_query)"
      ],
      "metadata": {
        "id": "C-zcGHLXQFuF",
        "outputId": "874c6784-de33-4e85-df19-dc3c029262b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<CONTEXT>\n",
            "src/context/language/javascript-parser.ts\n",
            "\n",
            "import { AbstractParser, EnclosingContext } from \"../../constants\";\n",
            "import * as parser from \"@babel/parser\";\n",
            "import traverse, { NodePath, Node } from \"@babel/traverse\";\n",
            "\n",
            "const processNode = (\n",
            "  path: NodePath<Node>,\n",
            "  lineStart: number,\n",
            "  lineEnd: number,\n",
            "  largestSize: number,\n",
            "  largestEnclosingContext: Node | null\n",
            ") => {\n",
            "  const { start, end } = path.node.loc;\n",
            "  if (start.line <= lineStart && lineEnd <= end.line) {\n",
            "    const size = end.line - start.line;\n",
            "    if (size > largestSize) {\n",
            "      largestSize = size;\n",
            "      largestEnclosingContext = path.node;\n",
            "    }\n",
            "  }\n",
            "  return { largestSize, largestEnclosingContext };\n",
            "};\n",
            "\n",
            "export class JavascriptParser implements AbstractParser {\n",
            "  findEnclosingContext(\n",
            "    file: string,\n",
            "    lineStart: number,\n",
            "    lineEnd: number\n",
            "  ): EnclosingContext {\n",
            "    const ast = parser.parse(file, {\n",
            "      sourceType: \"module\",\n",
            "      plugins: [\"jsx\", \"typescript\"], // To allow JSX and TypeScript\n",
            "    });\n",
            "    let largestEnclosingContext: Node = null;\n",
            "    let largestSize = 0;\n",
            "    traverse(ast, {\n",
            "      Function(path) {\n",
            "        ({ largestSize, largestEnclosingContext } = processNode(\n",
            "          path,\n",
            "          lineStart,\n",
            "          lineEnd,\n",
            "          largestSize,\n",
            "          largestEnclosingContext\n",
            "        ));\n",
            "      },\n",
            "      TSInterfaceDeclaration(path) {\n",
            "        ({ largestSize, largestEnclosingContext } = processNode(\n",
            "          path,\n",
            "          lineStart,\n",
            "          lineEnd,\n",
            "          largestSize,\n",
            "          largestEnclosingContext\n",
            "        ));\n",
            "      },\n",
            "    });\n",
            "    return {\n",
            "      enclosingContext: largestEnclosingContext,\n",
            "    } as EnclosingContext;\n",
            "  }\n",
            "\n",
            "  dryRun(file: string): { valid: boolean; error: string } {\n",
            "    try {\n",
            "      const ast = parser.parse(file, {\n",
            "        sourceType: \"module\",\n",
            "        plugins: [\"jsx\", \"typescript\"], // To allow JSX and TypeScript\n",
            "      });\n",
            "      return {\n",
            "        valid: true,\n",
            "        error: \"\",\n",
            "      };\n",
            "    } catch (exc) {\n",
            "      return {\n",
            "        valid: false,\n",
            "        error: exc,\n",
            "      };\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "-----------------\n",
            "\n",
            "src/context/language/python-parser.ts\n",
            "\n",
            "import { AbstractParser, EnclosingContext } from \"../../constants\";\n",
            "export class PythonParser implements AbstractParser {\n",
            "  findEnclosingContext(\n",
            "    file: string,\n",
            "    lineStart: number,\n",
            "    lineEnd: number\n",
            "  ): EnclosingContext {\n",
            "    // TODO: Implement this method for Python\n",
            "    return null;\n",
            "  }\n",
            "  dryRun(file: string): { valid: boolean; error: string } {\n",
            "    // TODO: Implement this method for Python\n",
            "    return { valid: false, error: \"Not implemented yet\" };\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "-----------------\n",
            "\n",
            "src/context/review.ts\n",
            "\n",
            "import {\n",
            "  AbstractParser,\n",
            "  PRFile,\n",
            "  PatchInfo,\n",
            "  getParserForExtension,\n",
            "} from \"../constants\";\n",
            "import * as diff from \"diff\";\n",
            "import { JavascriptParser } from \"./language/javascript-parser\";\n",
            "import { Node } from \"@babel/traverse\";\n",
            "\n",
            "const expandHunk = (\n",
            "  contents: string,\n",
            "  hunk: diff.Hunk,\n",
            "  linesAbove: number = 5,\n",
            "  linesBelow: number = 5\n",
            ") => {\n",
            "  const fileLines = contents.split(\"\\n\");\n",
            "  const curExpansion: string[] = [];\n",
            "  const start = Math.max(0, hunk.oldStart - 1 - linesAbove);\n",
            "  const end = Math.min(\n",
            "    fileLines.length,\n",
            "    hunk.oldStart - 1 + hunk.oldLines + linesBelow\n",
            "  );\n",
            "\n",
            "  for (let i = start; i < hunk.oldStart - 1; i++) {\n",
            "    curExpansion.push(fileLines[i]);\n",
            "  }\n",
            "\n",
            "  curExpansion.push(\n",
            "    `@@ -${hunk.oldStart},${hunk.oldLines} +${hunk.newStart},${hunk.newLines} @@`\n",
            "  );\n",
            "  hunk.lines.forEach((line) => {\n",
            "    if (!curExpansion.includes(line)) {\n",
            "      curExpansion.push(line);\n",
            "    }\n",
            "  });\n",
            "\n",
            "  for (let i = hunk.oldStart - 1 + hunk.oldLines; i < end; i++) {\n",
            "    curExpansion.push(fileLines[i]);\n",
            "  }\n",
            "  return curExpansion.join(\"\\n\");\n",
            "};\n",
            "\n",
            "const expandFileLines = (\n",
            "  file: PRFile,\n",
            "  linesAbove: number = 5,\n",
            "  linesBelow: number = 5\n",
            ") => {\n",
            "  const fileLines = file.old_contents.split(\"\\n\");\n",
            "  const patches: PatchInfo[] = diff.parsePatch(file.patch);\n",
            "  const expandedLines: string[][] = [];\n",
            "  patches.forEach((patch) => {\n",
            "    patch.hunks.forEach((hunk) => {\n",
            "      const curExpansion: string[] = [];\n",
            "      const start = Math.max(0, hunk.oldStart - 1 - linesAbove);\n",
            "      const end = Math.min(\n",
            "        fileLines.length,\n",
            "        hunk.oldStart - 1 + hunk.oldLines + linesBelow\n",
            "      );\n",
            "\n",
            "      for (let i = start; i < hunk.oldStart - 1; i++) {\n",
            "        curExpansion.push(fileLines[i]);\n",
            "      }\n",
            "\n",
            "      curExpansion.push(\n",
            "        `@@ -${hunk.oldStart},${hunk.oldLines} +${hunk.newStart},${hunk.newLines} @@`\n",
            "      );\n",
            "      hunk.lines.forEach((line) => {\n",
            "        if (!curExpansion.includes(line)) {\n",
            "          curExpansion.push(line);\n",
            "        }\n",
            "      });\n",
            "\n",
            "      for (let i = hunk.oldStart - 1 + hunk.oldLines; i < end; i++) {\n",
            "        curExpansion.push(fileLines[i]);\n",
            "      }\n",
            "      expandedLines.push(curExpansion);\n",
            "    });\n",
            "  });\n",
            "\n",
            "  return expandedLines;\n",
            "};\n",
            "\n",
            "export const expandedPatchStrategy = (file: PRFile) => {\n",
            "  const expandedPatches = expandFileLines(file);\n",
            "  const expansions = expandedPatches\n",
            "    .map((patchLines) => patchLines.join(\"\\n\"))\n",
            "    .join(\"\\n\\n\");\n",
            "  return `## ${file.filename}\\n\\n${expansions}`;\n",
            "};\n",
            "\n",
            "export const rawPatchStrategy = (file: PRFile) => {\n",
            "  return `## ${file.filename}\\n\\n${file.patch}`;\n",
            "};\n",
            "\n",
            "const trimHunk = (hunk: diff.Hunk): diff.Hunk => {\n",
            "  const startIdx = hunk.lines.findIndex(\n",
            "    (line) => line.startsWith(\"+\") || line.startsWith(\"-\")\n",
            "  );\n",
            "  const endIdx = hunk.lines\n",
            "    .slice()\n",
            "    .reverse()\n",
            "    .findIndex((line) => line.startsWith(\"+\") || line.startsWith(\"-\"));\n",
            "  const editLines = hunk.lines.slice(startIdx, hunk.lines.length - endIdx);\n",
            "  return { ...hunk, lines: editLines, newStart: startIdx + hunk.newStart };\n",
            "};\n",
            "\n",
            "const buildingScopeString = (\n",
            "  currentFile: string,\n",
            "  scope: Node,\n",
            "  hunk: diff.Hunk\n",
            ") => {\n",
            "  const res: string[] = [];\n",
            "  const trimmedHunk = trimHunk(hunk);\n",
            "  const functionStartLine = scope.loc.start.line;\n",
            "  const functionEndLine = scope.loc.end.line;\n",
            "  const updatedFileLines = currentFile.split(\"\\n\");\n",
            "  // Extract the lines of the function\n",
            "  const functionContext = updatedFileLines.slice(\n",
            "    functionStartLine - 1,\n",
            "    functionEndLine\n",
            "  );\n",
            "  // Calculate the index where the changes should be injected into the function\n",
            "  const injectionIdx =\n",
            "    hunk.newStart -\n",
            "    functionStartLine +\n",
            "    hunk.lines.findIndex(\n",
            "      (line) => line.startsWith(\"+\") || line.startsWith(\"-\")\n",
            "    );\n",
            "  // Count the number of lines that should be dropped from the function\n",
            "  const dropCount = trimmedHunk.lines.filter(\n",
            "    (line) => !line.startsWith(\"-\")\n",
            "  ).length;\n",
            "\n",
            "  const hunkHeader = `@@ -${hunk.oldStart},${hunk.oldLines} +${hunk.newStart},${hunk.newLines} @@`;\n",
            "  // Inject the changes into the function, dropping the necessary lines\n",
            "  functionContext.splice(injectionIdx, dropCount, ...trimmedHunk.lines);\n",
            "\n",
            "  res.push(functionContext.join(\"\\n\"));\n",
            "  res.unshift(hunkHeader);\n",
            "  return res;\n",
            "};\n",
            "\n",
            "/*\n",
            "line nums are 0 index, file is 1 index\n",
            "*/\n",
            "const combineHunks = (\n",
            "  file: string,\n",
            "  overlappingHunks: diff.Hunk[]\n",
            "): diff.Hunk => {\n",
            "  if (!overlappingHunks || overlappingHunks.length === 0) {\n",
            "    throw \"Overlapping hunks are empty, this should never happen.\";\n",
            "  }\n",
            "  const sortedHunks = overlappingHunks.sort((a, b) => a.newStart - b.newStart);\n",
            "  const fileLines = file.split(\"\\n\");\n",
            "  let lastHunkEnd = sortedHunks[0].newStart + sortedHunks[0].newLines;\n",
            "\n",
            "  const combinedHunk: diff.Hunk = {\n",
            "    oldStart: sortedHunks[0].oldStart,\n",
            "    oldLines: sortedHunks[0].oldLines,\n",
            "    newStart: sortedHunks[0].newStart,\n",
            "    newLines: sortedHunks[0].newLines,\n",
            "    lines: [...sortedHunks[0].lines],\n",
            "    linedelimiters: [...sortedHunks[0].linedelimiters],\n",
            "  };\n",
            "\n",
            "  for (let i = 1; i < sortedHunks.length; i++) {\n",
            "    const hunk = sortedHunks[i];\n",
            "\n",
            "    // If there's a gap between the last hunk and this one, add the lines in between\n",
            "    if (hunk.newStart > lastHunkEnd) {\n",
            "      combinedHunk.lines.push(\n",
            "        ...fileLines.slice(lastHunkEnd - 1, hunk.newStart - 1)\n",
            "      );\n",
            "      combinedHunk.newLines += hunk.newStart - lastHunkEnd;\n",
            "    }\n",
            "\n",
            "    combinedHunk.oldLines += hunk.oldLines;\n",
            "    combinedHunk.newLines += hunk.newLines;\n",
            "    combinedHunk.lines.push(...hunk.lines);\n",
            "    combinedHunk.linedelimiters.push(...hunk.linedelimiters);\n",
            "\n",
            "    lastHunkEnd = hunk.newStart + hunk.newLines;\n",
            "  }\n",
            "  return combinedHunk;\n",
            "};\n",
            "\n",
            "const diffContextPerHunk = (file: PRFile, parser: AbstractParser) => {\n",
            "  const updatedFile = diff.applyPatch(file.old_contents, file.patch);\n",
            "  const patches = diff.parsePatch(file.patch);\n",
            "  if (!updatedFile || typeof updatedFile !== \"string\") {\n",
            "    console.log(\"APPLYING PATCH ERROR - FALLINGBACK\");\n",
            "    throw \"THIS SHOULD NOT HAPPEN!\";\n",
            "  }\n",
            "\n",
            "  const hunks: diff.Hunk[] = [];\n",
            "  const order: number[] = [];\n",
            "  const scopeRangeHunkMap = new Map<string, diff.Hunk[]>();\n",
            "  const scopeRangeNodeMap = new Map<string, Node>();\n",
            "  const expandStrategy: diff.Hunk[] = [];\n",
            "\n",
            "  patches.forEach((p) => {\n",
            "    p.hunks.forEach((hunk) => {\n",
            "      hunks.push(hunk);\n",
            "    });\n",
            "  });\n",
            "\n",
            "  hunks.forEach((hunk, idx) => {\n",
            "    try {\n",
            "      const trimmedHunk = trimHunk(hunk);\n",
            "      const insertions = hunk.lines.filter((line) =>\n",
            "        line.startsWith(\"+\")\n",
            "      ).length;\n",
            "      const lineStart = trimmedHunk.newStart;\n",
            "      const lineEnd = lineStart + insertions;\n",
            "      const largestEnclosingFunction = parser.findEnclosingContext(\n",
            "        updatedFile,\n",
            "        lineStart,\n",
            "        lineEnd\n",
            "      ).enclosingContext;\n",
            "\n",
            "      if (largestEnclosingFunction) {\n",
            "        const enclosingRangeKey = `${largestEnclosingFunction.loc.start.line} -> ${largestEnclosingFunction.loc.end.line}`;\n",
            "        let existingHunks = scopeRangeHunkMap.get(enclosingRangeKey) || [];\n",
            "        existingHunks.push(hunk);\n",
            "        scopeRangeHunkMap.set(enclosingRangeKey, existingHunks);\n",
            "        scopeRangeNodeMap.set(enclosingRangeKey, largestEnclosingFunction);\n",
            "      } else {\n",
            "        throw \"No enclosing function.\";\n",
            "      }\n",
            "      order.push(idx);\n",
            "    } catch (exc) {\n",
            "      console.log(file.filename);\n",
            "      console.log(\"NORMAL STRATEGY\");\n",
            "      console.log(exc);\n",
            "      expandStrategy.push(hunk);\n",
            "      order.push(idx);\n",
            "    }\n",
            "  });\n",
            "\n",
            "  const scopeStategy: [string, diff.Hunk][] = []; // holds map range key and combined hunk: [[key, hunk]]\n",
            "  for (const [range, hunks] of scopeRangeHunkMap.entries()) {\n",
            "    const combinedHunk = combineHunks(updatedFile, hunks);\n",
            "    scopeStategy.push([range, combinedHunk]);\n",
            "  }\n",
            "\n",
            "  const contexts: string[] = [];\n",
            "  scopeStategy.forEach(([rangeKey, hunk]) => {\n",
            "    const context = buildingScopeString(\n",
            "      updatedFile,\n",
            "      scopeRangeNodeMap.get(rangeKey),\n",
            "      hunk\n",
            "    ).join(\"\\n\");\n",
            "    contexts.push(context);\n",
            "  });\n",
            "  expandStrategy.forEach((hunk) => {\n",
            "    const context = expandHunk(file.old_contents, hunk);\n",
            "    contexts.push(context);\n",
            "  });\n",
            "  return contexts;\n",
            "};\n",
            "\n",
            "const functionContextPatchStrategy = (\n",
            "  file: PRFile,\n",
            "  parser: AbstractParser\n",
            "): string => {\n",
            "  let res = null;\n",
            "  try {\n",
            "    const contextChunks = diffContextPerHunk(file, parser);\n",
            "    res = `## ${file.filename}\\n\\n${contextChunks.join(\"\\n\\n\")}`;\n",
            "  } catch (exc) {\n",
            "    console.log(exc);\n",
            "    res = expandedPatchStrategy(file);\n",
            "  }\n",
            "  return res;\n",
            "};\n",
            "\n",
            "export const smarterContextPatchStrategy = (file: PRFile) => {\n",
            "  const parser: AbstractParser = getParserForExtension(file.filename);\n",
            "  if (parser != null) {\n",
            "    return functionContextPatchStrategy(file, parser);\n",
            "  } else {\n",
            "    return expandedPatchStrategy(file);\n",
            "  }\n",
            "};\n",
            "\n",
            "\n",
            "-----------------\n",
            "\n",
            "src/constants.ts\n",
            "\n",
            "import { Node } from \"@babel/traverse\";\n",
            "import { JavascriptParser } from \"./context/language/javascript-parser\";\n",
            "import { ChatCompletionMessageParam } from \"groq-sdk/resources/chat/completions\";\n",
            "\n",
            "export interface PRFile {\n",
            "  sha: string;\n",
            "  filename: string;\n",
            "  status:\n",
            "    | \"added\"\n",
            "    | \"removed\"\n",
            "    | \"renamed\"\n",
            "    | \"changed\"\n",
            "    | \"modified\"\n",
            "    | \"copied\"\n",
            "    | \"unchanged\";\n",
            "  additions: number;\n",
            "  deletions: number;\n",
            "  changes: number;\n",
            "  blob_url: string;\n",
            "  raw_url: string;\n",
            "  contents_url: string;\n",
            "  patch?: string;\n",
            "  previous_filename?: string;\n",
            "  patchTokenLength?: number;\n",
            "  old_contents?: string;\n",
            "  current_contents?: string;\n",
            "}\n",
            "\n",
            "export interface BuilderResponse {\n",
            "  comment: string;\n",
            "  structuredComments: any[];\n",
            "}\n",
            "\n",
            "export interface Builders {\n",
            "  convoBuilder: (diff: string) => ChatCompletionMessageParam[];\n",
            "  responseBuilder: (feedbacks: string[]) => Promise<BuilderResponse>;\n",
            "}\n",
            "\n",
            "export interface PatchInfo {\n",
            "  hunks: {\n",
            "    oldStart: number;\n",
            "    oldLines: number;\n",
            "    newStart: number;\n",
            "    newLines: number;\n",
            "    lines: string[];\n",
            "  }[];\n",
            "}\n",
            "\n",
            "export interface PRSuggestion {\n",
            "  describe: string;\n",
            "  type: string;\n",
            "  comment: string;\n",
            "  code: string;\n",
            "  filename: string;\n",
            "  toString: () => string;\n",
            "  identity: () => string;\n",
            "}\n",
            "\n",
            "export interface CodeSuggestion {\n",
            "  file: string;\n",
            "  line_start: number;\n",
            "  line_end: number;\n",
            "  correction: string;\n",
            "  comment: string;\n",
            "}\n",
            "\n",
            "export interface ChatMessage {\n",
            "  role: string;\n",
            "  content: string;\n",
            "}\n",
            "\n",
            "export interface Review {\n",
            "  review: BuilderResponse;\n",
            "  suggestions: CodeSuggestion[];\n",
            "}\n",
            "\n",
            "export interface BranchDetails {\n",
            "  name: string;\n",
            "  sha: string;\n",
            "  url: string;\n",
            "}\n",
            "\n",
            "export const sleep = async (ms: number) => {\n",
            "  return new Promise((resolve) => setTimeout(resolve, ms));\n",
            "};\n",
            "\n",
            "export const processGitFilepath = (filepath: string) => {\n",
            "  // Remove the leading '/' if it exists\n",
            "  return filepath.startsWith(\"/\") ? filepath.slice(1) : filepath;\n",
            "};\n",
            "\n",
            "export interface EnclosingContext {\n",
            "  enclosingContext: Node | null;\n",
            "}\n",
            "\n",
            "export interface AbstractParser {\n",
            "  findEnclosingContext(\n",
            "    file: string,\n",
            "    lineStart: number,\n",
            "    lineEnd: number\n",
            "  ): EnclosingContext;\n",
            "  dryRun(file: string): { valid: boolean; error: string };\n",
            "}\n",
            "\n",
            "const EXTENSIONS_TO_PARSERS: Map<string, AbstractParser> = new Map([\n",
            "  [\"ts\", new JavascriptParser()],\n",
            "  [\"tsx\", new JavascriptParser()],\n",
            "  [\"js\", new JavascriptParser()],\n",
            "  [\"jsx\", new JavascriptParser()],\n",
            "]);\n",
            "\n",
            "export const getParserForExtension = (filename: string) => {\n",
            "  const fileExtension = filename.split(\".\").pop().toLowerCase();\n",
            "  return EXTENSIONS_TO_PARSERS.get(fileExtension) || null;\n",
            "};\n",
            "\n",
            "export const assignLineNumbers = (contents: string): string => {\n",
            "  const lines = contents.split(\"\\n\");\n",
            "  let lineNumber = 1;\n",
            "  const linesWithNumbers = lines.map((line) => {\n",
            "    const numberedLine = `${lineNumber}: ${line}`;\n",
            "    lineNumber++;\n",
            "    return numberedLine;\n",
            "  });\n",
            "  return linesWithNumbers.join(\"\\n\");\n",
            "};\n",
            "\n",
            "\n",
            "-----------------\n",
            "\n",
            "src/data/PRSuggestionImpl.ts\n",
            "\n",
            "import { PRSuggestion } from \"../constants\";\n",
            "\n",
            "export class PRSuggestionImpl implements PRSuggestion {\n",
            "  describe: string;\n",
            "  type: string;\n",
            "  comment: string;\n",
            "  code: string;\n",
            "  filename: string;\n",
            "\n",
            "  constructor(\n",
            "    describe: string,\n",
            "    type: string,\n",
            "    comment: string,\n",
            "    code: string,\n",
            "    filename: string\n",
            "  ) {\n",
            "    this.describe = describe;\n",
            "    this.type = type;\n",
            "    this.comment = comment;\n",
            "    this.code = code;\n",
            "    this.filename = filename;\n",
            "  }\n",
            "\n",
            "  toString(): string {\n",
            "    const xmlElements = [\n",
            "      `<suggestion>`,\n",
            "      `  <describe>${this.describe}</describe>`,\n",
            "      `  <type>${this.type}</type>`,\n",
            "      `  <comment>${this.comment}</comment>`,\n",
            "      `  <code>${this.code}</code>`,\n",
            "      `  <filename>${this.filename}</filename>`,\n",
            "      `</suggestion>`,\n",
            "    ];\n",
            "    return xmlElements.join(\"\\n\");\n",
            "  }\n",
            "\n",
            "  identity(): string {\n",
            "    return `${this.filename}:${this.comment}`;\n",
            "  }\n",
            "}\n",
            "\n",
            "--------------\n",
            "</CONTEXT>\n",
            "\n",
            "\n",
            "\n",
            "MY QUESTION:\n",
            "How is the javascript parser used?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"You are a Senior Software Engineer, who is an expert in TypeScript.\n",
        "\n",
        "Answer the question I have about the codebase based on the context provided.\n",
        "Always consider all of the context provided to answer my question\"\"\"\n",
        "\n",
        "\n",
        "llm_response = client.chat.completions.create(\n",
        "    model=\"qwen/qwen2.5-vl-32b-instruct:free\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": augumented_query}\n",
        "    ]\n",
        ")\n",
        "\n",
        "response = llm_response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "X5CcnC3KQFwa"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "WgOMLwcjQFyw",
        "outputId": "c6d4cdb2-23ac-4c6f-87fa-9ffa8cd56858",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The JavaScript parser, defined in `src/context/language/javascript-parser.ts`, is used to analyze and extract meaningful context from JavaScript (and TypeScript) files based on the changes made in a pull request. Here's a detailed breakdown of how it is used:\n",
            "\n",
            "### 1. **Implementation of `AbstractParser` Interface**\n",
            "The `JavascriptParser` class implements the `AbstractParser` interface, which defines two methods:\n",
            "- `findEnclosingContext(file: string, lineStart: number, lineEnd: number): EnclosingContext`\n",
            "- `dryRun(file: string): { valid: boolean; error: string }`\n",
            "\n",
            "#### `findEnclosingContext`:\n",
            "This method is responsible for finding the largest enclosing context (e.g., a function or interface) that contains the lines of interest (`lineStart` to `lineEnd`). It uses the Babel parser and traverser to analyze the Abstract Syntax Tree (AST) of the file.\n",
            "\n",
            "- **Steps**:\n",
            "  1. Parses the file into an AST using `@babel/parser`.\n",
            "  2. Traverses the AST to find nodes (e.g., `Function` or `TSInterfaceDeclaration`) that enclose the specified line range.\n",
            "  3. Determines the \"largest\" enclosing context based on the number of lines it spans.\n",
            "  4. Returns the enclosing context as part of the `EnclosingContext` object.\n",
            "\n",
            "#### `dryRun`:\n",
            "This method is used to validate whether the file can be parsed without errors. It attempts to parse the file and returns:\n",
            "- `valid: true` if parsing succeeds.\n",
            "- `valid: false` with an error message if parsing fails.\n",
            "\n",
            "### 2. **Usage in `src/context/review.ts`**\n",
            "The `JavascriptParser` is used in the `smarterContextPatchStrategy` function, which is responsible for generating context-aware diffs for pull requests. Here's how it is used:\n",
            "\n",
            "#### `diffContextPerHunk`:\n",
            "This function is the core of the context-aware diff generation. It uses the `findEnclosingContext` method of the `JavascriptParser` to identify the enclosing context for each hunk of the diff.\n",
            "\n",
            "- **Steps**:\n",
            "  1. **Parse the Patch**:\n",
            "     - The `file.patch` is parsed into hunks using the `diff` library.\n",
            "  2. **Trim and Analyze Each Hunk**:\n",
            "     - Each hunk is trimmed to remove unnecessary lines (e.g., context lines).\n",
            "     - The `lineStart` and `lineEnd` of the trimmed hunk are determined.\n",
            "  3. **Find Enclosing Context**:\n",
            "     - The `findEnclosingContext` method is called with the updated file contents (`updatedFile`) and the line range of the trimmed hunk.\n",
            "     - This method returns the largest enclosing context (e.g., a function or interface) that contains the lines of interest.\n",
            "  4. **Group Hunks by Enclosing Context**:\n",
            "     - Hunks that belong to the same enclosing context are grouped together.\n",
            "  5. **Combine Overlapping Hunks**:\n",
            "     - If multiple hunks belong to the same enclosing context, they are combined into a single hunk.\n",
            "  6. **Build Context Strings**:\n",
            "     - For each enclosing context, a context string is built by injecting the changes into the function or interface.\n",
            "     - For hunks that couldn't be grouped into an enclosing context, a fallback strategy is used (e.g., `expandHunk`).\n",
            "\n",
            "#### `functionContextPatchStrategy`:\n",
            "This function orchestrates the use of `diffContextPerHunk` to generate a context-aware diff for a file. It:\n",
            "- Calls `diffContextPerHunk` to get the context-aware hunks.\n",
            "- Formats the results into a string that includes the filename and the context-aware hunks.\n",
            "\n",
            "#### `smarterContextPatchStrategy`:\n",
            "This is the entry point for generating context-aware diffs. It:\n",
            "- Determines the appropriate parser for the file based on its extension using `getParserForExtension`.\n",
            "- If a parser is available (e.g., `JavascriptParser` for `.js`, `.jsx`, `.ts`, `.tsx` files), it uses `functionContextPatchStrategy` to generate the context-aware diff.\n",
            "- If no parser is available, it falls back to `expandedPatchStrategy`, which provides a simpler diff without context-aware analysis.\n",
            "\n",
            "### 3. **How It Works in Practice**\n",
            "When a pull request is processed:\n",
            "1. The `smarterContextPatchStrategy` function is called for each file in the pull request.\n",
            "2. If the file is a JavaScript or TypeScript file, the `JavascriptParser` is used to analyze the file and extract the enclosing context for each hunk.\n",
            "3. The enclosing context is used to generate a more meaningful diff that shows the function or interface in which the changes were made.\n",
            "4. If the file is not supported by a parser (e.g., Python files), a fallback strategy is used.\n",
            "\n",
            "### 4. **Example Workflow**\n",
            "Suppose a pull request modifies a JavaScript file (`example.js`):\n",
            "```javascript\n",
            "function add(a, b) {\n",
            "  return a + b;\n",
            "}\n",
            "\n",
            "function multiply(a, b) {\n",
            "  return a * b;\n",
            "}\n",
            "```\n",
            "If the change is:\n",
            "```diff\n",
            "diff --git a/example.js b/example.js\n",
            "index 0000000..0000000 100644\n",
            "--- a/example.js\n",
            "+++ b/example.js\n",
            "@@ -1,4 +1,4 @@\n",
            " function add(a, b) {\n",
            "-  return a + b;\n",
            "+  return a + b + 1;\n",
            " }\n",
            " \n",
            " function multiply(a, b) {\n",
            "```\n",
            "- The `diffContextPerHunk` function will:\n",
            "  - Identify the hunk that modifies the `add` function.\n",
            "  - Use `findEnclosingContext` to determine that the enclosing context is the `add` function.\n",
            "  - Generate a context-aware diff that shows the `add` function with the changes injected.\n",
            "- The resulting diff might look like:\n",
            "  ```\n",
            "  ## example.js\n",
            "  @@ -1,4 +1,4 @@\n",
            "  function add(a, b) {\n",
            "-  return a + b;\n",
            "+  return a + b + 1;\n",
            "  }\n",
            "  ```\n",
            "\n",
            "### 5. **Why It's Useful**\n",
            "- **Context Awareness**: By identifying the enclosing context, the parser ensures that the diff is meaningful and shows the relevant function or interface.\n",
            "- **Improved Review Experience**: Reviewers can quickly understand the context of the changes without having to manually search for the enclosing function or interface.\n",
            "- **Scalability**: The parser can be extended to support other languages (e.g., Python) by implementing similar logic in their respective parsers.\n",
            "\n",
            "### Summary\n",
            "The `JavascriptParser` is used to analyze JavaScript and TypeScript files and extract the largest enclosing context for changes in a pull request. This information is then used to generate context-aware diffs, improving the review experience by providing meaningful context around the changes. The parser is integrated into the `smarterContextPatchStrategy` function, which dynamically selects the appropriate parser based on the file extension.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"You are a Senior Software Engineer, who is an expert in TypeScript.\n",
        "\n",
        "Answer the question I have about the codebase based on the context provided.\n",
        "Always consider all of the context provided to answer my question\"\"\"\n",
        "\n",
        "\n",
        "llm_response = client.chat.completions.create(\n",
        "    model=\"openai/o4-mini-high\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": augumented_query}\n",
        "    ]\n",
        ")\n",
        "\n",
        "response = llm_response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "x1sCBPJZRmPf"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "poiyxGKAR4C9",
        "outputId": "239fad82-05b3-4764-c5a5-24fac1986ee0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In our code-review pipeline the JavascriptParser is really just a little wrapper around Babel that lets us carve your diff up by “what function (or interface) did I actually change?” instead of dumping you the whole  file or a raw git hunk.\n",
            "\n",
            "Here’s the high-level flow of how it’s used:\n",
            "\n",
            "1.  getParserForExtension (in constants.ts)  \n",
            "    we register one single JavascriptParser instance for the extensions  \n",
            "    `.js, .jsx, .ts, .tsx`.  \n",
            "\n",
            "2.  smarterContextPatchStrategy (in src/context/review.ts)  \n",
            "    when you ask for a “smart” diff context, we call  \n",
            "    getParserForExtension(filename) → your JavascriptParser.  \n",
            "\n",
            "3.  diffContextPerHunk(file, parser)  \n",
            "    • we apply the patch to get the “updated” file text  \n",
            "    • pull out each git hunk via `diff.parsePatch`  \n",
            "    • for each hunk, compute the range of the *new* lines that were inserted  \n",
            "    • **call** `parser.findEnclosingContext(updatedFile, lineStart, lineEnd)`  \n",
            "\n",
            "4.  JavascriptParser.findEnclosingContext  \n",
            "    • does a `@babel/parser.parse(file)` → AST with TS & JSX enabled  \n",
            "    • `traverse` the AST looking at every `Function` and  \n",
            "      `TSInterfaceDeclaration` node  \n",
            "    • for each node, if its loc (start/end lines) fully contains your changed lines,  \n",
            "      pick the *largest* one (deepest enclosing scope)  \n",
            "    • return that node as your EnclosingContext  \n",
            "\n",
            "5.  Back in diffContextPerHunk  \n",
            "    • we now know “these lines happened in function foo() { … }”  \n",
            "    • we group all hunks by that function’s start/end loc  \n",
            "    • combine overlapping hunks, splice them into the original function body,  \n",
            "      and produce a much tighter diff snippet just showing you that function  \n",
            "\n",
            "6.  Fallback  \n",
            "    if parsing fails or no enclosing context is found (or the file isn’t .js/.ts, etc.),  \n",
            "    we fall back to the “expanded patch” strategy that simply prints ±5 lines of context  \n",
            "    around each hunk.\n",
            "\n",
            "So, in short: the JavascriptParser lives in src/context/language/javascript-parser.ts, it implements AbstractParser, and it’s wired into the review logic to let us find the AST node (function or interface) that contains your changes so we can generate more focused, “function-level” diff contexts instead of raw patch dumps.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_rag(query, model=\"qwen/qwen2.5-vl-32b-instruct:free\"):\n",
        "\n",
        "    query_embedding = get_huggingface_embeddings(query)\n",
        "\n",
        "    top_matches = pinecone_index.query(vector=query_embedding.tolist(),\n",
        "                                   top_k=5,\n",
        "                                   include_metadata=True,\n",
        "                                   namespace=\"https://github.com/CoderAgent/SecureAgent\")\n",
        "\n",
        "    context = [item['metadata']['text'] for item in top_matches['matches']]\n",
        "\n",
        "    augumented_query = \"<CONTEXT>\\n\" + \"\\n\\n-----------------\\n\\n\".join(context) + \"\\n--------------\\n</CONTEXT>\\n\\n\\n\\nMY QUESTION:\\n\" + query\n",
        "\n",
        "    system_prompt = \"\"\"You are a Senior Software Engineer, who is an expert in TypeScript.\n",
        "\n",
        "    Answer the question I have about the codebase based on the context provided.\n",
        "    Always consider all of the context provided to answer my question\"\"\"\n",
        "\n",
        "\n",
        "    llm_response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": augumented_query}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    response = llm_response.choices[0].message.content\n",
        "\n",
        "    print(response)\n"
      ],
      "metadata": {
        "id": "fmD7vgUaR4Fh"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perform_rag(\"How is the javascript parser used?\")"
      ],
      "metadata": {
        "id": "_iMMskZ9R4H2",
        "outputId": "3d5bfc80-f78b-4995-aca3-fffb4707ede3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The JavaScript parser (`JavascriptParser`) is used to analyze and extract meaningful context from JavaScript files (including TypeScript and JSX files) based on the changes made in a pull request. Here's a detailed breakdown of how it is used:\n",
            "\n",
            "### 1. **Implementation of `AbstractParser` Interface**\n",
            "The `JavascriptParser` class implements the `AbstractParser` interface, which defines two methods:\n",
            "- `findEnclosingContext(file: string, lineStart: number, lineEnd: number): EnclosingContext`\n",
            "- `dryRun(file: string): { valid: boolean; error: string }`\n",
            "\n",
            "#### `findEnclosingContext`:\n",
            "This method is responsible for finding the largest enclosing context (e.g., a function or interface) that contains the lines of interest (`lineStart` to `lineEnd`). It uses the Babel parser and traverser to analyze the Abstract Syntax Tree (AST) of the file.\n",
            "\n",
            "- **Steps**:\n",
            "  1. Parse the file into an AST using `@babel/parser`.\n",
            "  2. Traverse the AST using `@babel/traverse` to find nodes (e.g., `Function` or `TSInterfaceDeclaration`) that enclose the specified line range.\n",
            "  3. Use the `processNode` helper function to determine the largest enclosing context based on the size of the node (number of lines it spans).\n",
            "  4. Return the largest enclosing context as part of the `EnclosingContext` object.\n",
            "\n",
            "#### `dryRun`:\n",
            "This method is used to validate whether the file can be parsed without errors. It attempts to parse the file and returns:\n",
            "- `valid: true` if parsing succeeds.\n",
            "- `valid: false` with an error message if parsing fails.\n",
            "\n",
            "### 2. **Usage in `smarterContextPatchStrategy`**\n",
            "The `smarterContextPatchStrategy` function in `src/context/review.ts` uses the `JavascriptParser` to provide a more intelligent context for code reviews. Here's how it works:\n",
            "\n",
            "#### **Steps**:\n",
            "1. **Retrieve the Parser**:\n",
            "   - The `getParserForExtension` function in `src/constants.ts` is used to retrieve the appropriate parser for the file based on its extension. For JavaScript/TypeScript files, it returns an instance of `JavascriptParser`.\n",
            "\n",
            "2. **Extract Context for Each Hunk**:\n",
            "   - The `diffContextPerHunk` function is called with the `file` and the `parser` (an instance of `JavascriptParser`).\n",
            "   - For each hunk in the diff, the `findEnclosingContext` method of the `JavascriptParser` is used to find the largest enclosing context that contains the lines affected by the hunk.\n",
            "   - If an enclosing context is found, it is used to build a more focused context around the changes (e.g., the function or interface where the changes occurred).\n",
            "   - If no enclosing context is found, a fallback strategy is used to expand the hunk context.\n",
            "\n",
            "3. **Build Context Strings**:\n",
            "   - The `buildingScopeString` function is used to construct a string representation of the context around the changes, including the enclosing function or interface and the diff hunk.\n",
            "   - If no enclosing context is found, the `expandHunk` function is used to expand the hunk context.\n",
            "\n",
            "4. **Combine Contexts**:\n",
            "   - The contexts for all hunks are combined into a single string, which is returned as part of the review strategy.\n",
            "\n",
            "### 3. **Example Workflow**:\n",
            "Suppose a pull request modifies a JavaScript file (`example.js`), and the changes are represented as a diff. The `smarterContextPatchStrategy` function would:\n",
            "1. Parse the file using `JavascriptParser`.\n",
            "2. Identify the hunks in the diff.\n",
            "3. For each hunk, find the largest enclosing context using `findEnclosingContext`.\n",
            "4. Build a focused context around the changes, including the enclosing function or interface.\n",
            "5. Return the combined context as part of the review strategy.\n",
            "\n",
            "### 4. **Fallback Mechanism**:\n",
            "If the `JavascriptParser` fails to find an enclosing context or if the file cannot be parsed, the `smarterContextPatchStrategy` falls back to the `expandedPatchStrategy`, which provides a broader context by expanding the hunks.\n",
            "\n",
            "### 5. **Integration with Review Strategy**:\n",
            "The `smarterContextPatchStrategy` is used in the review process to provide more meaningful and focused context to the reviewer. This helps in understanding the changes in the context of the surrounding code, making the review process more efficient.\n",
            "\n",
            "### Summary:\n",
            "The `JavascriptParser` is a crucial component for analyzing JavaScript/TypeScript files and extracting meaningful context around code changes. It is used in the `smarterContextPatchStrategy` to provide focused and intelligent context for code reviews, enhancing the review experience by highlighting the relevant parts of the code. If the parser fails, a fallback strategy is used to ensure that some context is always provided.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kg27tBreR4KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EZeXlXz0R4MR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}